"""Model optimization utilities for deployment.

Techniques:
1. Quantization: INT8 quantization for 4x speedup, 4x size reduction
2. Pruning: Remove unimportant weights for faster inference
3. Knowledge Distillation: Train smaller student models
4. ONNX Export: Export to ONNX for optimized cross-platform deployment

Target: 50% inference time reduction with <5% accuracy loss
"""

from typing import Optional, Dict
from pathlib import Path
import torch
import torch.nn as nn


class ModelOptimizer:
    """Optimization utilities for paper quality models."""

    @staticmethod
    def quantize_model(
        model: nn.Module,
        output_path: str,
        quantization_type: str = "dynamic"
    ) -> nn.Module:
        """Quantize model to INT8 for faster inference.

        Args:
            model: PyTorch model to quantize
            output_path: Path to save quantized model
            quantization_type: "dynamic" or "static"

        Returns:
            Quantized model

        Performance:
        - 4x smaller model size
        - 2-4x faster inference
        - <2% accuracy loss (typically)
        """
        model.eval()

        if quantization_type == "dynamic":
            # Dynamic quantization (easiest, works well for RNN/Transformer)
            quantized_model = torch.quantization.quantize_dynamic(
                model,
                {nn.Linear, nn.LSTM, nn.GRU},  # Layers to quantize
                dtype=torch.qint8
            )
        else:
            # Static quantization (requires calibration data)
            # TODO: Implement static quantization with calibration dataset
            raise NotImplementedError("Static quantization requires calibration dataset")

        # Save quantized model
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        torch.save(quantized_model.state_dict(), output_path)

        print(f"‚úÖ Quantized model saved to: {output_path}")
        print(f"   Type: {quantization_type}")
        print(f"   Expected speedup: 2-4x")
        print(f"   Expected size reduction: 4x")

        return quantized_model

    @staticmethod
    def prune_model(
        model: nn.Module,
        pruning_amount: float = 0.3,
        output_path: Optional[str] = None
    ) -> nn.Module:
        """Prune unimportant weights from model.

        Args:
            model: PyTorch model to prune
            pruning_amount: Fraction of weights to prune (0.0-1.0)
            output_path: Optional path to save pruned model

        Returns:
            Pruned model

        Performance:
        - Faster inference (10-30%)
        - Smaller model size
        - May require fine-tuning to recover accuracy
        """
        import torch.nn.utils.prune as prune

        model.eval()

        # Get all linear layers
        parameters_to_prune = []
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                parameters_to_prune.append((module, 'weight'))

        # Apply L1 unstructured pruning
        prune.global_unstructured(
            parameters_to_prune,
            pruning_method=prune.L1Unstructured,
            amount=pruning_amount,
        )

        # Make pruning permanent
        for module, param_name in parameters_to_prune:
            prune.remove(module, param_name)

        # Save if requested
        if output_path:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            torch.save(model.state_dict(), output_path)
            print(f"‚úÖ Pruned model saved to: {output_path}")

        print(f"   Pruning amount: {pruning_amount * 100:.1f}%")
        print(f"   ‚ö†Ô∏è  May require fine-tuning to recover accuracy")

        return model

    @staticmethod
    def export_to_onnx(
        model: nn.Module,
        sample_input: torch.Tensor,
        output_path: str,
        opset_version: int = 14
    ):
        """Export model to ONNX format for optimized deployment.

        Args:
            model: PyTorch model to export
            sample_input: Sample input tensor for tracing
            output_path: Path to save ONNX model
            opset_version: ONNX opset version

        ONNX benefits:
        - Cross-platform deployment (CPU, GPU, mobile, edge)
        - Optimized inference engines (ONNX Runtime, TensorRT)
        - 2-3x faster inference with ONNX Runtime
        """
        model.eval()

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        # Export to ONNX
        torch.onnx.export(
            model,
            sample_input,
            output_path,
            export_params=True,
            opset_version=opset_version,
            do_constant_folding=True,  # Optimize constant folding
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )

        print(f"‚úÖ ONNX model exported to: {output_path}")
        print(f"   Opset version: {opset_version}")
        print(f"   Use ONNX Runtime for optimized inference")

    @staticmethod
    def measure_inference_time(
        model: nn.Module,
        sample_input: torch.Tensor,
        num_iterations: int = 100,
        warmup_iterations: int = 10
    ) -> Dict[str, float]:
        """Measure model inference performance.

        Args:
            model: Model to benchmark
            sample_input: Sample input tensor
            num_iterations: Number of inference iterations
            warmup_iterations: Warmup iterations (excluded from timing)

        Returns:
            Performance metrics dictionary
        """
        import time

        model.eval()
        device = next(model.parameters()).device
        sample_input = sample_input.to(device)

        # Warmup
        with torch.no_grad():
            for _ in range(warmup_iterations):
                _ = model(sample_input)

        # Synchronize GPU
        if device.type == 'cuda':
            torch.cuda.synchronize()

        # Measure inference time
        times = []
        with torch.no_grad():
            for _ in range(num_iterations):
                start = time.perf_counter()
                _ = model(sample_input)

                if device.type == 'cuda':
                    torch.cuda.synchronize()

                end = time.perf_counter()
                times.append((end - start) * 1000)  # Convert to ms

        # Calculate statistics
        import statistics

        metrics = {
            "mean_ms": statistics.mean(times),
            "median_ms": statistics.median(times),
            "std_ms": statistics.stdev(times) if len(times) > 1 else 0,
            "min_ms": min(times),
            "max_ms": max(times),
            "throughput_samples_per_sec": 1000.0 / statistics.mean(times)
        }

        return metrics

    @staticmethod
    def compare_model_performance(
        original_model: nn.Module,
        optimized_model: nn.Module,
        sample_input: torch.Tensor
    ) -> Dict:
        """Compare performance between original and optimized models.

        Args:
            original_model: Original model
            optimized_model: Optimized model
            sample_input: Sample input for benchmarking

        Returns:
            Comparison results
        """
        print("Benchmarking original model...")
        original_metrics = ModelOptimizer.measure_inference_time(
            original_model,
            sample_input
        )

        print("Benchmarking optimized model...")
        optimized_metrics = ModelOptimizer.measure_inference_time(
            optimized_model,
            sample_input
        )

        # Calculate improvements
        speedup = original_metrics["mean_ms"] / optimized_metrics["mean_ms"]

        # Model size comparison
        def get_model_size_mb(model):
            param_size = sum(p.nelement() * p.element_size() for p in model.parameters())
            buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())
            return (param_size + buffer_size) / (1024 ** 2)

        original_size = get_model_size_mb(original_model)
        optimized_size = get_model_size_mb(optimized_model)
        size_reduction = original_size / optimized_size

        comparison = {
            "original": {
                "inference_time_ms": original_metrics["mean_ms"],
                "throughput": original_metrics["throughput_samples_per_sec"],
                "size_mb": original_size
            },
            "optimized": {
                "inference_time_ms": optimized_metrics["mean_ms"],
                "throughput": optimized_metrics["throughput_samples_per_sec"],
                "size_mb": optimized_size
            },
            "improvements": {
                "speedup": speedup,
                "size_reduction": size_reduction,
                "time_saved_percent": (1 - 1/speedup) * 100
            }
        }

        return comparison

    @staticmethod
    def optimize_for_deployment(
        model_path: str,
        model_class,
        output_dir: str = "models/optimized",
        techniques: list = None
    ) -> Dict[str, str]:
        """Apply multiple optimization techniques for deployment.

        Args:
            model_path: Path to trained model weights
            model_class: Model class (e.g., HybridPaperScorer)
            output_dir: Output directory for optimized models
            techniques: List of techniques to apply (default: all)

        Returns:
            Dictionary mapping technique names to output paths
        """
        if techniques is None:
            techniques = ["quantization", "pruning", "onnx"]

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load original model
        model = model_class(device=device)
        model.load_weights(model_path)
        model.eval()

        output_paths = {}
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        print("=" * 80)
        print("MODEL OPTIMIZATION FOR DEPLOYMENT")
        print("=" * 80)
        print(f"Original model: {model_path}")
        print(f"Techniques: {', '.join(techniques)}")
        print()

        # Quantization
        if "quantization" in techniques:
            print("üîß Applying quantization...")
            quantized_path = str(output_path / "quantized_model.pt")
            quantized_model = ModelOptimizer.quantize_model(
                model,
                quantized_path,
                quantization_type="dynamic"
            )
            output_paths["quantized"] = quantized_path
            print()

        # Pruning
        if "pruning" in techniques:
            print("‚úÇÔ∏è  Applying pruning...")
            pruned_path = str(output_path / "pruned_model.pt")
            pruned_model = ModelOptimizer.prune_model(
                model,
                pruning_amount=0.3,
                output_path=pruned_path
            )
            output_paths["pruned"] = pruned_path
            print()

        # ONNX Export
        if "onnx" in techniques:
            print("üì¶ Exporting to ONNX...")
            onnx_path = str(output_path / "model.onnx")

            # Create sample input (adjust dimensions as needed)
            # This is a placeholder - actual dimensions depend on model
            sample_text = "Sample paper text for export"

            # For hybrid/multitask models, we need to trace through the feature extraction
            # This is simplified - production code would need proper input preparation
            try:
                sample_input = torch.randn(1, 788).to(device)  # 788 = 768 + 20
                ModelOptimizer.export_to_onnx(
                    model.fusion if hasattr(model, 'fusion') else model,
                    sample_input,
                    onnx_path
                )
                output_paths["onnx"] = onnx_path
            except Exception as e:
                print(f"‚ö†Ô∏è  ONNX export failed: {e}")
                print("   This is expected for models with complex input preprocessing")

            print()

        print("=" * 80)
        print("OPTIMIZATION COMPLETE")
        print("=" * 80)
        print("Output models:")
        for technique, path in output_paths.items():
            print(f"  {technique}: {path}")

        return output_paths


# Utility for loading optimized models in production
class OptimizedModelLoader:
    """Helper for loading and using optimized models in production."""

    @staticmethod
    def load_quantized_model(model_path: str, model_class, device=None):
        """Load quantized model for inference.

        Args:
            model_path: Path to quantized model
            model_class: Model class
            device: Device (CPU recommended for quantized models)

        Returns:
            Loaded quantized model
        """
        device = device or torch.device("cpu")  # Quantized models work best on CPU
        model = model_class(device=device)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        return model

    @staticmethod
    def load_onnx_model(onnx_path: str):
        """Load ONNX model with ONNX Runtime.

        Args:
            onnx_path: Path to ONNX model

        Returns:
            ONNX Runtime inference session

        Note:
            Requires: pip install onnxruntime or onnxruntime-gpu
        """
        try:
            import onnxruntime as ort

            # Create inference session
            session = ort.InferenceSession(
                onnx_path,
                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
            )

            return session

        except ImportError:
            raise ImportError(
                "ONNX Runtime not installed. "
                "Install with: pip install onnxruntime"
            )
