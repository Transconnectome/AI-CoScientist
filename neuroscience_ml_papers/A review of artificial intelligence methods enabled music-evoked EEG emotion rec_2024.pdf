<!DOCTYPE html><html  lang="en" data-capo=""><head><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Frontiers | A review of artificial intelligence methods enabled music-evoked EEG emotion recognition and their applications</title>
<link rel="stylesheet" href="/ap-2024/_nuxt/entry.DQYypDUM.css">
<link rel="stylesheet" href="/ap-2024/_nuxt/vue-core.Fp3hcEis.css">
<link rel="stylesheet" href="/ap-2024/_nuxt/explainer.CMppEa5M.css">
<link rel="stylesheet" href="/ap-2024/_nuxt/ArticleDetails.mD5mgYDd.css">
<link rel="stylesheet" href="/ap-2024/_nuxt/ArticleLayoutHeader.D9gZ2sBt.css">
<link rel="stylesheet" href="/ap-2024/_nuxt/AnnouncementCard.Db7nmstV.css">
<link rel="stylesheet" href="/ap-2024/_nuxt/FloatingButtons.xuP8gC33.css">
<link rel="stylesheet" href="/ap-2024/_nuxt/ArticleEvent.D0PaxIW7.css">
<link rel="stylesheet" href="/ap-2024/_nuxt/SimilarArticles.BMee4Fk4.css">
<link rel="stylesheet" href="/ap-2024/_nuxt/ArticleTemplateBanner.CXmOJ7NH.css">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/BXwRzDm6.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/B674Mi_H.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/BXedFy5e.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/eQMj-Fph.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/wtPE5RM2.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/BUyL5Z_3.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/CxAFdcXK.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/DHv8pTSw.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/BHPWSuhB.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/x5HZk7Le.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/CNJXQ6ZR.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/Dss5ciUI.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/g1oT8bcl.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/CPRHjy1u.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/DDayk0N_.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/BlKsMhAy.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/Ttmv6dO3.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/v_uhO3_6.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/CDht2Axh.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/VtxHbjd2.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/Df210yKO.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/L0bfV2sU.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/D01RgZAA.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/DrK18lk-.js">
<link rel="modulepreload" as="script" crossorigin href="/ap-2024/_nuxt/Cno1pHUS.js">
<link rel="prefetch" as="script" crossorigin href="/ap-2024/_nuxt/Dlz5g2Id.js">
<link rel="prefetch" as="style" crossorigin href="/ap-2024/_nuxt/ArticleHubLayout.q6CU8_bN.css">
<link rel="prefetch" as="script" crossorigin href="/ap-2024/_nuxt/GtA5jEuR.js">
<link rel="prefetch" as="script" crossorigin href="/ap-2024/_nuxt/D1ZTLht9.js">
<link rel="prefetch" as="script" crossorigin href="/ap-2024/_nuxt/GWtHKqLR.js">
<link rel="prefetch" as="script" crossorigin href="/ap-2024/_nuxt/Bfcx7IzU.js">
<meta name="theme-color" content="#0C4DED">
<meta name="mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" data-hid="4cd079c">window.NREUM||(NREUM={});NREUM.info = {"agent":"","beacon":"bam.nr-data.net","errorBeacon":"bam.nr-data.net","licenseKey":"598a124f17","applicationID":"586843029","agentToken":null,"applicationTime":4.232187,"transactionName":"MQcDMkECCkNSW0YMWghNLwlBDgVcWkJXAWAUC04MXBYWXlJUQUpbAxcTCUAADVVdW1dKVBQWCAVfBhcfGRdUC1wIEU9UA1FQHgIMAlUBUlZOAEYPCA==","queueTime":0,"ttGuid":"fa559626aacbc33b"}; (window.NREUM||(NREUM={})).init={privacy:{cookies_enabled:true},ajax:{deny_list:["bam.nr-data.net"]},distributed_tracing:{enabled:true}};(window.NREUM||(NREUM={})).loader_config={agentID:"594460232",accountID:"230385",trustKey:"230385",xpid:"VgUHUl5WGwYIUllWBAEFXw==",licenseKey:"598a124f17",applicationID:"586843029"};;/*! For license information please see nr-loader-spa-1.298.0.min.js.LICENSE.txt */
(()=>{var e,t,r={8122:(e,t,r)=>{"use strict";r.d(t,{a:()=>i});var n=r(944);function i(e,t){try{if(!e||"object"!=typeof e)return(0,n.R)(3);if(!t||"object"!=typeof t)return(0,n.R)(4);const r=Object.create(Object.getPrototypeOf(t),Object.getOwnPropertyDescriptors(t)),o=0===Object.keys(r).length?e:r;for(let a in o)if(void 0!==e[a])try{if(null===e[a]){r[a]=null;continue}Array.isArray(e[a])&&Array.isArray(t[a])?r[a]=Array.from(new Set([...e[a],...t[a]])):"object"==typeof e[a]&&"object"==typeof t[a]?r[a]=i(e[a],t[a]):r[a]=e[a]}catch(e){r[a]||(0,n.R)(1,e)}return r}catch(e){(0,n.R)(2,e)}}},2555:(e,t,r)=>{"use strict";r.d(t,{D:()=>s,f:()=>a});var n=r(384),i=r(8122);const o={beacon:n.NT.beacon,errorBeacon:n.NT.errorBeacon,licenseKey:void 0,applicationID:void 0,sa:void 0,queueTime:void 0,applicationTime:void 0,ttGuid:void 0,user:void 0,account:void 0,product:void 0,extra:void 0,jsAttributes:{},userAttributes:void 0,atts:void 0,transactionName:void 0,tNamePlain:void 0};function a(e){try{return!!e.licenseKey&&!!e.errorBeacon&&!!e.applicationID}catch(e){return!1}}const s=e=>(0,i.a)(e,o)},7699:(e,t,r)=>{"use strict";r.d(t,{It:()=>i,No:()=>n,qh:()=>a,uh:()=>o});const n=16e3,i=1e6,o="NR_CONTAINER_AGENT",a="SESSION_ERROR"},9324:(e,t,r)=>{"use strict";r.d(t,{F3:()=>i,Xs:()=>o,Yq:()=>a,xv:()=>n});const n="1.298.0",i="PROD",o="CDN",a="^2.0.0-alpha.18"},6154:(e,t,r)=>{"use strict";r.d(t,{A4:()=>s,OF:()=>d,RI:()=>i,WN:()=>h,bv:()=>o,gm:()=>a,lR:()=>f,m:()=>u,mw:()=>c,sb:()=>l});var n=r(1863);const i="undefined"!=typeof window&&!!window.document,o="undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self.navigator instanceof WorkerNavigator||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis.navigator instanceof WorkerNavigator),a=i?window:"undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis),s="complete"===a?.document?.readyState,c=Boolean("hidden"===a?.document?.visibilityState),u=""+a?.location,d=/iPad|iPhone|iPod/.test(a.navigator?.userAgent),l=d&&"undefined"==typeof SharedWorker,f=(()=>{const e=a.navigator?.userAgent?.match(/Firefox[/\s](\d+\.\d+)/);return Array.isArray(e)&&e.length>=2?+e[1]:0})(),h=Date.now()-(0,n.t)()},7295:(e,t,r)=>{"use strict";r.d(t,{Xv:()=>a,gX:()=>i,iW:()=>o});var n=[];function i(e){if(!e||o(e))return!1;if(0===n.length)return!0;for(var t=0;t<n.length;t++){var r=n[t];if("*"===r.hostname)return!1;if(s(r.hostname,e.hostname)&&c(r.pathname,e.pathname))return!1}return!0}function o(e){return void 0===e.hostname}function a(e){if(n=[],e&&e.length)for(var t=0;t<e.length;t++){let r=e[t];if(!r)continue;0===r.indexOf("http://")?r=r.substring(7):0===r.indexOf("https://")&&(r=r.substring(8));const i=r.indexOf("/");let o,a;i>0?(o=r.substring(0,i),a=r.substring(i)):(o=r,a="");let[s]=o.split(":");n.push({hostname:s,pathname:a})}}function s(e,t){return!(e.length>t.length)&&t.indexOf(e)===t.length-e.length}function c(e,t){return 0===e.indexOf("/")&&(e=e.substring(1)),0===t.indexOf("/")&&(t=t.substring(1)),""===e||e===t}},3241:(e,t,r)=>{"use strict";r.d(t,{W:()=>o});var n=r(6154);const i="newrelic";function o(e={}){try{n.gm.dispatchEvent(new CustomEvent(i,{detail:e}))}catch(e){}}},1687:(e,t,r)=>{"use strict";r.d(t,{Ak:()=>u,Ze:()=>f,x3:()=>d});var n=r(3241),i=r(7836),o=r(3606),a=r(860),s=r(2646);const c={};function u(e,t){const r={staged:!1,priority:a.P3[t]||0};l(e),c[e].get(t)||c[e].set(t,r)}function d(e,t){e&&c[e]&&(c[e].get(t)&&c[e].delete(t),p(e,t,!1),c[e].size&&h(e))}function l(e){if(!e)throw new Error("agentIdentifier required");c[e]||(c[e]=new Map)}function f(e="",t="feature",r=!1){if(l(e),!e||!c[e].get(t)||r)return p(e,t);c[e].get(t).staged=!0,h(e)}function h(e){const t=Array.from(c[e]);t.every((([e,t])=>t.staged))&&(t.sort(((e,t)=>e[1].priority-t[1].priority)),t.forEach((([t])=>{c[e].delete(t),p(e,t)})))}function p(e,t,r=!0){const a=e?i.ee.get(e):i.ee,c=o.i.handlers;if(!a.aborted&&a.backlog&&c){if((0,n.W)({agentIdentifier:e,type:"lifecycle",name:"drain",feature:t}),r){const e=a.backlog[t],r=c[t];if(r){for(let t=0;e&&t<e.length;++t)g(e[t],r);Object.entries(r).forEach((([e,t])=>{Object.values(t||{}).forEach((t=>{t[0]?.on&&t[0]?.context()instanceof s.y&&t[0].on(e,t[1])}))}))}}a.isolatedBacklog||delete c[t],a.backlog[t]=null,a.emit("drain-"+t,[])}}function g(e,t){var r=e[1];Object.values(t[r]||{}).forEach((t=>{var r=e[0];if(t[0]===r){var n=t[1],i=e[3],o=e[2];n.apply(i,o)}}))}},7836:(e,t,r)=>{"use strict";r.d(t,{P:()=>s,ee:()=>c});var n=r(384),i=r(8990),o=r(2646),a=r(5607);const s="nr@context:".concat(a.W),c=function e(t,r){var n={},a={},d={},l=!1;try{l=16===r.length&&u.initializedAgents?.[r]?.runtime.isolatedBacklog}catch(e){}var f={on:p,addEventListener:p,removeEventListener:function(e,t){var r=n[e];if(!r)return;for(var i=0;i<r.length;i++)r[i]===t&&r.splice(i,1)},emit:function(e,r,n,i,o){!1!==o&&(o=!0);if(c.aborted&&!i)return;t&&o&&t.emit(e,r,n);var s=h(n);g(e).forEach((e=>{e.apply(s,r)}));var u=v()[a[e]];u&&u.push([f,e,r,s]);return s},get:m,listeners:g,context:h,buffer:function(e,t){const r=v();if(t=t||"feature",f.aborted)return;Object.entries(e||{}).forEach((([e,n])=>{a[n]=t,t in r||(r[t]=[])}))},abort:function(){f._aborted=!0,Object.keys(f.backlog).forEach((e=>{delete f.backlog[e]}))},isBuffering:function(e){return!!v()[a[e]]},debugId:r,backlog:l?{}:t&&"object"==typeof t.backlog?t.backlog:{},isolatedBacklog:l};return Object.defineProperty(f,"aborted",{get:()=>{let e=f._aborted||!1;return e||(t&&(e=t.aborted),e)}}),f;function h(e){return e&&e instanceof o.y?e:e?(0,i.I)(e,s,(()=>new o.y(s))):new o.y(s)}function p(e,t){n[e]=g(e).concat(t)}function g(e){return n[e]||[]}function m(t){return d[t]=d[t]||e(f,t)}function v(){return f.backlog}}(void 0,"globalEE"),u=(0,n.Zm)();u.ee||(u.ee=c)},2646:(e,t,r)=>{"use strict";r.d(t,{y:()=>n});class n{constructor(e){this.contextId=e}}},9908:(e,t,r)=>{"use strict";r.d(t,{d:()=>n,p:()=>i});var n=r(7836).ee.get("handle");function i(e,t,r,i,o){o?(o.buffer([e],i),o.emit(e,t,r)):(n.buffer([e],i),n.emit(e,t,r))}},3606:(e,t,r)=>{"use strict";r.d(t,{i:()=>o});var n=r(9908);o.on=a;var i=o.handlers={};function o(e,t,r,o){a(o||n.d,i,e,t,r)}function a(e,t,r,i,o){o||(o="feature"),e||(e=n.d);var a=t[o]=t[o]||{};(a[r]=a[r]||[]).push([e,i])}},3878:(e,t,r)=>{"use strict";function n(e,t){return{capture:e,passive:!1,signal:t}}function i(e,t,r=!1,i){window.addEventListener(e,t,n(r,i))}function o(e,t,r=!1,i){document.addEventListener(e,t,n(r,i))}r.d(t,{DD:()=>o,jT:()=>n,sp:()=>i})},5607:(e,t,r)=>{"use strict";r.d(t,{W:()=>n});const n=(0,r(9566).bz)()},9566:(e,t,r)=>{"use strict";r.d(t,{LA:()=>s,ZF:()=>c,bz:()=>a,el:()=>u});var n=r(6154);const i="xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";function o(e,t){return e?15&e[t]:16*Math.random()|0}function a(){const e=n.gm?.crypto||n.gm?.msCrypto;let t,r=0;return e&&e.getRandomValues&&(t=e.getRandomValues(new Uint8Array(30))),i.split("").map((e=>"x"===e?o(t,r++).toString(16):"y"===e?(3&o()|8).toString(16):e)).join("")}function s(e){const t=n.gm?.crypto||n.gm?.msCrypto;let r,i=0;t&&t.getRandomValues&&(r=t.getRandomValues(new Uint8Array(e)));const a=[];for(var s=0;s<e;s++)a.push(o(r,i++).toString(16));return a.join("")}function c(){return s(16)}function u(){return s(32)}},2614:(e,t,r)=>{"use strict";r.d(t,{BB:()=>a,H3:()=>n,g:()=>u,iL:()=>c,tS:()=>s,uh:()=>i,wk:()=>o});const n="NRBA",i="SESSION",o=144e5,a=18e5,s={STARTED:"session-started",PAUSE:"session-pause",RESET:"session-reset",RESUME:"session-resume",UPDATE:"session-update"},c={SAME_TAB:"same-tab",CROSS_TAB:"cross-tab"},u={OFF:0,FULL:1,ERROR:2}},1863:(e,t,r)=>{"use strict";function n(){return Math.floor(performance.now())}r.d(t,{t:()=>n})},7485:(e,t,r)=>{"use strict";r.d(t,{D:()=>i});var n=r(6154);function i(e){if(0===(e||"").indexOf("data:"))return{protocol:"data"};try{const t=new URL(e,location.href),r={port:t.port,hostname:t.hostname,pathname:t.pathname,search:t.search,protocol:t.protocol.slice(0,t.protocol.indexOf(":")),sameOrigin:t.protocol===n.gm?.location?.protocol&&t.host===n.gm?.location?.host};return r.port&&""!==r.port||("http:"===t.protocol&&(r.port="80"),"https:"===t.protocol&&(r.port="443")),r.pathname&&""!==r.pathname?r.pathname.startsWith("/")||(r.pathname="/".concat(r.pathname)):r.pathname="/",r}catch(e){return{}}}},944:(e,t,r)=>{"use strict";r.d(t,{R:()=>i});var n=r(3241);function i(e,t){"function"==typeof console.debug&&(console.debug("New Relic Warning: https://github.com/newrelic/newrelic-browser-agent/blob/main/docs/warning-codes.md#".concat(e),t),(0,n.W)({agentIdentifier:null,drained:null,type:"data",name:"warn",feature:"warn",data:{code:e,secondary:t}}))}},5701:(e,t,r)=>{"use strict";r.d(t,{B:()=>o,t:()=>a});var n=r(3241);const i=new Set,o={};function a(e,t){const r=t.agentIdentifier;o[r]??={},e&&"object"==typeof e&&(i.has(r)||(t.ee.emit("rumresp",[e]),o[r]=e,i.add(r),(0,n.W)({agentIdentifier:r,loaded:!0,drained:!0,type:"lifecycle",name:"load",feature:void 0,data:e})))}},8990:(e,t,r)=>{"use strict";r.d(t,{I:()=>i});var n=Object.prototype.hasOwnProperty;function i(e,t,r){if(n.call(e,t))return e[t];var i=r();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,t,{value:i,writable:!0,enumerable:!1}),i}catch(e){}return e[t]=i,i}},6389:(e,t,r)=>{"use strict";function n(e,t=500,r={}){const n=r?.leading||!1;let i;return(...r)=>{n&&void 0===i&&(e.apply(this,r),i=setTimeout((()=>{i=clearTimeout(i)}),t)),n||(clearTimeout(i),i=setTimeout((()=>{e.apply(this,r)}),t))}}function i(e){let t=!1;return(...r)=>{t||(t=!0,e.apply(this,r))}}r.d(t,{J:()=>i,s:()=>n})},1910:(e,t,r)=>{"use strict";r.d(t,{i:()=>o});var n=r(944);const i=new Map;function o(...e){return e.every((e=>{if(i.has(e))return i.get(e);const t="function"==typeof e&&e.toString().includes("[native code]");return t||(0,n.R)(64,e?.name||e?.toString()),i.set(e,t),t}))}},3304:(e,t,r)=>{"use strict";r.d(t,{A:()=>o});var n=r(7836);const i=()=>{const e=new WeakSet;return(t,r)=>{if("object"==typeof r&&null!==r){if(e.has(r))return;e.add(r)}return r}};function o(e){try{return JSON.stringify(e,i())??""}catch(e){try{n.ee.emit("internal-error",[e])}catch(e){}return""}}},3496:(e,t,r)=>{"use strict";function n(e){return!e||!(!e.licenseKey||!e.applicationID)}function i(e,t){return!e||e.licenseKey===t.info.licenseKey&&e.applicationID===t.info.applicationID}r.d(t,{A:()=>i,I:()=>n})},5289:(e,t,r)=>{"use strict";r.d(t,{GG:()=>o,Qr:()=>s,sB:()=>a});var n=r(3878);function i(){return"undefined"==typeof document||"complete"===document.readyState}function o(e,t){if(i())return e();(0,n.sp)("load",e,t)}function a(e){if(i())return e();(0,n.DD)("DOMContentLoaded",e)}function s(e){if(i())return e();(0,n.sp)("popstate",e)}},384:(e,t,r)=>{"use strict";r.d(t,{NT:()=>a,US:()=>d,Zm:()=>s,bQ:()=>u,dV:()=>c,pV:()=>l});var n=r(6154),i=r(1863),o=r(1910);const a={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net"};function s(){return n.gm.NREUM||(n.gm.NREUM={}),void 0===n.gm.newrelic&&(n.gm.newrelic=n.gm.NREUM),n.gm.NREUM}function c(){let e=s();return e.o||(e.o={ST:n.gm.setTimeout,SI:n.gm.setImmediate||n.gm.setInterval,CT:n.gm.clearTimeout,XHR:n.gm.XMLHttpRequest,REQ:n.gm.Request,EV:n.gm.Event,PR:n.gm.Promise,MO:n.gm.MutationObserver,FETCH:n.gm.fetch,WS:n.gm.WebSocket},(0,o.i)(...Object.values(e.o))),e}function u(e,t){let r=s();r.initializedAgents??={},t.initializedAt={ms:(0,i.t)(),date:new Date},r.initializedAgents[e]=t}function d(e,t){s()[e]=t}function l(){return function(){let e=s();const t=e.info||{};e.info={beacon:a.beacon,errorBeacon:a.errorBeacon,...t}}(),function(){let e=s();const t=e.init||{};e.init={...t}}(),c(),function(){let e=s();const t=e.loader_config||{};e.loader_config={...t}}(),s()}},2843:(e,t,r)=>{"use strict";r.d(t,{u:()=>i});var n=r(3878);function i(e,t=!1,r,i){(0,n.DD)("visibilitychange",(function(){if(t)return void("hidden"===document.visibilityState&&e());e(document.visibilityState)}),r,i)}},8139:(e,t,r)=>{"use strict";r.d(t,{u:()=>f});var n=r(7836),i=r(3434),o=r(8990),a=r(6154);const s={},c=a.gm.XMLHttpRequest,u="addEventListener",d="removeEventListener",l="nr@wrapped:".concat(n.P);function f(e){var t=function(e){return(e||n.ee).get("events")}(e);if(s[t.debugId]++)return t;s[t.debugId]=1;var r=(0,i.YM)(t,!0);function f(e){r.inPlace(e,[u,d],"-",p)}function p(e,t){return e[1]}return"getPrototypeOf"in Object&&(a.RI&&h(document,f),c&&h(c.prototype,f),h(a.gm,f)),t.on(u+"-start",(function(e,t){var n=e[1];if(null!==n&&("function"==typeof n||"object"==typeof n)&&"newrelic"!==e[0]){var i=(0,o.I)(n,l,(function(){var e={object:function(){if("function"!=typeof n.handleEvent)return;return n.handleEvent.apply(n,arguments)},function:n}[typeof n];return e?r(e,"fn-",null,e.name||"anonymous"):n}));this.wrapped=e[1]=i}})),t.on(d+"-start",(function(e){e[1]=this.wrapped||e[1]})),t}function h(e,t,...r){let n=e;for(;"object"==typeof n&&!Object.prototype.hasOwnProperty.call(n,u);)n=Object.getPrototypeOf(n);n&&t(n,...r)}},3434:(e,t,r)=>{"use strict";r.d(t,{Jt:()=>o,YM:()=>u});var n=r(7836),i=r(5607);const o="nr@original:".concat(i.W),a=50;var s=Object.prototype.hasOwnProperty,c=!1;function u(e,t){return e||(e=n.ee),r.inPlace=function(e,t,n,i,o){n||(n="");const a="-"===n.charAt(0);for(let s=0;s<t.length;s++){const c=t[s],u=e[c];l(u)||(e[c]=r(u,a?c+n:n,i,c,o))}},r.flag=o,r;function r(t,r,n,c,u){return l(t)?t:(r||(r=""),nrWrapper[o]=t,function(e,t,r){if(Object.defineProperty&&Object.keys)try{return Object.keys(e).forEach((function(r){Object.defineProperty(t,r,{get:function(){return e[r]},set:function(t){return e[r]=t,t}})})),t}catch(e){d([e],r)}for(var n in e)s.call(e,n)&&(t[n]=e[n])}(t,nrWrapper,e),nrWrapper);function nrWrapper(){var o,s,l,f;let h;try{s=this,o=[...arguments],l="function"==typeof n?n(o,s):n||{}}catch(t){d([t,"",[o,s,c],l],e)}i(r+"start",[o,s,c],l,u);const p=performance.now();let g;try{return f=t.apply(s,o),g=performance.now(),f}catch(e){throw g=performance.now(),i(r+"err",[o,s,e],l,u),h=e,h}finally{const e=g-p,t={start:p,end:g,duration:e,isLongTask:e>=a,methodName:c,thrownError:h};t.isLongTask&&i("long-task",[t,s],l,u),i(r+"end",[o,s,f],l,u)}}}function i(r,n,i,o){if(!c||t){var a=c;c=!0;try{e.emit(r,n,i,t,o)}catch(t){d([t,r,n,i],e)}c=a}}}function d(e,t){t||(t=n.ee);try{t.emit("internal-error",e)}catch(e){}}function l(e){return!(e&&"function"==typeof e&&e.apply&&!e[o])}},9300:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K7.ajax},3333:(e,t,r)=>{"use strict";r.d(t,{$v:()=>u,TZ:()=>n,Zp:()=>i,kd:()=>c,mq:()=>s,nf:()=>a,qN:()=>o});const n=r(860).K7.genericEvents,i=["auxclick","click","copy","keydown","paste","scrollend"],o=["focus","blur"],a=4,s=1e3,c=["PageAction","UserAction","BrowserPerformance"],u={MARKS:"experimental.marks",MEASURES:"experimental.measures",RESOURCES:"experimental.resources"}},6774:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K7.jserrors},993:(e,t,r)=>{"use strict";r.d(t,{A$:()=>o,ET:()=>a,TZ:()=>s,p_:()=>i});var n=r(860);const i={ERROR:"ERROR",WARN:"WARN",INFO:"INFO",DEBUG:"DEBUG",TRACE:"TRACE"},o={OFF:0,ERROR:1,WARN:2,INFO:3,DEBUG:4,TRACE:5},a="log",s=n.K7.logging},3785:(e,t,r)=>{"use strict";r.d(t,{R:()=>c,b:()=>u});var n=r(9908),i=r(1863),o=r(860),a=r(8154),s=r(993);function c(e,t,r={},c=s.p_.INFO,u,d=(0,i.t)()){(0,n.p)(a.xV,["API/logging/".concat(c.toLowerCase(),"/called")],void 0,o.K7.metrics,e),(0,n.p)(s.ET,[d,t,r,c,u],void 0,o.K7.logging,e)}function u(e){return"string"==typeof e&&Object.values(s.p_).some((t=>t===e.toUpperCase().trim()))}},8154:(e,t,r)=>{"use strict";r.d(t,{z_:()=>o,XG:()=>s,TZ:()=>n,rs:()=>i,xV:()=>a});r(6154),r(9566),r(384);const n=r(860).K7.metrics,i="sm",o="cm",a="storeSupportabilityMetrics",s="storeEventMetrics"},6630:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K7.pageViewEvent},782:(e,t,r)=>{"use strict";r.d(t,{T:()=>n});const n=r(860).K7.pageViewTiming},6344:(e,t,r)=>{"use strict";r.d(t,{BB:()=>d,G4:()=>o,Qb:()=>l,TZ:()=>i,Ug:()=>a,_s:()=>s,bc:()=>u,yP:()=>c});var n=r(2614);const i=r(860).K7.sessionReplay,o={RECORD:"recordReplay",PAUSE:"pauseReplay",ERROR_DURING_REPLAY:"errorDuringReplay"},a=.12,s={DomContentLoaded:0,Load:1,FullSnapshot:2,IncrementalSnapshot:3,Meta:4,Custom:5},c={[n.g.ERROR]:15e3,[n.g.FULL]:3e5,[n.g.OFF]:0},u={RESET:{message:"Session was reset",sm:"Reset"},IMPORT:{message:"Recorder failed to import",sm:"Import"},TOO_MANY:{message:"429: Too Many Requests",sm:"Too-Many"},TOO_BIG:{message:"Payload was too large",sm:"Too-Big"},CROSS_TAB:{message:"Session Entity was set to OFF on another tab",sm:"Cross-Tab"},ENTITLEMENTS:{message:"Session Replay is not allowed and will not be started",sm:"Entitlement"}},d=5e3,l={API:"api",RESUME:"resume",SWITCH_TO_FULL:"switchToFull",INITIALIZE:"initialize",PRELOAD:"preload"}},5270:(e,t,r)=>{"use strict";r.d(t,{Aw:()=>a,SR:()=>o,rF:()=>s});var n=r(384),i=r(7767);function o(e){return!!(0,n.dV)().o.MO&&(0,i.V)(e)&&!0===e?.session_trace.enabled}function a(e){return!0===e?.session_replay.preload&&o(e)}function s(e,t){try{if("string"==typeof t?.type){if("password"===t.type.toLowerCase())return"*".repeat(e?.length||0);if(void 0!==t?.dataset?.nrUnmask||t?.classList?.contains("nr-unmask"))return e}}catch(e){}return"string"==typeof e?e.replace(/[\S]/g,"*"):"*".repeat(e?.length||0)}},3738:(e,t,r)=>{"use strict";r.d(t,{He:()=>i,Kp:()=>s,Lc:()=>u,Rz:()=>d,TZ:()=>n,bD:()=>o,d3:()=>a,jx:()=>l,sl:()=>f,uP:()=>c});const n=r(860).K7.sessionTrace,i="bstResource",o="resource",a="-start",s="-end",c="fn"+a,u="fn"+s,d="pushState",l=1e3,f=3e4},3962:(e,t,r)=>{"use strict";r.d(t,{AM:()=>a,O2:()=>l,OV:()=>o,Qu:()=>f,TZ:()=>c,ih:()=>h,pP:()=>s,t1:()=>d,tC:()=>i,wD:()=>u});var n=r(860);const i=["click","keydown","submit"],o="popstate",a="api",s="initialPageLoad",c=n.K7.softNav,u=5e3,d=500,l={INITIAL_PAGE_LOAD:"",ROUTE_CHANGE:1,UNSPECIFIED:2},f={INTERACTION:1,AJAX:2,CUSTOM_END:3,CUSTOM_TRACER:4},h={IP:"in progress",PF:"pending finish",FIN:"finished",CAN:"cancelled"}},7378:(e,t,r)=>{"use strict";r.d(t,{$p:()=>x,BR:()=>b,Kp:()=>R,L3:()=>y,Lc:()=>c,NC:()=>o,SG:()=>d,TZ:()=>i,U6:()=>p,UT:()=>m,d3:()=>w,dT:()=>f,e5:()=>E,gx:()=>v,l9:()=>l,oW:()=>h,op:()=>g,rw:()=>u,tH:()=>A,uP:()=>s,wW:()=>T,xq:()=>a});var n=r(384);const i=r(860).K7.spa,o=["click","submit","keypress","keydown","keyup","change"],a=999,s="fn-start",c="fn-end",u="cb-start",d="api-ixn-",l="remaining",f="interaction",h="spaNode",p="jsonpNode",g="fetch-start",m="fetch-done",v="fetch-body-",b="jsonp-end",y=(0,n.dV)().o.ST,w="-start",R="-end",x="-body",T="cb"+R,E="jsTime",A="fetch"},4234:(e,t,r)=>{"use strict";r.d(t,{W:()=>o});var n=r(7836),i=r(1687);class o{constructor(e,t){this.agentIdentifier=e,this.ee=n.ee.get(e),this.featureName=t,this.blocked=!1}deregisterDrain(){(0,i.x3)(this.agentIdentifier,this.featureName)}}},7767:(e,t,r)=>{"use strict";r.d(t,{V:()=>i});var n=r(6154);const i=e=>n.RI&&!0===e?.privacy.cookies_enabled},1741:(e,t,r)=>{"use strict";r.d(t,{W:()=>o});var n=r(944),i=r(4261);class o{#e(e,...t){if(this[e]!==o.prototype[e])return this[e](...t);(0,n.R)(35,e)}addPageAction(e,t){return this.#e(i.hG,e,t)}register(e){return this.#e(i.eY,e)}recordCustomEvent(e,t){return this.#e(i.fF,e,t)}setPageViewName(e,t){return this.#e(i.Fw,e,t)}setCustomAttribute(e,t,r){return this.#e(i.cD,e,t,r)}noticeError(e,t){return this.#e(i.o5,e,t)}setUserId(e){return this.#e(i.Dl,e)}setApplicationVersion(e){return this.#e(i.nb,e)}setErrorHandler(e){return this.#e(i.bt,e)}addRelease(e,t){return this.#e(i.k6,e,t)}log(e,t){return this.#e(i.$9,e,t)}start(){return this.#e(i.d3)}finished(e){return this.#e(i.BL,e)}recordReplay(){return this.#e(i.CH)}pauseReplay(){return this.#e(i.Tb)}addToTrace(e){return this.#e(i.U2,e)}setCurrentRouteName(e){return this.#e(i.PA,e)}interaction(){return this.#e(i.dT)}wrapLogger(e,t,r){return this.#e(i.Wb,e,t,r)}measure(e,t){return this.#e(i.V1,e,t)}}},4261:(e,t,r)=>{"use strict";r.d(t,{$9:()=>d,BL:()=>c,CH:()=>p,Dl:()=>R,Fw:()=>w,PA:()=>v,Pl:()=>n,Tb:()=>f,U2:()=>a,V1:()=>E,Wb:()=>T,bt:()=>y,cD:()=>b,d3:()=>x,dT:()=>u,eY:()=>g,fF:()=>h,hG:()=>o,hw:()=>i,k6:()=>s,nb:()=>m,o5:()=>l});const n="api-",i=n+"ixn-",o="addPageAction",a="addToTrace",s="addRelease",c="finished",u="interaction",d="log",l="noticeError",f="pauseReplay",h="recordCustomEvent",p="recordReplay",g="register",m="setApplicationVersion",v="setCurrentRouteName",b="setCustomAttribute",y="setErrorHandler",w="setPageViewName",R="setUserId",x="start",T="wrapLogger",E="measure"},5205:(e,t,r)=>{"use strict";r.d(t,{j:()=>S});var n=r(384),i=r(1741);var o=r(2555),a=r(3333);const s=e=>{if(!e||"string"!=typeof e)return!1;try{document.createDocumentFragment().querySelector(e)}catch{return!1}return!0};var c=r(2614),u=r(944),d=r(8122);const l="[data-nr-mask]",f=e=>(0,d.a)(e,(()=>{const e={feature_flags:[],experimental:{marks:!1,measures:!1,resources:!1},mask_selector:"*",block_selector:"[data-nr-block]",mask_input_options:{color:!1,date:!1,"datetime-local":!1,email:!1,month:!1,number:!1,range:!1,search:!1,tel:!1,text:!1,time:!1,url:!1,week:!1,textarea:!1,select:!1,password:!0}};return{ajax:{deny_list:void 0,block_internal:!0,enabled:!0,autoStart:!0},api:{allow_registered_children:!0,duplicate_registered_data:!1},distributed_tracing:{enabled:void 0,exclude_newrelic_header:void 0,cors_use_newrelic_header:void 0,cors_use_tracecontext_headers:void 0,allowed_origins:void 0},get feature_flags(){return e.feature_flags},set feature_flags(t){e.feature_flags=t},generic_events:{enabled:!0,autoStart:!0},harvest:{interval:30},jserrors:{enabled:!0,autoStart:!0},logging:{enabled:!0,autoStart:!0},metrics:{enabled:!0,autoStart:!0},obfuscate:void 0,page_action:{enabled:!0},page_view_event:{enabled:!0,autoStart:!0},page_view_timing:{enabled:!0,autoStart:!0},performance:{get capture_marks(){return e.feature_flags.includes(a.$v.MARKS)||e.experimental.marks},set capture_marks(t){e.experimental.marks=t},get capture_measures(){return e.feature_flags.includes(a.$v.MEASURES)||e.experimental.measures},set capture_measures(t){e.experimental.measures=t},capture_detail:!0,resources:{get enabled(){return e.feature_flags.includes(a.$v.RESOURCES)||e.experimental.resources},set enabled(t){e.experimental.resources=t},asset_types:[],first_party_domains:[],ignore_newrelic:!0}},privacy:{cookies_enabled:!0},proxy:{assets:void 0,beacon:void 0},session:{expiresMs:c.wk,inactiveMs:c.BB},session_replay:{autoStart:!0,enabled:!1,preload:!1,sampling_rate:10,error_sampling_rate:100,collect_fonts:!1,inline_images:!1,fix_stylesheets:!0,mask_all_inputs:!0,get mask_text_selector(){return e.mask_selector},set mask_text_selector(t){s(t)?e.mask_selector="".concat(t,",").concat(l):""===t||null===t?e.mask_selector=l:(0,u.R)(5,t)},get block_class(){return"nr-block"},get ignore_class(){return"nr-ignore"},get mask_text_class(){return"nr-mask"},get block_selector(){return e.block_selector},set block_selector(t){s(t)?e.block_selector+=",".concat(t):""!==t&&(0,u.R)(6,t)},get mask_input_options(){return e.mask_input_options},set mask_input_options(t){t&&"object"==typeof t?e.mask_input_options={...t,password:!0}:(0,u.R)(7,t)}},session_trace:{enabled:!0,autoStart:!0},soft_navigations:{enabled:!0,autoStart:!0},spa:{enabled:!0,autoStart:!0},ssl:void 0,user_actions:{enabled:!0,elementAttributes:["id","className","tagName","type"]}}})());var h=r(6154),p=r(9324);let g=0;const m={buildEnv:p.F3,distMethod:p.Xs,version:p.xv,originTime:h.WN},v={appMetadata:{},customTransaction:void 0,denyList:void 0,disabled:!1,entityManager:void 0,harvester:void 0,isolatedBacklog:!1,isRecording:!1,loaderType:void 0,maxBytes:3e4,obfuscator:void 0,onerror:void 0,ptid:void 0,releaseIds:{},session:void 0,timeKeeper:void 0,jsAttributesMetadata:{bytes:0},get harvestCount(){return++g}},b=e=>{const t=(0,d.a)(e,v),r=Object.keys(m).reduce(((e,t)=>(e[t]={value:m[t],writable:!1,configurable:!0,enumerable:!0},e)),{});return Object.defineProperties(t,r)};var y=r(5701);const w=e=>{const t=e.startsWith("http");e+="/",r.p=t?e:"https://"+e};var R=r(7836),x=r(3241);const T={accountID:void 0,trustKey:void 0,agentID:void 0,licenseKey:void 0,applicationID:void 0,xpid:void 0},E=e=>(0,d.a)(e,T),A=new Set;function S(e,t={},r,a){let{init:s,info:c,loader_config:u,runtime:d={},exposed:l=!0}=t;if(!c){const e=(0,n.pV)();s=e.init,c=e.info,u=e.loader_config}e.init=f(s||{}),e.loader_config=E(u||{}),c.jsAttributes??={},h.bv&&(c.jsAttributes.isWorker=!0),e.info=(0,o.D)(c);const p=e.init,g=[c.beacon,c.errorBeacon];A.has(e.agentIdentifier)||(p.proxy.assets&&(w(p.proxy.assets),g.push(p.proxy.assets)),p.proxy.beacon&&g.push(p.proxy.beacon),function(e){const t=(0,n.pV)();Object.getOwnPropertyNames(i.W.prototype).forEach((r=>{const n=i.W.prototype[r];if("function"!=typeof n||"constructor"===n)return;let o=t[r];e[r]&&!1!==e.exposed&&"micro-agent"!==e.runtime?.loaderType&&(t[r]=(...t)=>{const n=e[r](...t);return o?o(...t):n})}))}(e),(0,n.US)("activatedFeatures",y.B),e.runSoftNavOverSpa&&=!0===p.soft_navigations.enabled&&p.feature_flags.includes("soft_nav")),d.denyList=[...p.ajax.deny_list||[],...p.ajax.block_internal?g:[]],d.ptid=e.agentIdentifier,d.loaderType=r,e.runtime=b(d),A.has(e.agentIdentifier)||(e.ee=R.ee.get(e.agentIdentifier),e.exposed=l,(0,x.W)({agentIdentifier:e.agentIdentifier,drained:!!y.B?.[e.agentIdentifier],type:"lifecycle",name:"initialize",feature:void 0,data:e.config})),A.add(e.agentIdentifier)}},8374:(e,t,r)=>{r.nc=(()=>{try{return document?.currentScript?.nonce}catch(e){}return""})()},860:(e,t,r)=>{"use strict";r.d(t,{$J:()=>d,K7:()=>c,P3:()=>u,XX:()=>i,Yy:()=>s,df:()=>o,qY:()=>n,v4:()=>a});const n="events",i="jserrors",o="browser/blobs",a="rum",s="browser/logs",c={ajax:"ajax",genericEvents:"generic_events",jserrors:i,logging:"logging",metrics:"metrics",pageAction:"page_action",pageViewEvent:"page_view_event",pageViewTiming:"page_view_timing",sessionReplay:"session_replay",sessionTrace:"session_trace",softNav:"soft_navigations",spa:"spa"},u={[c.pageViewEvent]:1,[c.pageViewTiming]:2,[c.metrics]:3,[c.jserrors]:4,[c.spa]:5,[c.ajax]:6,[c.sessionTrace]:7,[c.softNav]:8,[c.sessionReplay]:9,[c.logging]:10,[c.genericEvents]:11},d={[c.pageViewEvent]:a,[c.pageViewTiming]:n,[c.ajax]:n,[c.spa]:n,[c.softNav]:n,[c.metrics]:i,[c.jserrors]:i,[c.sessionTrace]:o,[c.sessionReplay]:o,[c.logging]:s,[c.genericEvents]:"ins"}}},n={};function i(e){var t=n[e];if(void 0!==t)return t.exports;var o=n[e]={exports:{}};return r[e](o,o.exports,i),o.exports}i.m=r,i.d=(e,t)=>{for(var r in t)i.o(t,r)&&!i.o(e,r)&&Object.defineProperty(e,r,{enumerable:!0,get:t[r]})},i.f={},i.e=e=>Promise.all(Object.keys(i.f).reduce(((t,r)=>(i.f[r](e,t),t)),[])),i.u=e=>({212:"nr-spa-compressor",249:"nr-spa-recorder",478:"nr-spa"}[e]+"-1.298.0.min.js"),i.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),e={},t="NRBA-1.298.0.PROD:",i.l=(r,n,o,a)=>{if(e[r])e[r].push(n);else{var s,c;if(void 0!==o)for(var u=document.getElementsByTagName("script"),d=0;d<u.length;d++){var l=u[d];if(l.getAttribute("src")==r||l.getAttribute("data-webpack")==t+o){s=l;break}}if(!s){c=!0;var f={478:"sha512-HSzyNKgJkuD44GAhaqv0J6DKcfr1w2jxbMOXpVRlEPzOJ5GjWJWQZIdOL87SoGmx16NaL73pHxiN9KyHn0UrgA==",249:"sha512-xTIxx7hc1QvTaXfB6dqbBMAiZHtwW1OwcFbBfxC79mvUj0Pv1mHSmucWTWPFxLHkrm634DEJq3+YEWA3rMzWbQ==",212:"sha512-SsdMj4Co3WAfG+frLMYFYoHlVeE1k16lyb/G4bKQ2fIFXjgqC9R56ukLcI2p2IitTkwwpEAJ9qMssxBjVA/D3Q=="};(s=document.createElement("script")).charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.setAttribute("data-webpack",t+o),s.src=r,0!==s.src.indexOf(window.location.origin+"/")&&(s.crossOrigin="anonymous"),f[a]&&(s.integrity=f[a])}e[r]=[n];var h=(t,n)=>{s.onerror=s.onload=null,clearTimeout(p);var i=e[r];if(delete e[r],s.parentNode&&s.parentNode.removeChild(s),i&&i.forEach((e=>e(n))),t)return t(n)},p=setTimeout(h.bind(null,void 0,{type:"timeout",target:s}),12e4);s.onerror=h.bind(null,s.onerror),s.onload=h.bind(null,s.onload),c&&document.head.appendChild(s)}},i.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.p="https://js-agent.newrelic.com/",(()=>{var e={38:0,788:0};i.f.j=(t,r)=>{var n=i.o(e,t)?e[t]:void 0;if(0!==n)if(n)r.push(n[2]);else{var o=new Promise(((r,i)=>n=e[t]=[r,i]));r.push(n[2]=o);var a=i.p+i.u(t),s=new Error;i.l(a,(r=>{if(i.o(e,t)&&(0!==(n=e[t])&&(e[t]=void 0),n)){var o=r&&("load"===r.type?"missing":r.type),a=r&&r.target&&r.target.src;s.message="Loading chunk "+t+" failed.\n("+o+": "+a+")",s.name="ChunkLoadError",s.type=o,s.request=a,n[1](s)}}),"chunk-"+t,t)}};var t=(t,r)=>{var n,o,[a,s,c]=r,u=0;if(a.some((t=>0!==e[t]))){for(n in s)i.o(s,n)&&(i.m[n]=s[n]);if(c)c(i)}for(t&&t(r);u<a.length;u++)o=a[u],i.o(e,o)&&e[o]&&e[o][0](),e[o]=0},r=self["webpackChunk:NRBA-1.298.0.PROD"]=self["webpackChunk:NRBA-1.298.0.PROD"]||[];r.forEach(t.bind(null,0)),r.push=t.bind(null,r.push.bind(r))})(),(()=>{"use strict";i(8374);var e=i(9566),t=i(1741);class r extends t.W{agentIdentifier=(0,e.LA)(16)}var n=i(860);const o=Object.values(n.K7);var a=i(5205);var s=i(9908),c=i(1863),u=i(4261),d=i(3241),l=i(944),f=i(5701),h=i(8154);function p(e,t,i,o){const a=o||i;!a||a[e]&&a[e]!==r.prototype[e]||(a[e]=function(){(0,s.p)(h.xV,["API/"+e+"/called"],void 0,n.K7.metrics,i.ee),(0,d.W)({agentIdentifier:i.agentIdentifier,drained:!!f.B?.[i.agentIdentifier],type:"data",name:"api",feature:u.Pl+e,data:{}});try{return t.apply(this,arguments)}catch(e){(0,l.R)(23,e)}})}function g(e,t,r,n,i){const o=e.info;null===r?delete o.jsAttributes[t]:o.jsAttributes[t]=r,(i||null===r)&&(0,s.p)(u.Pl+n,[(0,c.t)(),t,r],void 0,"session",e.ee)}var m=i(1687),v=i(4234),b=i(5289),y=i(6154),w=i(5270),R=i(7767),x=i(6389),T=i(7699);class E extends v.W{constructor(e,t){super(e.agentIdentifier,t),this.agentRef=e,this.abortHandler=void 0,this.featAggregate=void 0,this.onAggregateImported=void 0,this.deferred=Promise.resolve(),!1===e.init[this.featureName].autoStart?this.deferred=new Promise(((t,r)=>{this.ee.on("manual-start-all",(0,x.J)((()=>{(0,m.Ak)(e.agentIdentifier,this.featureName),t()})))})):(0,m.Ak)(e.agentIdentifier,t)}importAggregator(e,t,r={}){if(this.featAggregate)return;let n;this.onAggregateImported=new Promise((e=>{n=e}));const o=async()=>{let o;await this.deferred;try{if((0,R.V)(e.init)){const{setupAgentSession:t}=await i.e(478).then(i.bind(i,2955));o=t(e)}}catch(e){(0,l.R)(20,e),this.ee.emit("internal-error",[e]),(0,s.p)(T.qh,[e],void 0,this.featureName,this.ee)}try{if(!this.#t(this.featureName,o,e.init))return(0,m.Ze)(this.agentIdentifier,this.featureName),void n(!1);const{Aggregate:i}=await t();this.featAggregate=new i(e,r),e.runtime.harvester.initializedAggregates.push(this.featAggregate),n(!0)}catch(e){(0,l.R)(34,e),this.abortHandler?.(),(0,m.Ze)(this.agentIdentifier,this.featureName,!0),n(!1),this.ee&&this.ee.abort()}};y.RI?(0,b.GG)((()=>o()),!0):o()}#t(e,t,r){if(this.blocked)return!1;switch(e){case n.K7.sessionReplay:return(0,w.SR)(r)&&!!t;case n.K7.sessionTrace:return!!t;default:return!0}}}var A=i(6630),S=i(2614);class _ extends E{static featureName=A.T;constructor(e){var t;super(e,A.T),this.setupInspectionEvents(e.agentIdentifier),t=e,p(u.Fw,(function(e,r){"string"==typeof e&&("/"!==e.charAt(0)&&(e="/"+e),t.runtime.customTransaction=(r||"http://custom.transaction")+e,(0,s.p)(u.Pl+u.Fw,[(0,c.t)()],void 0,void 0,t.ee))}),t),this.ee.on("api-send-rum",((e,t)=>(0,s.p)("send-rum",[e,t],void 0,this.featureName,this.ee))),this.importAggregator(e,(()=>i.e(478).then(i.bind(i,1983))))}setupInspectionEvents(e){const t=(t,r)=>{t&&(0,d.W)({agentIdentifier:e,timeStamp:t.timeStamp,loaded:"complete"===t.target.readyState,type:"window",name:r,data:t.target.location+""})};(0,b.sB)((e=>{t(e,"DOMContentLoaded")})),(0,b.GG)((e=>{t(e,"load")})),(0,b.Qr)((e=>{t(e,"navigate")})),this.ee.on(S.tS.UPDATE,((t,r)=>{(0,d.W)({agentIdentifier:e,type:"lifecycle",name:"session",data:r})}))}}var N=i(384);var O=i(2843),I=i(3878),P=i(782);class j extends E{static featureName=P.T;constructor(e){super(e,P.T),y.RI&&((0,O.u)((()=>(0,s.p)("docHidden",[(0,c.t)()],void 0,P.T,this.ee)),!0),(0,I.sp)("pagehide",(()=>(0,s.p)("winPagehide",[(0,c.t)()],void 0,P.T,this.ee))),this.importAggregator(e,(()=>i.e(478).then(i.bind(i,9917)))))}}class k extends E{static featureName=h.TZ;constructor(e){super(e,h.TZ),y.RI&&document.addEventListener("securitypolicyviolation",(e=>{(0,s.p)(h.xV,["Generic/CSPViolation/Detected"],void 0,this.featureName,this.ee)})),this.importAggregator(e,(()=>i.e(478).then(i.bind(i,8351))))}}var C=i(6774),L=i(3304);class M{constructor(e,t,r,n,i){this.name="UncaughtError",this.message="string"==typeof e?e:(0,L.A)(e),this.sourceURL=t,this.line=r,this.column=n,this.__newrelic=i}}function H(e){return U(e)?e:new M(void 0!==e?.message?e.message:e,e?.filename||e?.sourceURL,e?.lineno||e?.line,e?.colno||e?.col,e?.__newrelic,e?.cause)}function D(e){const t="Unhandled Promise Rejection: ";if(!e?.reason)return;if(U(e.reason)){try{e.reason.message.startsWith(t)||(e.reason.message=t+e.reason.message)}catch(e){}return H(e.reason)}const r=H(e.reason);return(r.message||"").startsWith(t)||(r.message=t+r.message),r}function K(e){if(e.error instanceof SyntaxError&&!/:\d+$/.test(e.error.stack?.trim())){const t=new M(e.message,e.filename,e.lineno,e.colno,e.error.__newrelic,e.cause);return t.name=SyntaxError.name,t}return U(e.error)?e.error:H(e)}function U(e){return e instanceof Error&&!!e.stack}function F(e,t,r,i,o=(0,c.t)()){"string"==typeof e&&(e=new Error(e)),(0,s.p)("err",[e,o,!1,t,r.runtime.isRecording,void 0,i],void 0,n.K7.jserrors,r.ee)}var W=i(3496),B=i(993),G=i(3785);function V(e,{customAttributes:t={},level:r=B.p_.INFO}={},n,i,o=(0,c.t)()){(0,G.R)(n.ee,e,t,r,i,o)}function z(e,t,r,i,o=(0,c.t)()){(0,s.p)(u.Pl+u.hG,[o,e,t,i],void 0,n.K7.genericEvents,r.ee)}function Z(e){p(u.eY,(function(t){return function(e,t){const r={};let i,o;(0,l.R)(54,"newrelic.register"),e.init.api.allow_registered_children||(i=()=>(0,l.R)(55));t&&(0,W.I)(t)||(i=()=>(0,l.R)(48,t));const a={addPageAction:(n,i={})=>{u(z,[n,{...r,...i},e],t)},log:(n,i={})=>{u(V,[n,{...i,customAttributes:{...r,...i.customAttributes||{}}},e],t)},noticeError:(n,i={})=>{u(F,[n,{...r,...i},e],t)},setApplicationVersion:e=>{r["application.version"]=e},setCustomAttribute:(e,t)=>{r[e]=t},setUserId:e=>{r["enduser.id"]=e},metadata:{customAttributes:r,target:t,get connected(){return o||Promise.reject(new Error("Failed to connect"))}}};i?i():o=new Promise(((n,i)=>{try{const o=e.runtime?.entityManager;let s=!!o?.get().entityGuid,c=o?.getEntityGuidFor(t.licenseKey,t.applicationID),u=!!c;if(s&&u)t.entityGuid=c,n(a);else{const d=setTimeout((()=>i(new Error("Failed to connect - Timeout"))),15e3);function l(r){(0,W.A)(r,e)?s||=!0:t.licenseKey===r.licenseKey&&t.applicationID===r.applicationID&&(u=!0,t.entityGuid=r.entityGuid),s&&u&&(clearTimeout(d),e.ee.removeEventListener("entity-added",l),n(a))}e.ee.emit("api-send-rum",[r,t]),e.ee.on("entity-added",l)}}catch(f){i(f)}}));const u=async(t,r,a)=>{if(i)return i();const u=(0,c.t)();(0,s.p)(h.xV,["API/register/".concat(t.name,"/called")],void 0,n.K7.metrics,e.ee);try{await o;const n=e.init.api.duplicate_registered_data;(!0===n||Array.isArray(n)&&n.includes(a.entityGuid))&&t(...r,void 0,u),t(...r,a.entityGuid,u)}catch(e){(0,l.R)(50,e)}};return a}(e,t)}),e)}class q extends E{static featureName=C.T;constructor(e){var t;super(e,C.T),t=e,p(u.o5,((e,r)=>F(e,r,t)),t),function(e){p(u.bt,(function(t){e.runtime.onerror=t}),e)}(e),function(e){let t=0;p(u.k6,(function(e,r){++t>10||(this.runtime.releaseIds[e.slice(-200)]=(""+r).slice(-200))}),e)}(e),Z(e);try{this.removeOnAbort=new AbortController}catch(e){}this.ee.on("internal-error",((t,r)=>{this.abortHandler&&(0,s.p)("ierr",[H(t),(0,c.t)(),!0,{},e.runtime.isRecording,r],void 0,this.featureName,this.ee)})),y.gm.addEventListener("unhandledrejection",(t=>{this.abortHandler&&(0,s.p)("err",[D(t),(0,c.t)(),!1,{unhandledPromiseRejection:1},e.runtime.isRecording],void 0,this.featureName,this.ee)}),(0,I.jT)(!1,this.removeOnAbort?.signal)),y.gm.addEventListener("error",(t=>{this.abortHandler&&(0,s.p)("err",[K(t),(0,c.t)(),!1,{},e.runtime.isRecording],void 0,this.featureName,this.ee)}),(0,I.jT)(!1,this.removeOnAbort?.signal)),this.abortHandler=this.#r,this.importAggregator(e,(()=>i.e(478).then(i.bind(i,2176))))}#r(){this.removeOnAbort?.abort(),this.abortHandler=void 0}}var X=i(8990);let Y=1;function Q(e){const t=typeof e;return!e||"object"!==t&&"function"!==t?-1:e===y.gm?0:(0,X.I)(e,"nr@id",(function(){return Y++}))}function J(e){if("string"==typeof e&&e.length)return e.length;if("object"==typeof e){if("undefined"!=typeof ArrayBuffer&&e instanceof ArrayBuffer&&e.byteLength)return e.byteLength;if("undefined"!=typeof Blob&&e instanceof Blob&&e.size)return e.size;if(!("undefined"!=typeof FormData&&e instanceof FormData))try{return(0,L.A)(e).length}catch(e){return}}}var ee=i(8139),te=i(7836),re=i(3434);const ne={},ie=["open","send"];function oe(e){var t=e||te.ee;const r=function(e){return(e||te.ee).get("xhr")}(t);if(void 0===y.gm.XMLHttpRequest)return r;if(ne[r.debugId]++)return r;ne[r.debugId]=1,(0,ee.u)(t);var n=(0,re.YM)(r),i=y.gm.XMLHttpRequest,o=y.gm.MutationObserver,a=y.gm.Promise,s=y.gm.setInterval,c="readystatechange",u=["onload","onerror","onabort","onloadstart","onloadend","onprogress","ontimeout"],d=[],f=y.gm.XMLHttpRequest=function(e){const t=new i(e),o=r.context(t);try{r.emit("new-xhr",[t],o),t.addEventListener(c,(a=o,function(){var e=this;e.readyState>3&&!a.resolved&&(a.resolved=!0,r.emit("xhr-resolved",[],e)),n.inPlace(e,u,"fn-",b)}),(0,I.jT)(!1))}catch(e){(0,l.R)(15,e);try{r.emit("internal-error",[e])}catch(e){}}var a;return t};function h(e,t){n.inPlace(t,["onreadystatechange"],"fn-",b)}if(function(e,t){for(var r in e)t[r]=e[r]}(i,f),f.prototype=i.prototype,n.inPlace(f.prototype,ie,"-xhr-",b),r.on("send-xhr-start",(function(e,t){h(e,t),function(e){d.push(e),o&&(p?p.then(v):s?s(v):(g=-g,m.data=g))}(t)})),r.on("open-xhr-start",h),o){var p=a&&a.resolve();if(!s&&!a){var g=1,m=document.createTextNode(g);new o(v).observe(m,{characterData:!0})}}else t.on("fn-end",(function(e){e[0]&&e[0].type===c||v()}));function v(){for(var e=0;e<d.length;e++)h(0,d[e]);d.length&&(d=[])}function b(e,t){return t}return r}var ae="fetch-",se=ae+"body-",ce=["arrayBuffer","blob","json","text","formData"],ue=y.gm.Request,de=y.gm.Response,le="prototype";const fe={};function he(e){const t=function(e){return(e||te.ee).get("fetch")}(e);if(!(ue&&de&&y.gm.fetch))return t;if(fe[t.debugId]++)return t;function r(e,r,n){var i=e[r];"function"==typeof i&&(e[r]=function(){var e,r=[...arguments],o={};t.emit(n+"before-start",[r],o),o[te.P]&&o[te.P].dt&&(e=o[te.P].dt);var a=i.apply(this,r);return t.emit(n+"start",[r,e],a),a.then((function(e){return t.emit(n+"end",[null,e],a),e}),(function(e){throw t.emit(n+"end",[e],a),e}))})}return fe[t.debugId]=1,ce.forEach((e=>{r(ue[le],e,se),r(de[le],e,se)})),r(y.gm,"fetch",ae),t.on(ae+"end",(function(e,r){var n=this;if(r){var i=r.headers.get("content-length");null!==i&&(n.rxSize=i),t.emit(ae+"done",[null,r],n)}else t.emit(ae+"done",[e],n)})),t}var pe=i(7485);class ge{constructor(e){this.agentRef=e}generateTracePayload(t){const r=this.agentRef.loader_config;if(!this.shouldGenerateTrace(t)||!r)return null;var n=(r.accountID||"").toString()||null,i=(r.agentID||"").toString()||null,o=(r.trustKey||"").toString()||null;if(!n||!i)return null;var a=(0,e.ZF)(),s=(0,e.el)(),c=Date.now(),u={spanId:a,traceId:s,timestamp:c};return(t.sameOrigin||this.isAllowedOrigin(t)&&this.useTraceContextHeadersForCors())&&(u.traceContextParentHeader=this.generateTraceContextParentHeader(a,s),u.traceContextStateHeader=this.generateTraceContextStateHeader(a,c,n,i,o)),(t.sameOrigin&&!this.excludeNewrelicHeader()||!t.sameOrigin&&this.isAllowedOrigin(t)&&this.useNewrelicHeaderForCors())&&(u.newrelicHeader=this.generateTraceHeader(a,s,c,n,i,o)),u}generateTraceContextParentHeader(e,t){return"00-"+t+"-"+e+"-01"}generateTraceContextStateHeader(e,t,r,n,i){return i+"@nr=0-1-"+r+"-"+n+"-"+e+"----"+t}generateTraceHeader(e,t,r,n,i,o){if(!("function"==typeof y.gm?.btoa))return null;var a={v:[0,1],d:{ty:"Browser",ac:n,ap:i,id:e,tr:t,ti:r}};return o&&n!==o&&(a.d.tk=o),btoa((0,L.A)(a))}shouldGenerateTrace(e){return this.agentRef.init?.distributed_tracing?.enabled&&this.isAllowedOrigin(e)}isAllowedOrigin(e){var t=!1;const r=this.agentRef.init?.distributed_tracing;if(e.sameOrigin)t=!0;else if(r?.allowed_origins instanceof Array)for(var n=0;n<r.allowed_origins.length;n++){var i=(0,pe.D)(r.allowed_origins[n]);if(e.hostname===i.hostname&&e.protocol===i.protocol&&e.port===i.port){t=!0;break}}return t}excludeNewrelicHeader(){var e=this.agentRef.init?.distributed_tracing;return!!e&&!!e.exclude_newrelic_header}useNewrelicHeaderForCors(){var e=this.agentRef.init?.distributed_tracing;return!!e&&!1!==e.cors_use_newrelic_header}useTraceContextHeadersForCors(){var e=this.agentRef.init?.distributed_tracing;return!!e&&!!e.cors_use_tracecontext_headers}}var me=i(9300),ve=i(7295),be=["load","error","abort","timeout"],ye=be.length,we=(0,N.dV)().o.REQ,Re=(0,N.dV)().o.XHR;const xe="X-NewRelic-App-Data";class Te extends E{static featureName=me.T;constructor(e){super(e,me.T),this.dt=new ge(e),this.handler=(e,t,r,n)=>(0,s.p)(e,t,r,n,this.ee);try{const e={xmlhttprequest:"xhr",fetch:"fetch",beacon:"beacon"};y.gm?.performance?.getEntriesByType("resource").forEach((t=>{if(t.initiatorType in e&&0!==t.responseStatus){const r={status:t.responseStatus},i={rxSize:t.transferSize,duration:Math.floor(t.duration),cbTime:0};Ee(r,t.name),this.handler("xhr",[r,i,t.startTime,t.responseEnd,e[t.initiatorType]],void 0,n.K7.ajax)}}))}catch(e){}he(this.ee),oe(this.ee),function(e,t,r,i){function o(e){var t=this;t.totalCbs=0,t.called=0,t.cbTime=0,t.end=E,t.ended=!1,t.xhrGuids={},t.lastSize=null,t.loadCaptureCalled=!1,t.params=this.params||{},t.metrics=this.metrics||{},t.latestLongtaskEnd=0,e.addEventListener("load",(function(r){A(t,e)}),(0,I.jT)(!1)),y.lR||e.addEventListener("progress",(function(e){t.lastSize=e.loaded}),(0,I.jT)(!1))}function a(e){this.params={method:e[0]},Ee(this,e[1]),this.metrics={}}function u(t,r){e.loader_config.xpid&&this.sameOrigin&&r.setRequestHeader("X-NewRelic-ID",e.loader_config.xpid);var n=i.generateTracePayload(this.parsedOrigin);if(n){var o=!1;n.newrelicHeader&&(r.setRequestHeader("newrelic",n.newrelicHeader),o=!0),n.traceContextParentHeader&&(r.setRequestHeader("traceparent",n.traceContextParentHeader),n.traceContextStateHeader&&r.setRequestHeader("tracestate",n.traceContextStateHeader),o=!0),o&&(this.dt=n)}}function d(e,r){var n=this.metrics,i=e[0],o=this;if(n&&i){var a=J(i);a&&(n.txSize=a)}this.startTime=(0,c.t)(),this.body=i,this.listener=function(e){try{"abort"!==e.type||o.loadCaptureCalled||(o.params.aborted=!0),("load"!==e.type||o.called===o.totalCbs&&(o.onloadCalled||"function"!=typeof r.onload)&&"function"==typeof o.end)&&o.end(r)}catch(e){try{t.emit("internal-error",[e])}catch(e){}}};for(var s=0;s<ye;s++)r.addEventListener(be[s],this.listener,(0,I.jT)(!1))}function l(e,t,r){this.cbTime+=e,t?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&"function"==typeof r.onload||"function"!=typeof this.end||this.end(r)}function f(e,t){var r=""+Q(e)+!!t;this.xhrGuids&&!this.xhrGuids[r]&&(this.xhrGuids[r]=!0,this.totalCbs+=1)}function p(e,t){var r=""+Q(e)+!!t;this.xhrGuids&&this.xhrGuids[r]&&(delete this.xhrGuids[r],this.totalCbs-=1)}function g(){this.endTime=(0,c.t)()}function m(e,r){r instanceof Re&&"load"===e[0]&&t.emit("xhr-load-added",[e[1],e[2]],r)}function v(e,r){r instanceof Re&&"load"===e[0]&&t.emit("xhr-load-removed",[e[1],e[2]],r)}function b(e,t,r){t instanceof Re&&("onload"===r&&(this.onload=!0),("load"===(e[0]&&e[0].type)||this.onload)&&(this.xhrCbStart=(0,c.t)()))}function w(e,r){this.xhrCbStart&&t.emit("xhr-cb-time",[(0,c.t)()-this.xhrCbStart,this.onload,r],r)}function R(e){var t,r=e[1]||{};if("string"==typeof e[0]?0===(t=e[0]).length&&y.RI&&(t=""+y.gm.location.href):e[0]&&e[0].url?t=e[0].url:y.gm?.URL&&e[0]&&e[0]instanceof URL?t=e[0].href:"function"==typeof e[0].toString&&(t=e[0].toString()),"string"==typeof t&&0!==t.length){t&&(this.parsedOrigin=(0,pe.D)(t),this.sameOrigin=this.parsedOrigin.sameOrigin);var n=i.generateTracePayload(this.parsedOrigin);if(n&&(n.newrelicHeader||n.traceContextParentHeader))if(e[0]&&e[0].headers)s(e[0].headers,n)&&(this.dt=n);else{var o={};for(var a in r)o[a]=r[a];o.headers=new Headers(r.headers||{}),s(o.headers,n)&&(this.dt=n),e.length>1?e[1]=o:e.push(o)}}function s(e,t){var r=!1;return t.newrelicHeader&&(e.set("newrelic",t.newrelicHeader),r=!0),t.traceContextParentHeader&&(e.set("traceparent",t.traceContextParentHeader),t.traceContextStateHeader&&e.set("tracestate",t.traceContextStateHeader),r=!0),r}}function x(e,t){this.params={},this.metrics={},this.startTime=(0,c.t)(),this.dt=t,e.length>=1&&(this.target=e[0]),e.length>=2&&(this.opts=e[1]);var r,n=this.opts||{},i=this.target;"string"==typeof i?r=i:"object"==typeof i&&i instanceof we?r=i.url:y.gm?.URL&&"object"==typeof i&&i instanceof URL&&(r=i.href),Ee(this,r);var o=(""+(i&&i instanceof we&&i.method||n.method||"GET")).toUpperCase();this.params.method=o,this.body=n.body,this.txSize=J(n.body)||0}function T(e,t){if(this.endTime=(0,c.t)(),this.params||(this.params={}),(0,ve.iW)(this.params))return;let i;this.params.status=t?t.status:0,"string"==typeof this.rxSize&&this.rxSize.length>0&&(i=+this.rxSize);const o={txSize:this.txSize,rxSize:i,duration:(0,c.t)()-this.startTime};r("xhr",[this.params,o,this.startTime,this.endTime,"fetch"],this,n.K7.ajax)}function E(e){const t=this.params,i=this.metrics;if(!this.ended){this.ended=!0;for(let t=0;t<ye;t++)e.removeEventListener(be[t],this.listener,!1);t.aborted||(0,ve.iW)(t)||(i.duration=(0,c.t)()-this.startTime,this.loadCaptureCalled||4!==e.readyState?null==t.status&&(t.status=0):A(this,e),i.cbTime=this.cbTime,r("xhr",[t,i,this.startTime,this.endTime,"xhr"],this,n.K7.ajax))}}function A(e,r){e.params.status=r.status;var i=function(e,t){var r=e.responseType;return"json"===r&&null!==t?t:"arraybuffer"===r||"blob"===r||"json"===r?J(e.response):"text"===r||""===r||void 0===r?J(e.responseText):void 0}(r,e.lastSize);if(i&&(e.metrics.rxSize=i),e.sameOrigin&&r.getAllResponseHeaders().indexOf(xe)>=0){var o=r.getResponseHeader(xe);o&&((0,s.p)(h.rs,["Ajax/CrossApplicationTracing/Header/Seen"],void 0,n.K7.metrics,t),e.params.cat=o.split(", ").pop())}e.loadCaptureCalled=!0}t.on("new-xhr",o),t.on("open-xhr-start",a),t.on("open-xhr-end",u),t.on("send-xhr-start",d),t.on("xhr-cb-time",l),t.on("xhr-load-added",f),t.on("xhr-load-removed",p),t.on("xhr-resolved",g),t.on("addEventListener-end",m),t.on("removeEventListener-end",v),t.on("fn-end",w),t.on("fetch-before-start",R),t.on("fetch-start",x),t.on("fn-start",b),t.on("fetch-done",T)}(e,this.ee,this.handler,this.dt),this.importAggregator(e,(()=>i.e(478).then(i.bind(i,3845))))}}function Ee(e,t){var r=(0,pe.D)(t),n=e.params||e;n.hostname=r.hostname,n.port=r.port,n.protocol=r.protocol,n.host=r.hostname+":"+r.port,n.pathname=r.pathname,e.parsedOrigin=r,e.sameOrigin=r.sameOrigin}const Ae={},Se=["pushState","replaceState"];function _e(e){const t=function(e){return(e||te.ee).get("history")}(e);return!y.RI||Ae[t.debugId]++||(Ae[t.debugId]=1,(0,re.YM)(t).inPlace(window.history,Se,"-")),t}var Ne=i(3738);function Oe(e){p(u.BL,(function(t=Date.now()){const r=t-y.WN;r<0&&(0,l.R)(62,t),(0,s.p)(h.XG,[u.BL,{time:r}],void 0,n.K7.metrics,e.ee),e.addToTrace({name:u.BL,start:t,origin:"nr"}),(0,s.p)(u.Pl+u.hG,[r,u.BL],void 0,n.K7.genericEvents,e.ee)}),e)}const{He:Ie,bD:Pe,d3:je,Kp:ke,TZ:Ce,Lc:Le,uP:Me,Rz:He}=Ne;class De extends E{static featureName=Ce;constructor(e){var t;super(e,Ce),t=e,p(u.U2,(function(e){if(!(e&&"object"==typeof e&&e.name&&e.start))return;const r={n:e.name,s:e.start-y.WN,e:(e.end||e.start)-y.WN,o:e.origin||"",t:"api"};r.s<0||r.e<0||r.e<r.s?(0,l.R)(61,{start:r.s,end:r.e}):(0,s.p)("bstApi",[r],void 0,n.K7.sessionTrace,t.ee)}),t),Oe(e);if(!(0,R.V)(e.init))return void this.deregisterDrain();const r=this.ee;let o;_e(r),this.eventsEE=(0,ee.u)(r),this.eventsEE.on(Me,(function(e,t){this.bstStart=(0,c.t)()})),this.eventsEE.on(Le,(function(e,t){(0,s.p)("bst",[e[0],t,this.bstStart,(0,c.t)()],void 0,n.K7.sessionTrace,r)})),r.on(He+je,(function(e){this.time=(0,c.t)(),this.startPath=location.pathname+location.hash})),r.on(He+ke,(function(e){(0,s.p)("bstHist",[location.pathname+location.hash,this.startPath,this.time],void 0,n.K7.sessionTrace,r)}));try{o=new PerformanceObserver((e=>{const t=e.getEntries();(0,s.p)(Ie,[t],void 0,n.K7.sessionTrace,r)})),o.observe({type:Pe,buffered:!0})}catch(e){}this.importAggregator(e,(()=>i.e(478).then(i.bind(i,6974))),{resourceObserver:o})}}var Ke=i(6344);class Ue extends E{static featureName=Ke.TZ;#n;recorder;constructor(e){var t;let r;super(e,Ke.TZ),t=e,p(u.CH,(function(){(0,s.p)(u.CH,[],void 0,n.K7.sessionReplay,t.ee)}),t),function(e){p(u.Tb,(function(){(0,s.p)(u.Tb,[],void 0,n.K7.sessionReplay,e.ee)}),e)}(e);try{r=JSON.parse(localStorage.getItem("".concat(S.H3,"_").concat(S.uh)))}catch(e){}(0,w.SR)(e.init)&&this.ee.on(Ke.G4.RECORD,(()=>this.#i())),this.#o(r)&&this.importRecorder().then((e=>{e.startRecording(Ke.Qb.PRELOAD,r?.sessionReplayMode)})),this.importAggregator(this.agentRef,(()=>i.e(478).then(i.bind(i,6167))),this),this.ee.on("err",(e=>{this.blocked||this.agentRef.runtime.isRecording&&(this.errorNoticed=!0,(0,s.p)(Ke.G4.ERROR_DURING_REPLAY,[e],void 0,this.featureName,this.ee))}))}#o(e){return e&&(e.sessionReplayMode===S.g.FULL||e.sessionReplayMode===S.g.ERROR)||(0,w.Aw)(this.agentRef.init)}importRecorder(){return this.recorder?Promise.resolve(this.recorder):(this.#n??=Promise.all([i.e(478),i.e(249)]).then(i.bind(i,8589)).then((({Recorder:e})=>(this.recorder=new e(this),this.recorder))).catch((e=>{throw this.ee.emit("internal-error",[e]),this.blocked=!0,e})),this.#n)}#i(){this.blocked||(this.featAggregate?this.featAggregate.mode!==S.g.FULL&&this.featAggregate.initializeRecording(S.g.FULL,!0,Ke.Qb.API):this.importRecorder().then((()=>{this.recorder.startRecording(Ke.Qb.API,S.g.FULL)})))}}var Fe=i(3962);function We(e){const t=e.ee.get("tracer");function r(){}p(u.dT,(function(e){return(new r).get("object"==typeof e?e:{})}),e);const i=r.prototype={createTracer:function(r,i){var o={},a=this,d="function"==typeof i;return(0,s.p)(h.xV,["API/createTracer/called"],void 0,n.K7.metrics,e.ee),e.runSoftNavOverSpa||(0,s.p)(u.hw+"tracer",[(0,c.t)(),r,o],a,n.K7.spa,e.ee),function(){if(t.emit((d?"":"no-")+"fn-start",[(0,c.t)(),a,d],o),d)try{return i.apply(this,arguments)}catch(e){const r="string"==typeof e?new Error(e):e;throw t.emit("fn-err",[arguments,this,r],o),r}finally{t.emit("fn-end",[(0,c.t)()],o)}}}};["actionText","setName","setAttribute","save","ignore","onEnd","getContext","end","get"].forEach((t=>{p.apply(this,[t,function(){return(0,s.p)(u.hw+t,[(0,c.t)(),...arguments],this,e.runSoftNavOverSpa?n.K7.softNav:n.K7.spa,e.ee),this},e,i])})),p(u.PA,(function(){e.runSoftNavOverSpa?(0,s.p)(u.hw+"routeName",[performance.now(),...arguments],void 0,n.K7.softNav,e.ee):(0,s.p)(u.Pl+"routeName",[(0,c.t)(),...arguments],this,n.K7.spa,e.ee)}),e)}class Be extends E{static featureName=Fe.TZ;constructor(e){if(super(e,Fe.TZ),We(e),!y.RI||!(0,N.dV)().o.MO)return;const t=_e(this.ee);try{this.removeOnAbort=new AbortController}catch(e){}Fe.tC.forEach((e=>{(0,I.sp)(e,(e=>{a(e)}),!0,this.removeOnAbort?.signal)}));const r=()=>(0,s.p)("newURL",[(0,c.t)(),""+window.location],void 0,this.featureName,this.ee);t.on("pushState-end",r),t.on("replaceState-end",r),(0,I.sp)(Fe.OV,(e=>{a(e),(0,s.p)("newURL",[e.timeStamp,""+window.location],void 0,this.featureName,this.ee)}),!0,this.removeOnAbort?.signal);let n=!1;const o=new((0,N.dV)().o.MO)(((e,t)=>{n||(n=!0,requestAnimationFrame((()=>{(0,s.p)("newDom",[(0,c.t)()],void 0,this.featureName,this.ee),n=!1})))})),a=(0,x.s)((e=>{(0,s.p)("newUIEvent",[e],void 0,this.featureName,this.ee),o.observe(document.body,{attributes:!0,childList:!0,subtree:!0,characterData:!0})}),100,{leading:!0});this.abortHandler=function(){this.removeOnAbort?.abort(),o.disconnect(),this.abortHandler=void 0},this.importAggregator(e,(()=>i.e(478).then(i.bind(i,4393))),{domObserver:o})}}var Ge=i(7378);const Ve={},ze=["appendChild","insertBefore","replaceChild"];function Ze(e){const t=function(e){return(e||te.ee).get("jsonp")}(e);if(!y.RI||Ve[t.debugId])return t;Ve[t.debugId]=!0;var r=(0,re.YM)(t),n=/[?&](?:callback|cb)=([^&#]+)/,i=/(.*)\.([^.]+)/,o=/^(\w+)(\.|$)(.*)$/;function a(e,t){if(!e)return t;const r=e.match(o),n=r[1];return a(r[3],t[n])}return r.inPlace(Node.prototype,ze,"dom-"),t.on("dom-start",(function(e){!function(e){if(!e||"string"!=typeof e.nodeName||"script"!==e.nodeName.toLowerCase())return;if("function"!=typeof e.addEventListener)return;var o=(s=e.src,c=s.match(n),c?c[1]:null);var s,c;if(!o)return;var u=function(e){var t=e.match(i);if(t&&t.length>=3)return{key:t[2],parent:a(t[1],window)};return{key:e,parent:window}}(o);if("function"!=typeof u.parent[u.key])return;var d={};function l(){t.emit("jsonp-end",[],d),e.removeEventListener("load",l,(0,I.jT)(!1)),e.removeEventListener("error",f,(0,I.jT)(!1))}function f(){t.emit("jsonp-error",[],d),t.emit("jsonp-end",[],d),e.removeEventListener("load",l,(0,I.jT)(!1)),e.removeEventListener("error",f,(0,I.jT)(!1))}r.inPlace(u.parent,[u.key],"cb-",d),e.addEventListener("load",l,(0,I.jT)(!1)),e.addEventListener("error",f,(0,I.jT)(!1)),t.emit("new-jsonp",[e.src],d)}(e[0])})),t}const qe={};function Xe(e){const t=function(e){return(e||te.ee).get("promise")}(e);if(qe[t.debugId])return t;qe[t.debugId]=!0;var r=t.context,n=(0,re.YM)(t),i=y.gm.Promise;return i&&function(){function e(r){var o=t.context(),a=n(r,"executor-",o,null,!1);const s=Reflect.construct(i,[a],e);return t.context(s).getCtx=function(){return o},s}y.gm.Promise=e,Object.defineProperty(e,"name",{value:"Promise"}),e.toString=function(){return i.toString()},Object.setPrototypeOf(e,i),["all","race"].forEach((function(r){const n=i[r];e[r]=function(e){let i=!1;[...e||[]].forEach((e=>{this.resolve(e).then(a("all"===r),a(!1))}));const o=n.apply(this,arguments);return o;function a(e){return function(){t.emit("propagate",[null,!i],o,!1,!1),i=i||!e}}}})),["resolve","reject"].forEach((function(r){const n=i[r];e[r]=function(e){const r=n.apply(this,arguments);return e!==r&&t.emit("propagate",[e,!0],r,!1,!1),r}})),e.prototype=i.prototype;const o=i.prototype.then;i.prototype.then=function(...e){var i=this,a=r(i);a.promise=i,e[0]=n(e[0],"cb-",a,null,!1),e[1]=n(e[1],"cb-",a,null,!1);const s=o.apply(this,e);return a.nextPromise=s,t.emit("propagate",[i,!0],s,!1,!1),s},i.prototype.then[re.Jt]=o,t.on("executor-start",(function(e){e[0]=n(e[0],"resolve-",this,null,!1),e[1]=n(e[1],"resolve-",this,null,!1)})),t.on("executor-err",(function(e,t,r){e[1](r)})),t.on("cb-end",(function(e,r,n){t.emit("propagate",[n,!0],this.nextPromise,!1,!1)})),t.on("propagate",(function(e,r,n){this.getCtx&&!r||(this.getCtx=function(){if(e instanceof Promise)var r=t.context(e);return r&&r.getCtx?r.getCtx():this})}))}(),t}const Ye={},$e="setTimeout",Qe="setInterval",Je="clearTimeout",et="-start",tt=[$e,"setImmediate",Qe,Je,"clearImmediate"];function rt(e){const t=function(e){return(e||te.ee).get("timer")}(e);if(Ye[t.debugId]++)return t;Ye[t.debugId]=1;var r=(0,re.YM)(t);return r.inPlace(y.gm,tt.slice(0,2),$e+"-"),r.inPlace(y.gm,tt.slice(2,3),Qe+"-"),r.inPlace(y.gm,tt.slice(3),Je+"-"),t.on(Qe+et,(function(e,t,n){e[0]=r(e[0],"fn-",null,n)})),t.on($e+et,(function(e,t,n){this.method=n,this.timerDuration=isNaN(e[1])?0:+e[1],e[0]=r(e[0],"fn-",this,n)})),t}const nt={};function it(e){const t=function(e){return(e||te.ee).get("mutation")}(e);if(!y.RI||nt[t.debugId])return t;nt[t.debugId]=!0;var r=(0,re.YM)(t),n=y.gm.MutationObserver;return n&&(window.MutationObserver=function(e){return this instanceof n?new n(r(e,"fn-")):n.apply(this,arguments)},MutationObserver.prototype=n.prototype),t}const{TZ:ot,d3:at,Kp:st,$p:ct,wW:ut,e5:dt,tH:lt,uP:ft,rw:ht,Lc:pt}=Ge;class gt extends E{static featureName=ot;constructor(e){if(super(e,ot),We(e),!y.RI)return;try{this.removeOnAbort=new AbortController}catch(e){}let t,r=0;const n=this.ee.get("tracer"),o=Ze(this.ee),a=Xe(this.ee),u=rt(this.ee),d=oe(this.ee),l=this.ee.get("events"),f=he(this.ee),h=_e(this.ee),p=it(this.ee);function g(e,t){h.emit("newURL",[""+window.location,t])}function m(){r++,t=window.location.hash,this[ft]=(0,c.t)()}function v(){r--,window.location.hash!==t&&g(0,!0);var e=(0,c.t)();this[dt]=~~this[dt]+e-this[ft],this[pt]=e}function b(e,t){e.on(t,(function(){this[t]=(0,c.t)()}))}this.ee.on(ft,m),a.on(ht,m),o.on(ht,m),this.ee.on(pt,v),a.on(ut,v),o.on(ut,v),this.ee.on("fn-err",((...t)=>{t[2]?.__newrelic?.[e.agentIdentifier]||(0,s.p)("function-err",[...t],void 0,this.featureName,this.ee)})),this.ee.buffer([ft,pt,"xhr-resolved"],this.featureName),l.buffer([ft],this.featureName),u.buffer(["setTimeout"+st,"clearTimeout"+at,ft],this.featureName),d.buffer([ft,"new-xhr","send-xhr"+at],this.featureName),f.buffer([lt+at,lt+"-done",lt+ct+at,lt+ct+st],this.featureName),h.buffer(["newURL"],this.featureName),p.buffer([ft],this.featureName),a.buffer(["propagate",ht,ut,"executor-err","resolve"+at],this.featureName),n.buffer([ft,"no-"+ft],this.featureName),o.buffer(["new-jsonp","cb-start","jsonp-error","jsonp-end"],this.featureName),b(f,lt+at),b(f,lt+"-done"),b(o,"new-jsonp"),b(o,"jsonp-end"),b(o,"cb-start"),h.on("pushState-end",g),h.on("replaceState-end",g),window.addEventListener("hashchange",g,(0,I.jT)(!0,this.removeOnAbort?.signal)),window.addEventListener("load",g,(0,I.jT)(!0,this.removeOnAbort?.signal)),window.addEventListener("popstate",(function(){g(0,r>1)}),(0,I.jT)(!0,this.removeOnAbort?.signal)),this.abortHandler=this.#r,this.importAggregator(e,(()=>i.e(478).then(i.bind(i,5592))))}#r(){this.removeOnAbort?.abort(),this.abortHandler=void 0}}var mt=i(3333);class vt extends E{static featureName=mt.TZ;constructor(e){super(e,mt.TZ);const t=[e.init.page_action.enabled,e.init.performance.capture_marks,e.init.performance.capture_measures,e.init.user_actions.enabled,e.init.performance.resources.enabled];var r;if(r=e,p(u.hG,((e,t)=>z(e,t,r)),r),function(e){p(u.fF,(function(){(0,s.p)(u.Pl+u.fF,[(0,c.t)(),...arguments],void 0,n.K7.genericEvents,e.ee)}),e)}(e),Oe(e),Z(e),function(e){p(u.V1,(function(t,r){const i=(0,c.t)(),{start:o,end:a,customAttributes:d}=r||{},f={customAttributes:d||{}};if("object"!=typeof f.customAttributes||"string"!=typeof t||0===t.length)return void(0,l.R)(57);const h=(e,t)=>null==e?t:"number"==typeof e?e:e instanceof PerformanceMark?e.startTime:Number.NaN;if(f.start=h(o,0),f.end=h(a,i),Number.isNaN(f.start)||Number.isNaN(f.end))(0,l.R)(57);else{if(f.duration=f.end-f.start,!(f.duration<0))return(0,s.p)(u.Pl+u.V1,[f,t],void 0,n.K7.genericEvents,e.ee),f;(0,l.R)(58)}}),e)}(e),y.RI&&(e.init.user_actions.enabled&&(mt.Zp.forEach((e=>(0,I.sp)(e,(e=>(0,s.p)("ua",[e],void 0,this.featureName,this.ee)),!0))),mt.qN.forEach((e=>{const t=(0,x.s)((e=>{(0,s.p)("ua",[e],void 0,this.featureName,this.ee)}),500,{leading:!0});(0,I.sp)(e,t)}))),e.init.performance.resources.enabled&&y.gm.PerformanceObserver?.supportedEntryTypes.includes("resource"))){new PerformanceObserver((e=>{e.getEntries().forEach((e=>{(0,s.p)("browserPerformance.resource",[e],void 0,this.featureName,this.ee)}))})).observe({type:"resource",buffered:!0})}t.some((e=>e))?this.importAggregator(e,(()=>i.e(478).then(i.bind(i,8019)))):this.deregisterDrain()}}var bt=i(2646);const yt=new Map;function wt(e,t,r,n){if("object"!=typeof t||!t||"string"!=typeof r||!r||"function"!=typeof t[r])return(0,l.R)(29);const i=function(e){return(e||te.ee).get("logger")}(e),o=(0,re.YM)(i),a=new bt.y(te.P);a.level=n.level,a.customAttributes=n.customAttributes;const s=t[r]?.[re.Jt]||t[r];return yt.set(s,a),o.inPlace(t,[r],"wrap-logger-",(()=>yt.get(s))),i}var Rt=i(1910);class xt extends E{static featureName=B.TZ;constructor(e){var t;super(e,B.TZ),t=e,p(u.$9,((e,r)=>V(e,r,t)),t),function(e){p(u.Wb,((t,r,{customAttributes:n={},level:i=B.p_.INFO}={})=>{wt(e.ee,t,r,{customAttributes:n,level:i})}),e)}(e),Z(e);const r=this.ee;["log","error","warn","info","debug","trace"].forEach((e=>{(0,Rt.i)(y.gm.console[e]),wt(r,y.gm.console,e,{level:"log"===e?"info":e})})),this.ee.on("wrap-logger-end",(function([e]){const{level:t,customAttributes:n}=this;(0,G.R)(r,e,n,t)})),this.importAggregator(e,(()=>i.e(478).then(i.bind(i,5288))))}}new class extends r{constructor(e){var t;(super(),y.gm)?(this.features={},(0,N.bQ)(this.agentIdentifier,this),this.desiredFeatures=new Set(e.features||[]),this.desiredFeatures.add(_),this.runSoftNavOverSpa=[...this.desiredFeatures].some((e=>e.featureName===n.K7.softNav)),(0,a.j)(this,e,e.loaderType||"agent"),t=this,p(u.cD,(function(e,r,n=!1){if("string"==typeof e){if(["string","number","boolean"].includes(typeof r)||null===r)return g(t,e,r,u.cD,n);(0,l.R)(40,typeof r)}else(0,l.R)(39,typeof e)}),t),function(e){p(u.Dl,(function(t){if("string"==typeof t||null===t)return g(e,"enduser.id",t,u.Dl,!0);(0,l.R)(41,typeof t)}),e)}(this),function(e){p(u.nb,(function(t){if("string"==typeof t||null===t)return g(e,"application.version",t,u.nb,!1);(0,l.R)(42,typeof t)}),e)}(this),function(e){p(u.d3,(function(){e.ee.emit("manual-start-all")}),e)}(this),this.run()):(0,l.R)(21)}get config(){return{info:this.info,init:this.init,loader_config:this.loader_config,runtime:this.runtime}}get api(){return this}run(){try{const e=function(e){const t={};return o.forEach((r=>{t[r]=!!e[r]?.enabled})),t}(this.init),t=[...this.desiredFeatures];t.sort(((e,t)=>n.P3[e.featureName]-n.P3[t.featureName])),t.forEach((t=>{if(!e[t.featureName]&&t.featureName!==n.K7.pageViewEvent)return;if(this.runSoftNavOverSpa&&t.featureName===n.K7.spa)return;if(!this.runSoftNavOverSpa&&t.featureName===n.K7.softNav)return;const r=function(e){switch(e){case n.K7.ajax:return[n.K7.jserrors];case n.K7.sessionTrace:return[n.K7.ajax,n.K7.pageViewEvent];case n.K7.sessionReplay:return[n.K7.sessionTrace];case n.K7.pageViewTiming:return[n.K7.pageViewEvent];default:return[]}}(t.featureName).filter((e=>!(e in this.features)));r.length>0&&(0,l.R)(36,{targetFeature:t.featureName,missingDependencies:r}),this.features[t.featureName]=new t(this)}))}catch(e){(0,l.R)(22,e);for(const e in this.features)this.features[e].abortHandler?.();const t=(0,N.Zm)();delete t.initializedAgents[this.agentIdentifier]?.features,delete this.sharedAggregator;return t.ee.get(this.agentIdentifier).abort(),!1}}}({features:[Te,_,j,De,Ue,k,q,vt,xt,Be,gt],loaderType:"spa"})})()})();</script>
<link rel="icon" type="image/png" sizes="16x16" href="https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_16-tenantFavicon-Frontiers.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_32-tenantFavicon-Frontiers.png">
<meta name="apple-mobile-web-app-title" content="Frontiers | Articles">
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full">
<meta property="description" name="description" content="Music is an archaic form of emotional expression and arousal that can induce strong emotional experiences in listeners, which has important research and prac...">
<meta property="og:title" name="title" content="Frontiers | A review of artificial intelligence methods enabled music-evoked EEG emotion recognition and their applications">
<meta property="og:description" name="description" content="Music is an archaic form of emotional expression and arousal that can induce strong emotional experiences in listeners, which has important research and prac...">
<meta name="keywords" content="artificial intelligence,emotion recognition,music-induced,personalization,applications">
<meta property="og:site_name" name="site_name" content="Frontiers">
<meta property="og:image" name="image" content="https://images-provider.frontiersin.org/api/ipx/w=1200&f=png/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg">
<meta property="og:type" name="type" content="article">
<meta property="og:url" name="url" content="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_volume" content="18">
<meta name="citation_journal_title" content="Frontiers in Neuroscience">
<meta name="citation_publisher" content="Frontiers">
<meta name="citation_journal_abbrev" content="Front. Neurosci.">
<meta name="citation_issn" content="1662-453X">
<meta name="citation_doi" content="10.3389/fnins.2024.1400444">
<meta name="citation_firstpage" content="1400444">
<meta name="citation_language" content="English">
<meta name="citation_title" content="A review of artificial intelligence methods enabled music-evoked EEG emotion recognition and their applications">
<meta name="citation_keywords" content="artificial intelligence; emotion recognition; music-induced; personalization; applications">
<meta name="citation_abstract" content="Music is an archaic form of emotional expression and arousal that can induce strong emotional experiences in listeners, which has important research and practical value in related fields such as emotion regulation. Among the various emotion recognition methods, the music-evoked emotion recognition method utilizing EEG signals provides real-time and direct brain response data, playing a crucial role in elucidating the neural mechanisms underlying music-induced emotions. Artificial intelligence technology has greatly facilitated the research on the recognition of music-evoked EEG emotions. AI algorithms have ushered in a new era for the extraction of characteristic frequency signals and the identification of novel feature signals. The robust computational capabilities of AI have provided fresh perspectives for the development of innovative quantitative models of emotions, tailored to various emotion recognition paradigms. The discourse surrounding AI algorithms in the context of emotional classification models is gaining momentum, with their applications in music therapy, neuroscience, and social activities increasingly coming under the spotlight. Through an in-depth analysis of the complete process of emotion recognition induced by music through electroencephalography (EEG) signals, we have systematically elucidated the influence of AI on pertinent research issues. This analysis offers a trove of innovative approaches that could pave the way for future research endeavors.">
<meta name="citation_article_type" content="Review">
<meta name="citation_pdf_url" content="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/pdf">
<meta name="citation_xml_url" content="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/xml">
<meta name="citation_fulltext_world_readable" content="yes">
<meta name="citation_online_date" content="2024/08/14">
<meta name="citation_publication_date" content="2024/09/04">
<meta name="citation_author" content="Su, Yan ">
<meta name="citation_author_institution" content="School of Art, Zhejiang International Studies University, China">
<meta name="citation_author" content="Liu, Yong ">
<meta name="citation_author_institution" content="School of Education, Hangzhou Normal University, China">
<meta name="citation_author" content="Xiao, Yan ">
<meta name="citation_author_institution" content="School of Arts and Media, Beijing Normal University, China">
<meta name="citation_author" content="Ma, Jiaqi ">
<meta name="citation_author_institution" content="College of Science, Zhejiang University of Technology, China">
<meta name="citation_author" content="Li, Dezhao ">
<meta name="citation_author_institution" content="College of Science, Zhejiang University of Technology, China">
<meta name="dc.identifier" content="doi:10.3389/fnins.2024.1400444">
<script type="application/ld+json">{"@context":"https://schema.org","@type":"ScholarlyArticle","headline":"A review of artificial intelligence methods enabled music-evoked EEG emotion recognition and their applications","author":[{"@type":"Person","name":"Yan Su","affiliation":["School of Art, Zhejiang International Studies University, China"]},{"@type":"Person","name":"Yong Liu","affiliation":["School of Education, Hangzhou Normal University, China"]},{"@type":"Person","name":"Yan Xiao","affiliation":["School of Arts and Media, Beijing Normal University, China"]},{"@type":"Person","name":"Jiaqi Ma","affiliation":["College of Science, Zhejiang University of Technology, China"]},{"@type":"Person","name":"Dezhao Li","affiliation":["College of Science, Zhejiang University of Technology, China"]}],"datePublished":"2024-09-04","dateModified":"2025-10-05","publisher":{"@type":"Organization","name":"Frontiers"},"isPartOf":{"@type":"PublicationIssue","datePublished":"2024","isPartOf":{"@type":"PublicationVolume","volumeNumber":"18","isPartOf":{"@type":"Periodical","name":"Frontiers in Neuroscience"}}},"citation":["https://doi.org/10.3389/fnins.2024.1400444"],"inLanguage":"en"}</script>
<script type="module" src="/ap-2024/_nuxt/BXwRzDm6.js" crossorigin></script>
<script id="unhead:payload" type="application/json">{"title":"Frontiers | Articles"}</script></head><body  class="body--v3"><div id="__nuxt"><!--[--><!----><div theme="purple"><nav class="Ibar"><div class="Ibar__main"><div class="Ibar__wrapper"><button class="Ibar__burger" aria-label="Open Menu" data-event="iBar-btn-openMenu"></button><div class="Ibar__logo"><a href="//www.frontiersin.org/" aria-label="Frontiershome" data-event="iBar-a-home" class="Ibar__logo__link"><svg class="Ibar__logo__svg" viewBox="0 0 2811 590" fill="none" xmlns="http://www.w3.org/2000/svg"><path class="Ibar__logo__text" d="M633.872 234.191h-42.674v-57.246h42.674c0-19.776 2.082-35.389 5.204-48.92 4.164-13.53 9.368-23.939 17.695-31.225 8.326-8.326 18.735-13.53 32.266-16.653 13.531-3.123 29.143-5.204 47.878-5.204h21.858c7.286 0 14.572 1.04 21.857 1.04v62.451c-8.326-1.041-16.653-2.082-23.939-2.082-10.408 0-17.694 1.041-23.939 4.164-6.245 3.122-9.368 10.408-9.368 22.898v13.531h53.083v57.246h-53.083v213.372h-89.512V234.191zM794.161 176.945h86.39v47.879h1.041c6.245-17.694 16.653-30.185 31.225-39.552 14.572-9.368 31.225-13.531 49.96-13.531h10.409c3.122 0 7.286 1.041 10.408 2.082v81.185c-6.245-2.082-11.449-3.122-16.653-4.163-5.204-1.041-11.449-1.041-16.654-1.041-11.449 0-20.816 2.082-29.143 5.204-8.327 3.123-15.613 8.327-20.817 14.572-5.204 6.245-10.408 12.49-12.49 20.817-3.123 8.326-4.163 15.612-4.163 23.939v133.228h-88.472V176.945h-1.041zM989.84 312.254c0-19.776 3.122-39.552 10.41-56.205 7.28-17.695 16.65-32.266 29.14-45.797 12.49-13.531 27.06-22.899 44.76-30.185 17.69-7.285 36.43-11.449 57.24-11.449 20.82 0 39.56 4.164 57.25 11.449 17.69 7.286 32.27 17.695 45.8 30.185 12.49 12.49 22.9 28.102 29.14 45.797 7.29 17.694 10.41 36.429 10.41 56.205 0 20.817-3.12 39.552-10.41 57.246-7.29 17.695-16.65 32.266-29.14 44.756-12.49 12.49-28.11 22.899-45.8 30.185-17.69 7.286-36.43 11.449-57.25 11.449-20.81 0-40.59-4.163-57.24-11.449-17.7-7.286-32.27-17.695-44.76-30.185-12.49-12.49-21.86-28.102-29.14-44.756-7.288-17.694-10.41-36.429-10.41-57.246zm88.47 0c0 8.327 1.04 17.694 3.12 26.021 2.09 9.368 5.21 16.653 9.37 23.939 4.16 7.286 9.37 13.531 16.65 17.695 7.29 4.163 15.62 7.285 26.03 7.285 10.4 0 18.73-2.081 26.02-7.285 7.28-4.164 12.49-10.409 16.65-17.695 4.16-7.286 7.29-15.612 9.37-23.939 2.08-9.368 3.12-17.694 3.12-26.021 0-8.327-1.04-17.694-3.12-26.021-2.08-9.368-5.21-16.653-9.37-23.939-4.16-7.286-9.37-13.531-16.65-17.695-7.29-5.204-15.62-7.285-26.02-7.285-10.41 0-18.74 2.081-26.03 7.285-7.28 5.205-12.49 10.409-16.65 17.695-4.16 7.286-7.28 15.612-9.37 23.939-2.08 9.368-3.12 17.694-3.12 26.021zM1306.25 176.945h86.39v37.47h1.04c4.17-7.286 9.37-13.531 15.62-18.735 6.24-5.204 13.53-10.408 20.81-14.572 7.29-4.163 15.62-7.286 23.94-9.367 8.33-2.082 16.66-3.123 24.98-3.123 22.9 0 40.6 4.164 53.09 11.449 13.53 7.286 22.89 16.654 29.14 27.062 6.24 10.409 10.41 21.858 12.49 34.348 2.08 12.49 2.08 22.898 2.08 33.307v172.779h-88.47V316.417v-27.061c0-9.368-1.04-16.654-4.16-23.94-3.13-7.286-7.29-12.49-13.53-16.653-6.25-4.164-15.62-6.245-27.07-6.245-8.32 0-15.61 2.081-21.85 5.204-6.25 3.122-11.45 7.286-14.58 13.531-4.16 5.204-6.24 11.449-8.32 18.735s-3.12 14.572-3.12 21.858v145.717h-88.48V176.945zM1780.88 234.19h-55.17v122.819c0 10.408 3.12 17.694 8.33 20.817 6.24 3.122 13.53 5.204 22.9 5.204 4.16 0 7.28 0 11.45-1.041h11.45v65.573c-8.33 0-15.62 1.041-23.94 2.082-8.33 1.04-16.66 1.041-23.94 1.041-18.74 0-34.35-2.082-46.84-5.205-12.49-3.122-21.86-8.326-29.14-15.612-7.29-7.286-12.49-16.654-14.58-29.144-3.12-12.49-4.16-27.062-4.16-45.797V234.19h-44.76v-57.246h44.76V94.717h88.47v82.227h55.17v57.246zM1902.66 143.639h-88.48V75.984h88.48v67.655zm-89.52 33.307h88.48v270.618h-88.48V176.946zM2024.43 334.111c1.04 18.735 6.25 33.307 16.66 44.756 10.4 11.449 24.98 16.653 43.71 16.653 10.41 0 20.82-2.081 30.19-7.286 9.36-5.204 16.65-12.49 20.81-22.898h83.27c-4.16 15.613-10.41 29.144-19.78 40.593-9.36 11.449-19.77 20.817-31.22 28.102-12.49 7.286-24.98 12.491-39.55 16.654-14.57 3.122-29.15 5.204-43.72 5.204-21.86 0-41.63-3.122-60.37-9.367-18.73-6.246-34.34-15.613-46.83-28.103-12.49-12.49-22.9-27.062-30.19-45.797-7.28-17.694-10.41-38.511-10.41-60.369 0-20.817 4.17-39.552 11.45-57.246 7.29-17.694 17.7-32.266 31.23-44.756 13.53-12.49 29.14-21.858 46.83-29.144 17.7-7.286 36.43-10.408 56.21-10.408 23.94 0 45.8 4.163 63.49 12.49 17.7 8.327 33.31 19.776 44.76 35.389 11.45 15.612 20.81 32.266 26.02 52.042 5.2 19.776 8.33 41.633 7.28 64.532h-199.84v-1.041zm110.33-49.961c-1.04-15.612-6.24-28.102-15.61-39.551-9.37-10.409-21.86-16.654-37.47-16.654s-28.1 5.204-38.51 15.613c-10.41 10.408-16.66 23.939-18.74 40.592h110.33zM2254.46 176.945h86.39v47.879h1.04c6.25-17.694 16.65-30.185 31.23-39.552 14.57-9.368 31.22-13.531 49.96-13.531h10.4c3.13 0 7.29 1.041 10.41 2.082v81.185c-6.24-2.082-11.45-3.122-16.65-4.163-5.21-1.041-11.45-1.041-16.65-1.041-11.45 0-20.82 2.082-29.15 5.204-8.32 3.123-15.61 8.327-20.81 14.572-6.25 6.245-10.41 12.49-12.49 20.817-3.13 8.326-4.17 15.612-4.17 23.939v133.228h-88.47V176.945h-1.04zM2534.45 359.091c0 7.286 1.04 12.49 4.16 17.694 3.12 5.204 6.24 9.368 10.41 12.49 4.16 3.123 9.36 5.204 14.57 7.286 6.24 2.082 11.45 2.082 17.69 2.082 4.17 0 8.33 0 13.53-2.082 5.21-1.041 9.37-3.123 13.53-5.204 4.17-2.082 7.29-5.204 10.41-9.368 3.13-4.163 4.17-8.327 4.17-13.531 0-5.204-2.09-9.367-5.21-12.49-3.12-3.122-7.28-6.245-11.45-8.327-4.16-2.081-9.36-4.163-14.57-5.204-5.2-1.041-9.37-2.081-13.53-3.122-13.53-3.123-28.1-6.245-42.67-9.368-14.58-3.122-28.11-7.286-40.6-12.49-12.49-6.245-22.9-13.531-30.18-23.939-8.33-10.409-11.45-23.94-11.45-42.675 0-16.653 4.16-30.184 11.45-40.592 8.33-10.409 17.69-18.736 30.18-24.981 12.49-6.245 26.02-10.408 40.6-13.53 14.57-3.123 28.1-4.164 41.63-4.164 14.57 0 29.14 1.041 43.71 4.164 14.58 2.081 27.07 7.285 39.56 13.53 12.49 6.245 21.85 15.613 29.14 27.062 7.29 11.45 11.45 26.021 12.49 43.716h-82.23c0-10.409-4.16-18.736-11.45-23.94-7.28-4.163-16.65-7.286-28.1-7.286-4.16 0-8.32 0-12.49 1.041-4.16 1.041-8.32 1.041-12.49 2.082-4.16 1.041-7.28 3.122-9.37 6.245-2.08 3.122-4.16 6.245-4.16 11.449 0 6.245 3.12 11.449 10.41 15.613 6.24 4.163 14.57 7.286 24.98 10.408 10.41 2.082 20.82 5.204 32.27 7.286 11.44 2.082 22.89 4.163 33.3 6.245 13.53 3.123 24.98 7.286 33.31 13.531 9.37 6.245 15.61 12.49 20.82 19.776 5.2 7.286 9.36 14.572 11.45 21.858 2.08 7.285 3.12 13.53 3.12 19.776 0 17.694-4.17 33.306-11.45 45.796-8.33 12.491-17.7 21.858-30.19 30.185-12.49 7.286-26.02 12.49-41.63 16.653-15.61 3.123-31.22 5.204-45.8 5.204-15.61 0-32.26-1.04-47.87-4.163-15.62-3.122-29.15-8.327-41.64-15.612a83.855 83.855 0 01-30.18-30.185c-8.33-12.49-12.49-28.102-12.49-46.838h84.31v-2.081z" fill="#FFFFFF"></path><path d="M0 481.911V281.028l187.351-58.287v200.882L0 481.911z" fill="#8BC53F"></path><path d="M187.351 423.623V222.741l126.983 87.431v200.882l-126.983-87.431z" fill="#EBD417"></path><path d="M126.982 569.341L0 481.911l187.351-58.287 126.983 87.43-187.352 58.287z" fill="#034EA1"></path><path d="M183.188 212.331l51.001-116.574 65.573 155.085-51.001 116.574-65.573-155.085z" fill="#712E74"></path><path d="M248.761 367.415l51.001-116.574 171.739-28.102-49.96 115.533-172.78 29.143z" fill="#009FD1"></path><path d="M299.762 250.842L234.189 95.757l171.739-28.103 65.573 155.085-171.739 28.103z" fill="#F6921E"></path><path d="M187.352 222.741L59.328 198.802 44.757 71.819 172.78 95.76l14.572 126.982z" fill="#DA2128"></path><path d="M172.78 95.758L44.757 71.818l70.777-70.776 128.023 23.94-70.777 70.776z" fill="#25BCBD"></path><path d="M258.129 153.005l-70.777 69.736-14.571-126.982 70.777-70.778 14.571 128.024z" fill="#00844A"></path></svg></a></div><a href="//www.frontiersin.org/journals/neuroscience" class="Ibar__journalName" data-event="iBar-a-journalHome"><div class="Ibar__journalName__container" logoclass="Ibar__logo--mixed"><div class="Ibar__journal__maskLogo" style="display:none;"><img class="Ibar__journal__logo" src></div><div class="Ibar__journalName"><span>Frontiers in</span><span> Neuroscience</span></div></div></a><div class="Ibar__dropdown--aboutUs" parent-data-event="iBar"><div class="Ibar__dropdown"><button class="Ibar__dropdown__trigger"><!----> About us</button><div class="Ibar__dropdown__menu"><div class="Ibar__dropdown__menu__header"><button class="Ibar__dropdown__menu__header__title" aria-label="Close Dropdown">About us</button><button class="Ibar__close" aria-label="Close Dropdown"></button></div><!--[--><div class="Ibar__dropdown__about"><!--[--><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">Who we are</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/mission" target="_self" data-event="iBar-aboutUs_0-a_whoWeAre">Mission and values</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/history" target="_self" data-event="iBar-aboutUs_0-a_whoWeAre">History</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/leadership" target="_self" data-event="iBar-aboutUs_0-a_whoWeAre">Leadership</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/awards" target="_self" data-event="iBar-aboutUs_0-a_whoWeAre">Awards</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">Impact and progress</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/impact" target="_self" data-event="iBar-aboutUs_1-a_impactAndProgress">Frontiers&#39; impact</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/annual-reports" target="_self" data-event="iBar-aboutUs_1-a_impactAndProgress">Our annual reports</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">Publishing model</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/how-we-publish" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">How we publish</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/open-access" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Open access</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/peer-review" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Peer review</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/research-integrity" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Research integrity</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/research-topics" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Research Topics</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/fair-data-management" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">FAIR² Data Management</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/fee-policy" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Fee policy</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">Services</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://publishingpartnerships.frontiersin.org/" target="_blank" data-event="iBar-aboutUs_3-a_services">Societies</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/open-access-agreements/consortia" target="_self" data-event="iBar-aboutUs_3-a_services">National consortia</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/open-access-agreements" target="_self" data-event="iBar-aboutUs_3-a_services">Institutional partnerships</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/collaborators" target="_self" data-event="iBar-aboutUs_3-a_services">Collaborators</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">More from Frontiers</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://forum.frontiersin.org/" target="_blank" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Frontiers Forum</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/frontiers-planet-prize" target="_self" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Frontiers Planet Prize</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://pressoffice.frontiersin.org/" target="_blank" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Press office</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/sustainability" target="_self" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Sustainability</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://careers.frontiersin.org/" target="_blank" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Career opportunities</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/contact" target="_self" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Contact us</a></li><!--]--></ul><!--]--></div><!--]--></div></div></div><!--[--><a class="Ibar__link" href="https://www.frontiersin.org/journals" data-event="iBar-a-allJournals">All journals</a><a class="Ibar__link" href="https://www.frontiersin.org/articles" data-event="iBar-a-allArticles">All articles</a><!--]--><a class="Ibar__button Ibar__submit" href="https://www.frontiersin.org/submission/submit?domainid=1&amp;fieldid=55&amp;specialtyid=0&amp;entitytype=2&amp;entityid=1" data-event="iBar-a-submit">Submit your research</a><div class="Ibar__spacer"></div><a class="Ibar__icon Ibar__icon--search" href="//www.frontiersin.org/search" aria-label="Search" target="_self" data-event="iBar-a-search"><span>Search</span></a><!----><!----><!----><div class="Ibar__userArea"></div></div></div><div><div class="Ibar__menu Ibar__menu--journal"><div class="Ibar__menu__header"><div class="Ibar__logo"><div class="Ibar__logo"><a href="//www.frontiersin.org/" aria-label="Frontiershome" data-event="iBar-a-home" class="Ibar__logo__link"><svg class="Ibar__logo__svg" viewBox="0 0 2811 590" fill="none" xmlns="http://www.w3.org/2000/svg"><path class="Ibar__logo__text" d="M633.872 234.191h-42.674v-57.246h42.674c0-19.776 2.082-35.389 5.204-48.92 4.164-13.53 9.368-23.939 17.695-31.225 8.326-8.326 18.735-13.53 32.266-16.653 13.531-3.123 29.143-5.204 47.878-5.204h21.858c7.286 0 14.572 1.04 21.857 1.04v62.451c-8.326-1.041-16.653-2.082-23.939-2.082-10.408 0-17.694 1.041-23.939 4.164-6.245 3.122-9.368 10.408-9.368 22.898v13.531h53.083v57.246h-53.083v213.372h-89.512V234.191zM794.161 176.945h86.39v47.879h1.041c6.245-17.694 16.653-30.185 31.225-39.552 14.572-9.368 31.225-13.531 49.96-13.531h10.409c3.122 0 7.286 1.041 10.408 2.082v81.185c-6.245-2.082-11.449-3.122-16.653-4.163-5.204-1.041-11.449-1.041-16.654-1.041-11.449 0-20.816 2.082-29.143 5.204-8.327 3.123-15.613 8.327-20.817 14.572-5.204 6.245-10.408 12.49-12.49 20.817-3.123 8.326-4.163 15.612-4.163 23.939v133.228h-88.472V176.945h-1.041zM989.84 312.254c0-19.776 3.122-39.552 10.41-56.205 7.28-17.695 16.65-32.266 29.14-45.797 12.49-13.531 27.06-22.899 44.76-30.185 17.69-7.285 36.43-11.449 57.24-11.449 20.82 0 39.56 4.164 57.25 11.449 17.69 7.286 32.27 17.695 45.8 30.185 12.49 12.49 22.9 28.102 29.14 45.797 7.29 17.694 10.41 36.429 10.41 56.205 0 20.817-3.12 39.552-10.41 57.246-7.29 17.695-16.65 32.266-29.14 44.756-12.49 12.49-28.11 22.899-45.8 30.185-17.69 7.286-36.43 11.449-57.25 11.449-20.81 0-40.59-4.163-57.24-11.449-17.7-7.286-32.27-17.695-44.76-30.185-12.49-12.49-21.86-28.102-29.14-44.756-7.288-17.694-10.41-36.429-10.41-57.246zm88.47 0c0 8.327 1.04 17.694 3.12 26.021 2.09 9.368 5.21 16.653 9.37 23.939 4.16 7.286 9.37 13.531 16.65 17.695 7.29 4.163 15.62 7.285 26.03 7.285 10.4 0 18.73-2.081 26.02-7.285 7.28-4.164 12.49-10.409 16.65-17.695 4.16-7.286 7.29-15.612 9.37-23.939 2.08-9.368 3.12-17.694 3.12-26.021 0-8.327-1.04-17.694-3.12-26.021-2.08-9.368-5.21-16.653-9.37-23.939-4.16-7.286-9.37-13.531-16.65-17.695-7.29-5.204-15.62-7.285-26.02-7.285-10.41 0-18.74 2.081-26.03 7.285-7.28 5.205-12.49 10.409-16.65 17.695-4.16 7.286-7.28 15.612-9.37 23.939-2.08 9.368-3.12 17.694-3.12 26.021zM1306.25 176.945h86.39v37.47h1.04c4.17-7.286 9.37-13.531 15.62-18.735 6.24-5.204 13.53-10.408 20.81-14.572 7.29-4.163 15.62-7.286 23.94-9.367 8.33-2.082 16.66-3.123 24.98-3.123 22.9 0 40.6 4.164 53.09 11.449 13.53 7.286 22.89 16.654 29.14 27.062 6.24 10.409 10.41 21.858 12.49 34.348 2.08 12.49 2.08 22.898 2.08 33.307v172.779h-88.47V316.417v-27.061c0-9.368-1.04-16.654-4.16-23.94-3.13-7.286-7.29-12.49-13.53-16.653-6.25-4.164-15.62-6.245-27.07-6.245-8.32 0-15.61 2.081-21.85 5.204-6.25 3.122-11.45 7.286-14.58 13.531-4.16 5.204-6.24 11.449-8.32 18.735s-3.12 14.572-3.12 21.858v145.717h-88.48V176.945zM1780.88 234.19h-55.17v122.819c0 10.408 3.12 17.694 8.33 20.817 6.24 3.122 13.53 5.204 22.9 5.204 4.16 0 7.28 0 11.45-1.041h11.45v65.573c-8.33 0-15.62 1.041-23.94 2.082-8.33 1.04-16.66 1.041-23.94 1.041-18.74 0-34.35-2.082-46.84-5.205-12.49-3.122-21.86-8.326-29.14-15.612-7.29-7.286-12.49-16.654-14.58-29.144-3.12-12.49-4.16-27.062-4.16-45.797V234.19h-44.76v-57.246h44.76V94.717h88.47v82.227h55.17v57.246zM1902.66 143.639h-88.48V75.984h88.48v67.655zm-89.52 33.307h88.48v270.618h-88.48V176.946zM2024.43 334.111c1.04 18.735 6.25 33.307 16.66 44.756 10.4 11.449 24.98 16.653 43.71 16.653 10.41 0 20.82-2.081 30.19-7.286 9.36-5.204 16.65-12.49 20.81-22.898h83.27c-4.16 15.613-10.41 29.144-19.78 40.593-9.36 11.449-19.77 20.817-31.22 28.102-12.49 7.286-24.98 12.491-39.55 16.654-14.57 3.122-29.15 5.204-43.72 5.204-21.86 0-41.63-3.122-60.37-9.367-18.73-6.246-34.34-15.613-46.83-28.103-12.49-12.49-22.9-27.062-30.19-45.797-7.28-17.694-10.41-38.511-10.41-60.369 0-20.817 4.17-39.552 11.45-57.246 7.29-17.694 17.7-32.266 31.23-44.756 13.53-12.49 29.14-21.858 46.83-29.144 17.7-7.286 36.43-10.408 56.21-10.408 23.94 0 45.8 4.163 63.49 12.49 17.7 8.327 33.31 19.776 44.76 35.389 11.45 15.612 20.81 32.266 26.02 52.042 5.2 19.776 8.33 41.633 7.28 64.532h-199.84v-1.041zm110.33-49.961c-1.04-15.612-6.24-28.102-15.61-39.551-9.37-10.409-21.86-16.654-37.47-16.654s-28.1 5.204-38.51 15.613c-10.41 10.408-16.66 23.939-18.74 40.592h110.33zM2254.46 176.945h86.39v47.879h1.04c6.25-17.694 16.65-30.185 31.23-39.552 14.57-9.368 31.22-13.531 49.96-13.531h10.4c3.13 0 7.29 1.041 10.41 2.082v81.185c-6.24-2.082-11.45-3.122-16.65-4.163-5.21-1.041-11.45-1.041-16.65-1.041-11.45 0-20.82 2.082-29.15 5.204-8.32 3.123-15.61 8.327-20.81 14.572-6.25 6.245-10.41 12.49-12.49 20.817-3.13 8.326-4.17 15.612-4.17 23.939v133.228h-88.47V176.945h-1.04zM2534.45 359.091c0 7.286 1.04 12.49 4.16 17.694 3.12 5.204 6.24 9.368 10.41 12.49 4.16 3.123 9.36 5.204 14.57 7.286 6.24 2.082 11.45 2.082 17.69 2.082 4.17 0 8.33 0 13.53-2.082 5.21-1.041 9.37-3.123 13.53-5.204 4.17-2.082 7.29-5.204 10.41-9.368 3.13-4.163 4.17-8.327 4.17-13.531 0-5.204-2.09-9.367-5.21-12.49-3.12-3.122-7.28-6.245-11.45-8.327-4.16-2.081-9.36-4.163-14.57-5.204-5.2-1.041-9.37-2.081-13.53-3.122-13.53-3.123-28.1-6.245-42.67-9.368-14.58-3.122-28.11-7.286-40.6-12.49-12.49-6.245-22.9-13.531-30.18-23.939-8.33-10.409-11.45-23.94-11.45-42.675 0-16.653 4.16-30.184 11.45-40.592 8.33-10.409 17.69-18.736 30.18-24.981 12.49-6.245 26.02-10.408 40.6-13.53 14.57-3.123 28.1-4.164 41.63-4.164 14.57 0 29.14 1.041 43.71 4.164 14.58 2.081 27.07 7.285 39.56 13.53 12.49 6.245 21.85 15.613 29.14 27.062 7.29 11.45 11.45 26.021 12.49 43.716h-82.23c0-10.409-4.16-18.736-11.45-23.94-7.28-4.163-16.65-7.286-28.1-7.286-4.16 0-8.32 0-12.49 1.041-4.16 1.041-8.32 1.041-12.49 2.082-4.16 1.041-7.28 3.122-9.37 6.245-2.08 3.122-4.16 6.245-4.16 11.449 0 6.245 3.12 11.449 10.41 15.613 6.24 4.163 14.57 7.286 24.98 10.408 10.41 2.082 20.82 5.204 32.27 7.286 11.44 2.082 22.89 4.163 33.3 6.245 13.53 3.123 24.98 7.286 33.31 13.531 9.37 6.245 15.61 12.49 20.82 19.776 5.2 7.286 9.36 14.572 11.45 21.858 2.08 7.285 3.12 13.53 3.12 19.776 0 17.694-4.17 33.306-11.45 45.796-8.33 12.491-17.7 21.858-30.19 30.185-12.49 7.286-26.02 12.49-41.63 16.653-15.61 3.123-31.22 5.204-45.8 5.204-15.61 0-32.26-1.04-47.87-4.163-15.62-3.122-29.15-8.327-41.64-15.612a83.855 83.855 0 01-30.18-30.185c-8.33-12.49-12.49-28.102-12.49-46.838h84.31v-2.081z" fill="#FFFFFF"></path><path d="M0 481.911V281.028l187.351-58.287v200.882L0 481.911z" fill="#8BC53F"></path><path d="M187.351 423.623V222.741l126.983 87.431v200.882l-126.983-87.431z" fill="#EBD417"></path><path d="M126.982 569.341L0 481.911l187.351-58.287 126.983 87.43-187.352 58.287z" fill="#034EA1"></path><path d="M183.188 212.331l51.001-116.574 65.573 155.085-51.001 116.574-65.573-155.085z" fill="#712E74"></path><path d="M248.761 367.415l51.001-116.574 171.739-28.102-49.96 115.533-172.78 29.143z" fill="#009FD1"></path><path d="M299.762 250.842L234.189 95.757l171.739-28.103 65.573 155.085-171.739 28.103z" fill="#F6921E"></path><path d="M187.352 222.741L59.328 198.802 44.757 71.819 172.78 95.76l14.572 126.982z" fill="#DA2128"></path><path d="M172.78 95.758L44.757 71.818l70.777-70.776 128.023 23.94-70.777 70.776z" fill="#25BCBD"></path><path d="M258.129 153.005l-70.777 69.736-14.571-126.982 70.777-70.778 14.571 128.024z" fill="#00844A"></path></svg></a></div></div><button class="Ibar__close" aria-label="Close Menu" data-event="iBarMenu-btn-closeMenu"></button></div><div class="Ibar__menu__wrapper"><div class="Ibar__menu__journal"><a href="//www.frontiersin.org/journals/neuroscience" data-event="iBarMenu-a-journalHome"><div class="Ibar__journalName__container"><div class="Ibar__journal__maskLogo" style="display:none;"><img class="Ibar__journal__logo" src></div><div class="Ibar__journalName"><span>Frontiers in</span><span> Neuroscience</span></div></div></a><div parent-data-event="iBarMenu"><div class="Ibar__dropdown"><button class="Ibar__dropdown__trigger"><!----> Sections</button><div class="Ibar__dropdown__menu"><div class="Ibar__dropdown__menu__header"><button class="Ibar__dropdown__menu__header__title" aria-label="Close Dropdown">Sections</button><button class="Ibar__close" aria-label="Close Dropdown"></button></div><!--[--><ul class="Ibar__dropdown__sections"><!--[--><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/auditory-cognitive-neuroscience" data-event="iBarJournal-sections-a_id_65">Auditory Cognitive Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/autonomic-neuroscience" data-event="iBarJournal-sections-a_id_157">Autonomic Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/brain-imaging-methods" data-event="iBarJournal-sections-a_id_600">Brain Imaging Methods</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/decision-neuroscience" data-event="iBarJournal-sections-a_id_33">Decision Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/gut-brain-axis" data-event="iBarJournal-sections-a_id_2416">Gut-Brain Axis</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neural-technology" data-event="iBarJournal-sections-a_id_1206">Neural Technology</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurodegeneration" data-event="iBarJournal-sections-a_id_73">Neurodegeneration</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurodevelopment" data-event="iBarJournal-sections-a_id_1944">Neurodevelopment</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroendocrine-science" data-event="iBarJournal-sections-a_id_113">Neuroendocrine Science</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroenergetics-and-brain-health" data-event="iBarJournal-sections-a_id_818">Neuroenergetics and Brain Health</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurogenesis" data-event="iBarJournal-sections-a_id_25">Neurogenesis</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurogenomics" data-event="iBarJournal-sections-a_id_19">Neurogenomics</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuromorphic-engineering" data-event="iBarJournal-sections-a_id_31">Neuromorphic Engineering</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuropharmacology" data-event="iBarJournal-sections-a_id_26">Neuropharmacology</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroprosthetics" data-event="iBarJournal-sections-a_id_23">Neuroprosthetics</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroscience-methods-and-techniques" data-event="iBarJournal-sections-a_id_3022">Neuroscience Methods and Techniques</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/perception-science" data-event="iBarJournal-sections-a_id_41">Perception Science</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/sleep-and-circadian-rhythms" data-event="iBarJournal-sections-a_id_1409">Sleep and Circadian Rhythms</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/social-and-evolutionary-neuroscience" data-event="iBarJournal-sections-a_id_57">Social and Evolutionary Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/translational-neuroscience" data-event="iBarJournal-sections-a_id_2450">Translational Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/visual-neuroscience" data-event="iBarJournal-sections-a_id_2411">Visual Neuroscience</a></li><!--]--></ul><!--]--></div></div></div><!--[--><a class="Ibar__link" href="//www.frontiersin.org/journals/neuroscience/articles" target="_self" data-event="iBar-a-articles">Articles</a><a class="Ibar__link" href="//www.frontiersin.org/journals/neuroscience/research-topics" target="_self" data-event="iBar-a-researchTopics">Research Topics</a><a class="Ibar__link" href="//www.frontiersin.org/journals/neuroscience/editors" target="_self" data-event="iBar-a-editorialBoard">Editorial board</a><!--]--><div parent-data-event="iBarMenu"><div class="Ibar__dropdown"><button class="Ibar__dropdown__trigger"><!----> About journal</button><div class="Ibar__dropdown__menu"><div class="Ibar__dropdown__menu__header"><button class="Ibar__dropdown__menu__header__title" aria-label="Close Dropdown">About journal</button><button class="Ibar__close" aria-label="Close Dropdown"></button></div><!--[--><div class="Ibar__dropdown__about"><!--[--><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title"><span>Scope</span></li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-editors" target="_self" data-event="iBar-aboutJournal_0-a_fieldChiefEditors">Field chief editors</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-scope" target="_self" data-event="iBar-aboutJournal_1-a_mission &amp;Scope">Mission &amp; scope</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-facts" target="_self" data-event="iBar-aboutJournal_2-a_facts">Facts</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-submission" target="_self" data-event="iBar-aboutJournal_3-a_journalSections">Journal sections</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-open" target="_self" data-event="iBar-aboutJournal_4-a_openAccessStatemen">Open access statement</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#copyright-statement" target="_self" data-event="iBar-aboutJournal_5-a_copyrightStatement">Copyright statement</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-quality" target="_self" data-event="iBar-aboutJournal_6-a_quality">Quality</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title"><span>For authors</span></li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/why-submit" target="_self" data-event="iBar-aboutJournal_0-a_whySubmit?">Why submit?</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/article-types" target="_self" data-event="iBar-aboutJournal_1-a_articleTypes">Article types</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/author-guidelines" target="_self" data-event="iBar-aboutJournal_2-a_authorGuidelines">Author guidelines</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/editor-guidelines" target="_self" data-event="iBar-aboutJournal_3-a_editorGuidelines">Editor guidelines</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/publishing-fees" target="_self" data-event="iBar-aboutJournal_4-a_publishingFees">Publishing fees</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/submission-checklist" target="_self" data-event="iBar-aboutJournal_5-a_submissionChecklis">Submission checklist</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/contact-editorial-office" target="_self" data-event="iBar-aboutJournal_6-a_contactEditorialOf">Contact editorial office</a></li><!--]--></ul><!--]--></div><!--]--></div></div></div></div><div class="Ibar__dropdown--aboutUs" parent-data-event="iBarMenu"><div class="Ibar__dropdown"><button class="Ibar__dropdown__trigger"><!----> About us</button><div class="Ibar__dropdown__menu"><div class="Ibar__dropdown__menu__header"><button class="Ibar__dropdown__menu__header__title" aria-label="Close Dropdown">About us</button><button class="Ibar__close" aria-label="Close Dropdown"></button></div><!--[--><div class="Ibar__dropdown__about"><!--[--><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">Who we are</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/mission" target="_self" data-event="iBar-aboutUs_0-a_whoWeAre">Mission and values</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/history" target="_self" data-event="iBar-aboutUs_0-a_whoWeAre">History</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/leadership" target="_self" data-event="iBar-aboutUs_0-a_whoWeAre">Leadership</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/awards" target="_self" data-event="iBar-aboutUs_0-a_whoWeAre">Awards</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">Impact and progress</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/impact" target="_self" data-event="iBar-aboutUs_1-a_impactAndProgress">Frontiers&#39; impact</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/annual-reports" target="_self" data-event="iBar-aboutUs_1-a_impactAndProgress">Our annual reports</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">Publishing model</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/how-we-publish" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">How we publish</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/open-access" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Open access</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/peer-review" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Peer review</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/research-integrity" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Research integrity</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/research-topics" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Research Topics</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/fair-data-management" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">FAIR² Data Management</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/fee-policy" target="_self" data-event="iBar-aboutUs_2-a_publishingModel">Fee policy</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">Services</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://publishingpartnerships.frontiersin.org/" target="_blank" data-event="iBar-aboutUs_3-a_services">Societies</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/open-access-agreements/consortia" target="_self" data-event="iBar-aboutUs_3-a_services">National consortia</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/open-access-agreements" target="_self" data-event="iBar-aboutUs_3-a_services">Institutional partnerships</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/collaborators" target="_self" data-event="iBar-aboutUs_3-a_services">Collaborators</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title">More from Frontiers</li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://forum.frontiersin.org/" target="_blank" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Frontiers Forum</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/frontiers-planet-prize" target="_self" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Frontiers Planet Prize</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://pressoffice.frontiersin.org/" target="_blank" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Press office</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/sustainability" target="_self" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Sustainability</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://careers.frontiersin.org/" target="_blank" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Career opportunities</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/about/contact" target="_self" data-event="iBar-aboutUs_4-a_moreFromFrontiers">Contact us</a></li><!--]--></ul><!--]--></div><!--]--></div></div></div><!--[--><a class="Ibar__link" href="https://www.frontiersin.org/journals" data-event="iBar-a-allJournals">All journals</a><a class="Ibar__link" href="https://www.frontiersin.org/articles" data-event="iBar-a-allArticles">All articles</a><!--]--><!----><!----><!----><a class="Ibar__button Ibar__submit" href="https://www.frontiersin.org/submission/submit?domainid=1&amp;fieldid=55&amp;specialtyid=0&amp;entitytype=2&amp;entityid=1" data-event="iBarMenu-a-submit">Submit your research</a></div></div></div><!--[--><div class="Ibar__journal Ibar__journal--main"><div class="Ibar__wrapper Ibar__wrapper--journal"><a href="//www.frontiersin.org/journals/neuroscience" class="Ibar__journalName" data-event="iBarJournal-a-journalHome"><div class="Ibar__journalName__container"><div class="Ibar__journal__maskLogo" style="display:none;"><img class="Ibar__journal__logo" src></div><div class="Ibar__journalName"><span>Frontiers in</span><span> Neuroscience</span></div></div></a><div parent-data-event="iBarJournal"><div class="Ibar__dropdown"><button class="Ibar__dropdown__trigger"><!----> Sections</button><div class="Ibar__dropdown__menu"><div class="Ibar__dropdown__menu__header"><button class="Ibar__dropdown__menu__header__title" aria-label="Close Dropdown">Sections</button><button class="Ibar__close" aria-label="Close Dropdown"></button></div><!--[--><ul class="Ibar__dropdown__sections"><!--[--><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/auditory-cognitive-neuroscience" data-event="iBarJournal-sections-a_id_65">Auditory Cognitive Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/autonomic-neuroscience" data-event="iBarJournal-sections-a_id_157">Autonomic Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/brain-imaging-methods" data-event="iBarJournal-sections-a_id_600">Brain Imaging Methods</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/decision-neuroscience" data-event="iBarJournal-sections-a_id_33">Decision Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/gut-brain-axis" data-event="iBarJournal-sections-a_id_2416">Gut-Brain Axis</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neural-technology" data-event="iBarJournal-sections-a_id_1206">Neural Technology</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurodegeneration" data-event="iBarJournal-sections-a_id_73">Neurodegeneration</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurodevelopment" data-event="iBarJournal-sections-a_id_1944">Neurodevelopment</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroendocrine-science" data-event="iBarJournal-sections-a_id_113">Neuroendocrine Science</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroenergetics-and-brain-health" data-event="iBarJournal-sections-a_id_818">Neuroenergetics and Brain Health</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurogenesis" data-event="iBarJournal-sections-a_id_25">Neurogenesis</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurogenomics" data-event="iBarJournal-sections-a_id_19">Neurogenomics</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuromorphic-engineering" data-event="iBarJournal-sections-a_id_31">Neuromorphic Engineering</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuropharmacology" data-event="iBarJournal-sections-a_id_26">Neuropharmacology</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroprosthetics" data-event="iBarJournal-sections-a_id_23">Neuroprosthetics</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroscience-methods-and-techniques" data-event="iBarJournal-sections-a_id_3022">Neuroscience Methods and Techniques</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/perception-science" data-event="iBarJournal-sections-a_id_41">Perception Science</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/sleep-and-circadian-rhythms" data-event="iBarJournal-sections-a_id_1409">Sleep and Circadian Rhythms</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/social-and-evolutionary-neuroscience" data-event="iBarJournal-sections-a_id_57">Social and Evolutionary Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/translational-neuroscience" data-event="iBarJournal-sections-a_id_2450">Translational Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/visual-neuroscience" data-event="iBarJournal-sections-a_id_2411">Visual Neuroscience</a></li><!--]--></ul><!--]--></div></div></div><!--[--><a class="Ibar__link" href="//www.frontiersin.org/journals/neuroscience/articles" target="_self" data-event="iBar-a-articles">Articles</a><a class="Ibar__link" href="//www.frontiersin.org/journals/neuroscience/research-topics" target="_self" data-event="iBar-a-researchTopics">Research Topics</a><a class="Ibar__link" href="//www.frontiersin.org/journals/neuroscience/editors" target="_self" data-event="iBar-a-editorialBoard">Editorial board</a><!--]--><div parent-data-event="iBarJournal"><div class="Ibar__dropdown"><button class="Ibar__dropdown__trigger"><!----> About journal</button><div class="Ibar__dropdown__menu"><div class="Ibar__dropdown__menu__header"><button class="Ibar__dropdown__menu__header__title" aria-label="Close Dropdown">About journal</button><button class="Ibar__close" aria-label="Close Dropdown"></button></div><!--[--><div class="Ibar__dropdown__about"><!--[--><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title"><span>Scope</span></li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-editors" target="_self" data-event="iBar-aboutJournal_0-a_fieldChiefEditors">Field chief editors</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-scope" target="_self" data-event="iBar-aboutJournal_1-a_mission &amp;Scope">Mission &amp; scope</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-facts" target="_self" data-event="iBar-aboutJournal_2-a_facts">Facts</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-submission" target="_self" data-event="iBar-aboutJournal_3-a_journalSections">Journal sections</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-open" target="_self" data-event="iBar-aboutJournal_4-a_openAccessStatemen">Open access statement</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#copyright-statement" target="_self" data-event="iBar-aboutJournal_5-a_copyrightStatement">Copyright statement</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-quality" target="_self" data-event="iBar-aboutJournal_6-a_quality">Quality</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title"><span>For authors</span></li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/why-submit" target="_self" data-event="iBar-aboutJournal_0-a_whySubmit?">Why submit?</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/article-types" target="_self" data-event="iBar-aboutJournal_1-a_articleTypes">Article types</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/author-guidelines" target="_self" data-event="iBar-aboutJournal_2-a_authorGuidelines">Author guidelines</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/editor-guidelines" target="_self" data-event="iBar-aboutJournal_3-a_editorGuidelines">Editor guidelines</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/publishing-fees" target="_self" data-event="iBar-aboutJournal_4-a_publishingFees">Publishing fees</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/submission-checklist" target="_self" data-event="iBar-aboutJournal_5-a_submissionChecklis">Submission checklist</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/contact-editorial-office" target="_self" data-event="iBar-aboutJournal_6-a_contactEditorialOf">Contact editorial office</a></li><!--]--></ul><!--]--></div><!--]--></div></div></div><div class="Ibar__spacer"></div></div></div><div class="Ibar__journal Ibar__journal--mix"><div class="Ibar__wrapper Ibar__wrapper--journal"><div class="Ibar__logo"><a href="//www.frontiersin.org/" aria-label="Frontiershome" data-event="iBar-a-home" class="Ibar__logo__link"><svg class="Ibar__logo__svg" viewBox="0 0 2811 590" fill="none" xmlns="http://www.w3.org/2000/svg"><path class="Ibar__logo__text" d="M633.872 234.191h-42.674v-57.246h42.674c0-19.776 2.082-35.389 5.204-48.92 4.164-13.53 9.368-23.939 17.695-31.225 8.326-8.326 18.735-13.53 32.266-16.653 13.531-3.123 29.143-5.204 47.878-5.204h21.858c7.286 0 14.572 1.04 21.857 1.04v62.451c-8.326-1.041-16.653-2.082-23.939-2.082-10.408 0-17.694 1.041-23.939 4.164-6.245 3.122-9.368 10.408-9.368 22.898v13.531h53.083v57.246h-53.083v213.372h-89.512V234.191zM794.161 176.945h86.39v47.879h1.041c6.245-17.694 16.653-30.185 31.225-39.552 14.572-9.368 31.225-13.531 49.96-13.531h10.409c3.122 0 7.286 1.041 10.408 2.082v81.185c-6.245-2.082-11.449-3.122-16.653-4.163-5.204-1.041-11.449-1.041-16.654-1.041-11.449 0-20.816 2.082-29.143 5.204-8.327 3.123-15.613 8.327-20.817 14.572-5.204 6.245-10.408 12.49-12.49 20.817-3.123 8.326-4.163 15.612-4.163 23.939v133.228h-88.472V176.945h-1.041zM989.84 312.254c0-19.776 3.122-39.552 10.41-56.205 7.28-17.695 16.65-32.266 29.14-45.797 12.49-13.531 27.06-22.899 44.76-30.185 17.69-7.285 36.43-11.449 57.24-11.449 20.82 0 39.56 4.164 57.25 11.449 17.69 7.286 32.27 17.695 45.8 30.185 12.49 12.49 22.9 28.102 29.14 45.797 7.29 17.694 10.41 36.429 10.41 56.205 0 20.817-3.12 39.552-10.41 57.246-7.29 17.695-16.65 32.266-29.14 44.756-12.49 12.49-28.11 22.899-45.8 30.185-17.69 7.286-36.43 11.449-57.25 11.449-20.81 0-40.59-4.163-57.24-11.449-17.7-7.286-32.27-17.695-44.76-30.185-12.49-12.49-21.86-28.102-29.14-44.756-7.288-17.694-10.41-36.429-10.41-57.246zm88.47 0c0 8.327 1.04 17.694 3.12 26.021 2.09 9.368 5.21 16.653 9.37 23.939 4.16 7.286 9.37 13.531 16.65 17.695 7.29 4.163 15.62 7.285 26.03 7.285 10.4 0 18.73-2.081 26.02-7.285 7.28-4.164 12.49-10.409 16.65-17.695 4.16-7.286 7.29-15.612 9.37-23.939 2.08-9.368 3.12-17.694 3.12-26.021 0-8.327-1.04-17.694-3.12-26.021-2.08-9.368-5.21-16.653-9.37-23.939-4.16-7.286-9.37-13.531-16.65-17.695-7.29-5.204-15.62-7.285-26.02-7.285-10.41 0-18.74 2.081-26.03 7.285-7.28 5.205-12.49 10.409-16.65 17.695-4.16 7.286-7.28 15.612-9.37 23.939-2.08 9.368-3.12 17.694-3.12 26.021zM1306.25 176.945h86.39v37.47h1.04c4.17-7.286 9.37-13.531 15.62-18.735 6.24-5.204 13.53-10.408 20.81-14.572 7.29-4.163 15.62-7.286 23.94-9.367 8.33-2.082 16.66-3.123 24.98-3.123 22.9 0 40.6 4.164 53.09 11.449 13.53 7.286 22.89 16.654 29.14 27.062 6.24 10.409 10.41 21.858 12.49 34.348 2.08 12.49 2.08 22.898 2.08 33.307v172.779h-88.47V316.417v-27.061c0-9.368-1.04-16.654-4.16-23.94-3.13-7.286-7.29-12.49-13.53-16.653-6.25-4.164-15.62-6.245-27.07-6.245-8.32 0-15.61 2.081-21.85 5.204-6.25 3.122-11.45 7.286-14.58 13.531-4.16 5.204-6.24 11.449-8.32 18.735s-3.12 14.572-3.12 21.858v145.717h-88.48V176.945zM1780.88 234.19h-55.17v122.819c0 10.408 3.12 17.694 8.33 20.817 6.24 3.122 13.53 5.204 22.9 5.204 4.16 0 7.28 0 11.45-1.041h11.45v65.573c-8.33 0-15.62 1.041-23.94 2.082-8.33 1.04-16.66 1.041-23.94 1.041-18.74 0-34.35-2.082-46.84-5.205-12.49-3.122-21.86-8.326-29.14-15.612-7.29-7.286-12.49-16.654-14.58-29.144-3.12-12.49-4.16-27.062-4.16-45.797V234.19h-44.76v-57.246h44.76V94.717h88.47v82.227h55.17v57.246zM1902.66 143.639h-88.48V75.984h88.48v67.655zm-89.52 33.307h88.48v270.618h-88.48V176.946zM2024.43 334.111c1.04 18.735 6.25 33.307 16.66 44.756 10.4 11.449 24.98 16.653 43.71 16.653 10.41 0 20.82-2.081 30.19-7.286 9.36-5.204 16.65-12.49 20.81-22.898h83.27c-4.16 15.613-10.41 29.144-19.78 40.593-9.36 11.449-19.77 20.817-31.22 28.102-12.49 7.286-24.98 12.491-39.55 16.654-14.57 3.122-29.15 5.204-43.72 5.204-21.86 0-41.63-3.122-60.37-9.367-18.73-6.246-34.34-15.613-46.83-28.103-12.49-12.49-22.9-27.062-30.19-45.797-7.28-17.694-10.41-38.511-10.41-60.369 0-20.817 4.17-39.552 11.45-57.246 7.29-17.694 17.7-32.266 31.23-44.756 13.53-12.49 29.14-21.858 46.83-29.144 17.7-7.286 36.43-10.408 56.21-10.408 23.94 0 45.8 4.163 63.49 12.49 17.7 8.327 33.31 19.776 44.76 35.389 11.45 15.612 20.81 32.266 26.02 52.042 5.2 19.776 8.33 41.633 7.28 64.532h-199.84v-1.041zm110.33-49.961c-1.04-15.612-6.24-28.102-15.61-39.551-9.37-10.409-21.86-16.654-37.47-16.654s-28.1 5.204-38.51 15.613c-10.41 10.408-16.66 23.939-18.74 40.592h110.33zM2254.46 176.945h86.39v47.879h1.04c6.25-17.694 16.65-30.185 31.23-39.552 14.57-9.368 31.22-13.531 49.96-13.531h10.4c3.13 0 7.29 1.041 10.41 2.082v81.185c-6.24-2.082-11.45-3.122-16.65-4.163-5.21-1.041-11.45-1.041-16.65-1.041-11.45 0-20.82 2.082-29.15 5.204-8.32 3.123-15.61 8.327-20.81 14.572-6.25 6.245-10.41 12.49-12.49 20.817-3.13 8.326-4.17 15.612-4.17 23.939v133.228h-88.47V176.945h-1.04zM2534.45 359.091c0 7.286 1.04 12.49 4.16 17.694 3.12 5.204 6.24 9.368 10.41 12.49 4.16 3.123 9.36 5.204 14.57 7.286 6.24 2.082 11.45 2.082 17.69 2.082 4.17 0 8.33 0 13.53-2.082 5.21-1.041 9.37-3.123 13.53-5.204 4.17-2.082 7.29-5.204 10.41-9.368 3.13-4.163 4.17-8.327 4.17-13.531 0-5.204-2.09-9.367-5.21-12.49-3.12-3.122-7.28-6.245-11.45-8.327-4.16-2.081-9.36-4.163-14.57-5.204-5.2-1.041-9.37-2.081-13.53-3.122-13.53-3.123-28.1-6.245-42.67-9.368-14.58-3.122-28.11-7.286-40.6-12.49-12.49-6.245-22.9-13.531-30.18-23.939-8.33-10.409-11.45-23.94-11.45-42.675 0-16.653 4.16-30.184 11.45-40.592 8.33-10.409 17.69-18.736 30.18-24.981 12.49-6.245 26.02-10.408 40.6-13.53 14.57-3.123 28.1-4.164 41.63-4.164 14.57 0 29.14 1.041 43.71 4.164 14.58 2.081 27.07 7.285 39.56 13.53 12.49 6.245 21.85 15.613 29.14 27.062 7.29 11.45 11.45 26.021 12.49 43.716h-82.23c0-10.409-4.16-18.736-11.45-23.94-7.28-4.163-16.65-7.286-28.1-7.286-4.16 0-8.32 0-12.49 1.041-4.16 1.041-8.32 1.041-12.49 2.082-4.16 1.041-7.28 3.122-9.37 6.245-2.08 3.122-4.16 6.245-4.16 11.449 0 6.245 3.12 11.449 10.41 15.613 6.24 4.163 14.57 7.286 24.98 10.408 10.41 2.082 20.82 5.204 32.27 7.286 11.44 2.082 22.89 4.163 33.3 6.245 13.53 3.123 24.98 7.286 33.31 13.531 9.37 6.245 15.61 12.49 20.82 19.776 5.2 7.286 9.36 14.572 11.45 21.858 2.08 7.285 3.12 13.53 3.12 19.776 0 17.694-4.17 33.306-11.45 45.796-8.33 12.491-17.7 21.858-30.19 30.185-12.49 7.286-26.02 12.49-41.63 16.653-15.61 3.123-31.22 5.204-45.8 5.204-15.61 0-32.26-1.04-47.87-4.163-15.62-3.122-29.15-8.327-41.64-15.612a83.855 83.855 0 01-30.18-30.185c-8.33-12.49-12.49-28.102-12.49-46.838h84.31v-2.081z" fill="#FFFFFF"></path><path d="M0 481.911V281.028l187.351-58.287v200.882L0 481.911z" fill="#8BC53F"></path><path d="M187.351 423.623V222.741l126.983 87.431v200.882l-126.983-87.431z" fill="#EBD417"></path><path d="M126.982 569.341L0 481.911l187.351-58.287 126.983 87.43-187.352 58.287z" fill="#034EA1"></path><path d="M183.188 212.331l51.001-116.574 65.573 155.085-51.001 116.574-65.573-155.085z" fill="#712E74"></path><path d="M248.761 367.415l51.001-116.574 171.739-28.102-49.96 115.533-172.78 29.143z" fill="#009FD1"></path><path d="M299.762 250.842L234.189 95.757l171.739-28.103 65.573 155.085-171.739 28.103z" fill="#F6921E"></path><path d="M187.352 222.741L59.328 198.802 44.757 71.819 172.78 95.76l14.572 126.982z" fill="#DA2128"></path><path d="M172.78 95.758L44.757 71.818l70.777-70.776 128.023 23.94-70.777 70.776z" fill="#25BCBD"></path><path d="M258.129 153.005l-70.777 69.736-14.571-126.982 70.777-70.778 14.571 128.024z" fill="#00844A"></path></svg></a></div><a href="//www.frontiersin.org/journals/neuroscience" class="Ibar__journalName" data-event="iBarJournal-a-journalHome"><div class="Ibar__journalName__container" logoclass="Ibar__logo--mixed"><div class="Ibar__journal__maskLogo" style="display:none;"><img class="Ibar__journal__logo" src></div><div class="Ibar__journalName"><span>Frontiers in</span><span> Neuroscience</span></div></div></a><div class="Ibar__spacer"></div><div parent-data-event="iBarJournal"><div class="Ibar__dropdown"><button class="Ibar__dropdown__trigger"><!----> Sections</button><div class="Ibar__dropdown__menu"><div class="Ibar__dropdown__menu__header"><button class="Ibar__dropdown__menu__header__title" aria-label="Close Dropdown">Sections</button><button class="Ibar__close" aria-label="Close Dropdown"></button></div><!--[--><ul class="Ibar__dropdown__sections"><!--[--><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/auditory-cognitive-neuroscience" data-event="iBarJournal-sections-a_id_65">Auditory Cognitive Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/autonomic-neuroscience" data-event="iBarJournal-sections-a_id_157">Autonomic Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/brain-imaging-methods" data-event="iBarJournal-sections-a_id_600">Brain Imaging Methods</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/decision-neuroscience" data-event="iBarJournal-sections-a_id_33">Decision Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/gut-brain-axis" data-event="iBarJournal-sections-a_id_2416">Gut-Brain Axis</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neural-technology" data-event="iBarJournal-sections-a_id_1206">Neural Technology</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurodegeneration" data-event="iBarJournal-sections-a_id_73">Neurodegeneration</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurodevelopment" data-event="iBarJournal-sections-a_id_1944">Neurodevelopment</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroendocrine-science" data-event="iBarJournal-sections-a_id_113">Neuroendocrine Science</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroenergetics-and-brain-health" data-event="iBarJournal-sections-a_id_818">Neuroenergetics and Brain Health</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurogenesis" data-event="iBarJournal-sections-a_id_25">Neurogenesis</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neurogenomics" data-event="iBarJournal-sections-a_id_19">Neurogenomics</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuromorphic-engineering" data-event="iBarJournal-sections-a_id_31">Neuromorphic Engineering</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuropharmacology" data-event="iBarJournal-sections-a_id_26">Neuropharmacology</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroprosthetics" data-event="iBarJournal-sections-a_id_23">Neuroprosthetics</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/neuroscience-methods-and-techniques" data-event="iBarJournal-sections-a_id_3022">Neuroscience Methods and Techniques</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/perception-science" data-event="iBarJournal-sections-a_id_41">Perception Science</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/sleep-and-circadian-rhythms" data-event="iBarJournal-sections-a_id_1409">Sleep and Circadian Rhythms</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/social-and-evolutionary-neuroscience" data-event="iBarJournal-sections-a_id_57">Social and Evolutionary Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/translational-neuroscience" data-event="iBarJournal-sections-a_id_2450">Translational Neuroscience</a></li><li class="Ibar__dropdown__sections__item"><a href="/journals/neuroscience/sections/visual-neuroscience" data-event="iBarJournal-sections-a_id_2411">Visual Neuroscience</a></li><!--]--></ul><!--]--></div></div></div><!--[--><a class="Ibar__link" href="//www.frontiersin.org/journals/neuroscience/articles" target="_self" data-event="iBar-a-articles">Articles</a><a class="Ibar__link" href="//www.frontiersin.org/journals/neuroscience/research-topics" target="_self" data-event="iBar-a-researchTopics">Research Topics</a><a class="Ibar__link" href="//www.frontiersin.org/journals/neuroscience/editors" target="_self" data-event="iBar-a-editorialBoard">Editorial board</a><!--]--><div parent-data-event="iBarJournal"><div class="Ibar__dropdown"><button class="Ibar__dropdown__trigger"><!----> About journal</button><div class="Ibar__dropdown__menu"><div class="Ibar__dropdown__menu__header"><button class="Ibar__dropdown__menu__header__title" aria-label="Close Dropdown">About journal</button><button class="Ibar__close" aria-label="Close Dropdown"></button></div><!--[--><div class="Ibar__dropdown__about"><!--[--><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title"><span>Scope</span></li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-editors" target="_self" data-event="iBar-aboutJournal_0-a_fieldChiefEditors">Field chief editors</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-scope" target="_self" data-event="iBar-aboutJournal_1-a_mission &amp;Scope">Mission &amp; scope</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-facts" target="_self" data-event="iBar-aboutJournal_2-a_facts">Facts</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-submission" target="_self" data-event="iBar-aboutJournal_3-a_journalSections">Journal sections</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-open" target="_self" data-event="iBar-aboutJournal_4-a_openAccessStatemen">Open access statement</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#copyright-statement" target="_self" data-event="iBar-aboutJournal_5-a_copyrightStatement">Copyright statement</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/about#about-quality" target="_self" data-event="iBar-aboutJournal_6-a_quality">Quality</a></li><!--]--></ul><ul class="Ibar__dropdown__about__block"><li class="Ibar__dropdown__about__block__title"><span>For authors</span></li><!--[--><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/why-submit" target="_self" data-event="iBar-aboutJournal_0-a_whySubmit?">Why submit?</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/article-types" target="_self" data-event="iBar-aboutJournal_1-a_articleTypes">Article types</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/author-guidelines" target="_self" data-event="iBar-aboutJournal_2-a_authorGuidelines">Author guidelines</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/editor-guidelines" target="_self" data-event="iBar-aboutJournal_3-a_editorGuidelines">Editor guidelines</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/publishing-fees" target="_self" data-event="iBar-aboutJournal_4-a_publishingFees">Publishing fees</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/submission-checklist" target="_self" data-event="iBar-aboutJournal_5-a_submissionChecklis">Submission checklist</a></li><li class="Ibar__dropdown__about__block__item"><a href="https://www.frontiersin.org/journals/neuroscience/for-authors/contact-editorial-office" target="_self" data-event="iBar-aboutJournal_6-a_contactEditorialOf">Contact editorial office</a></li><!--]--></ul><!--]--></div><!--]--></div></div></div><div class="Ibar__spacer"></div><a class="Ibar__button Ibar__submit" href="https://www.frontiersin.org/submission/submit?domainid=1&amp;fieldid=55&amp;specialtyid=0&amp;entitytype=2&amp;entityid=1" data-event="iBarJournal-a-submit"><span>Submit</span><span> your research</span></a><a class="Ibar__icon Ibar__icon--search" href="//www.frontiersin.org/search" aria-label="Search" target="_self" data-event="iBar-a-search"><span>Search</span></a><!----><!----><!----><div class="Ibar__userArea"></div></div></div><!--]--></nav><div class="ArticlePage"><!--[--><div class="Layout Layout--withAside Layout--withIbarMix"><div class="Alert Alert--info Alert--noInfo ArticleTemplateBanner"><div class="Alert__icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 48 48" width="24px" class="FeedbackIcon"><path fill="#fff" d="M10 10h28v28H10z"></path><path fill="var(--blue50)" d="M24 2.88a21.12 21.12 0 1 0 0 42.24 21.12 21.12 0 0 0 0-42.24ZM22.08 13.4a.96.96 0 0 1 .96-.96h1.92a.96.96 0 0 1 .96.96v1.92a.96.96 0 0 1-.96.96h-1.92a.96.96 0 0 1-.96-.96V13.4Zm5.76 21.2a.96.96 0 0 1-.96.97h-5.76a.96.96 0 0 1-.96-.96v-1.92a.96.96 0 0 1 .96-.96h.96v-7.68h-.96a.96.96 0 0 1-.96-.96v-1.92a.96.96 0 0 1 .96-.96h3.84a.96.96 0 0 1 .96.96v10.56h.96a.96.96 0 0 1 .96.96v1.92Z"></path></svg></div><div class="Alert__main"><p class="Alert__message">Your new experience awaits. Try the new design now and help us make it even better</p><!--[--><button type="button" class="Button Button--outline Button--grey80 Button--small ArticleTemplateBanner__switchButton" data-event="btn-action"><span>Switch to the new experience</span></button><!--]--><!----></div><ol class="Alert__info"><!--[--><!--]--></ol></div><!----><main class="Layout__main"><!----><div class="ArticleDetails"><div class="ArticleLayoutHeader"><div class="ArticleLayoutHeader__info"><p class="ArticleLayoutHeader__info__title">REVIEW article</p><p class="ArticleLayoutHeader__info__journalDate"><span>Front. Neurosci.</span><span>, 04 September 2024</span></p><p class="ArticleLayoutHeader__info__journalDate"> Sec. Auditory Cognitive Neuroscience</p><p class="ArticleLayoutHeader__info__doiVolume"><span>Volume 18 - 2024 | </span><a class="ArticleLayoutHeader__info__doi" href="https://doi.org/10.3389/fnins.2024.1400444">https://doi.org/10.3389/fnins.2024.1400444</a></p><!----></div><!----><!----></div><div class="ArticleDetails__main__content"><div class="ArticleDetails__main__content__main ArticleDetails__main__content__main--fullArticle"><div class="JournalAbstract"><div class="JournalAbstract__titleWrapper"><h1>A review of artificial intelligence methods enabled music-evoked EEG emotion recognition and their applications</h1><!----></div><!----></div><div class="JournalFullText"><div class="JournalAbstract"> <a id="h1" name="h1"></a>
 <div class="authors"><span class="author-wrapper notranslate"><a href="https://loop.frontiersin.org/people/2684571" class="user-id-2684571"><img class="pr5" src="https://loop.frontiersin.org/images/profile/2684571/74" onerror="this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';" alt="Yan Su">Yan Su</a><sup>1</sup></span><span class="author-wrapper notranslate"><img class="pr5" src="https://loop.frontiersin.org/cdn/images/profile/default_32.jpg" alt="Yong Liu" onerror="this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';">Yong Liu<sup>2</sup></span><span class="author-wrapper notranslate"><img class="pr5" src="https://loop.frontiersin.org/cdn/images/profile/default_32.jpg" alt="Yan Xiao" onerror="this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';">Yan Xiao<sup>3</sup></span><span class="author-wrapper notranslate"><a href="https://loop.frontiersin.org/people/2685361" class="user-id-2685361"><img class="pr5" src="https://loop.frontiersin.org/images/profile/2685361/74" onerror="this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';" alt="Jiaqi Ma">Jiaqi Ma</a><sup>4</sup></span><span class="author-wrapper notranslate"><a href="https://loop.frontiersin.org/people/506405" class="user-id-506405"><img class="pr5" src="https://loop.frontiersin.org/images/profile/506405/74" onerror="this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';" alt="Dezhao Li&#xA;">Dezhao Li</a><sup>4</sup><sup>&#x0002A;</sup></span></div> <ul class="notes"> <li><span><sup>1</sup></span>School of Art, Zhejiang International Studies University, Hangzhou, China</li> <li><span><sup>2</sup></span>School of Education, Hangzhou Normal University, Hangzhou, China</li> <li><span><sup>3</sup></span>School of Arts and Media, Beijing Normal University, Beijing, China</li> <li><span><sup>4</sup></span>College of Science, Zhejiang University of Technology, Hangzhou, China</li> </ul>
<p>Music is an archaic form of emotional expression and arousal that can induce strong emotional experiences in listeners, which has important research and practical value in related fields such as emotion regulation. Among the various emotion recognition methods, the music-evoked emotion recognition method utilizing EEG signals provides real-time and direct brain response data, playing a crucial role in elucidating the neural mechanisms underlying music-induced emotions. Artificial intelligence technology has greatly facilitated the research on the recognition of music-evoked EEG emotions. AI algorithms have ushered in a new era for the extraction of characteristic frequency signals and the identification of novel feature signals. The robust computational capabilities of AI have provided fresh perspectives for the development of innovative quantitative models of emotions, tailored to various emotion recognition paradigms. The discourse surrounding AI algorithms in the context of emotional classification models is gaining momentum, with their applications in music therapy, neuroscience, and social activities increasingly coming under the spotlight. Through an in-depth analysis of the complete process of emotion recognition induced by music through electroencephalography (EEG) signals, we have systematically elucidated the influence of AI on pertinent research issues. This analysis offers a trove of innovative approaches that could pave the way for future research endeavors.</p> <div class="clear"></div> </div> <div class="JournalFullText"> <a id="h2" name="h2"></a>
<h2>1 Introduction</h2>
<p class="mb15">Music serves as a unique medium for people to express their emotions and also can arouse strong emotional responses. Previous studies have shown that the emotional changes induced by appropriate music can relieve listeners&#x2019; mental stress (<a href="#ref65">Nawaz et al., 2019</a>; <a href="#ref19">Colin et al., 2023</a>), promote emotional expression ability (<a href="#ref67">Palazzi et al., 2021</a>; <a href="#ref63">Micallef Grimaud and Eerola, 2022</a>; <a href="#ref112">Zhang et al., 2022</a>), improve learning ability (<a href="#ref12">Bergee and Weingarten, 2021</a>; <a href="#ref57">Luo et al., 2023</a>), and so on. Moreover, it also can be applied in the regulation of mood-related disorders such as autism (<a href="#ref16">Carpente et al., 2022</a>; <a href="#ref33">Geretsegger et al., 2022</a>), depression (<a href="#ref32">Geipel et al., 2022</a>; <a href="#ref37">Hartmann et al., 2023</a>), and anxiety (<a href="#ref20">Contreras-Molina et al., 2021</a>; <a href="#ref56">Lu et al., 2021</a>). With the extensive applications of music-induced emotions in medical (<a href="#ref53">Liang et al., 2021</a>), neuroscience (<a href="#ref94">Thaut et al., 2021</a>; <a href="#ref30">Fedotchev et al., 2022</a>), and music retrieval fields (<a href="#ref34">Gomez-Canon et al., 2021</a>), the study of music-induced emotion recognition has received much attention in recent years.</p>
<p class="mb0">Empirical research on the effects of music on emotions has been discussed for more than three millennia (<a href="#ref72">Perlovsky, 2012</a>), while modern evidence-based work on the effects of music on emotions has its roots in the early 20th century (<a href="#ref40">Humphreys, 1998</a>). Western psychologists and musicians primarily conducted pioneering empirical research on music-induced emotions. A representative example is the experimental research conducted by the American psychologist and music educator Carl Emil Seashore on the emotional expression of music and the emotional impact of music on the listener, combining experiments and psychological tests and proposing the &#x201C;theory of musical expression,&#x201D; which emphasizes how elements such as melodies, rhythms, and harmonies of music affect people&#x2019;s emotional experiences, and lays the foundation for the subsequent development of related work (<a href="#ref62">Metfessel, 1950</a>). With the development of psychology neuroscience and other fields, people gradually realized that the study of music&#x2019;s induction of emotions also requires an understanding of auditory perception, emotion discrimination, and neural mechanism, which is an interdisciplinary research work (<a href="#ref21">Cui et al., 2022</a>; <a href="#ref78">Ryczkowska, 2022</a>). Musicologists have mainly studied the influence of music on emotion induction from the perspective of different musical features of music (<a href="#ref69">Panda et al., 2023</a>), including analyzing the influence of music on the listener&#x2019;s emotion from the perspective of musical elements (<a href="#ref77">Ruth and Schramm, 2021</a>), quantitatively analyzing the emotional features of music to find the relationship between the features and emotion (<a href="#ref80">Salakka et al., 2021</a>), developing emotion recognition algorithms based on musical features (<a href="#ref70">Pandey and Seeja, 2022</a>), and exploring cross-cultural emotional understanding of and response to specific musical features (<a href="#ref100">Wang et al., 2022</a>). These studies have made it possible to help people get a better understanding of the relationship between musical features and emotions, and provide theoretical support and practical guidance for the fields of music psychology, music therapy, and creativity, but researchers have also put forward different viewpoints on the individual differences in musical emotional responses and on the objective evaluative validity of emotions as in <a href="#fig1">Figure 1</a>.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Figure 1</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg" name="figure1" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg" alt="www.frontiersin.org" id="fig1" loading="lazy">
  </picture>
</a>
<p><b>Figure 1</b>. Research and applications of music-induced emotion classification with EEG.</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15">With the development of brain science and technology, researchers have found that signals generated by the central nervous system, such as electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (MRI) are more objective and reliable in the field of emotion research (<a href="#ref3">Alarcao and Fonseca, 2019</a>; <a href="#ref27">Egger et al., 2019</a>; <a href="#ref79">Saganowski et al., 2023</a>). Among various central nervous system signals, the monitoring of emotions using EEG signals is characterized by the convenience of noninvasive measurements, real-time measurements, and good objectivity. Research on emotion recognition based on EEG signals has been widely used in many disciplines in recent years and has received extensive attention from researchers as in <a href="#fig1">Figure 1</a>. Artificial intelligence (AI) techniques that integrate EEG signals for identifying emotions elicited by music leverage AI&#x2019;s robust capabilities in data analytics, pattern recognition, and learning, alongside the distinctive benefits of EEG for real-time, non-invasive monitoring of brain activity. AI-enabled EEG recognition of music-induced emotions can accurately and in real-time identify emotions, which has broad applications in many areas including music therapy, education, entertainment, and so on.</p>
<p class="mb15">How to accurately identify music-induced emotions has always been a difficult research problem due to the subjectivity, abstractness, and individualized differences of music-induced emotions. Researchers have explored a variety of physiological signals to carry out emotion recognition studies, in which using the signal characteristics of facial expressions, researchers have classified emotions including fear, sadness, disgust, surprise, and joy, and the accuracy of obtaining emotion discrimination can be as high as 81% or more, but there are inconsistencies between different cultures in the understanding of facial expressions and the way of expression of facial expressions, which affect the generalizability of the results of the study (<a href="#ref93">Tcherkassof and Dupr&#x000E9;, 2021</a>; <a href="#ref101">Witkower et al., 2021</a>). Physiological parameters such as galvanic skin response, heart rate, temperature, blood pressure, and respiration rate have also been utilized for emotion recognition, but these methods are relatively inaccurate for emotion discrimination and highly influenced by other factors (<a href="#ref27">Egger et al., 2019</a>; <a href="#ref79">Saganowski et al., 2023</a>).</p>
<p class="mb0">In this study, the research methods, processes, and characteristics of EEG in music-induced emotion recognition have been analyzed. The potential future development directions of music-induced emotion based on EEG also have been discussed, which can promote the development of fundamental and application research on music-induced emotion.</p> <a id="h3" name="h3"></a>
<h2>2 EEG signal and emotions</h2>
<p class="mb0">Measurement of EEG signals is capable of non-invasive, continuous recording of brain activity with a temporal resolution of a few milliseconds. Based on the characteristic waveform signals from different brain regions, EEG signals are widely used in cognitive neuroscience to research emotion regulation and processing, and the results of the related studies provide an important reference for further research on music-induced emotion recognition (<a href="#ref7">Apicella et al., 2022b</a>; <a href="#ref70">Pandey and Seeja, 2022</a>).</p>
<h3>2.1 EEG signals and acquisition method</h3>
<p class="mb0">The activity of the central nervous system of the brain is closely related to human emotions, mainly realized through electrical communication between neurons (<a href="#ref2">Ahmad et al., 2022</a>). When neurons are stimulated, the membrane potential will rise and fall to form weak electrical pulse signals and emotional changes can be monitored by recording and analyzing EEG signals. EEG signals have the characteristics of small amplitude (10&#x2013;100&#x2009;&#x03BC;V), many interference sources, and high uncertainty. To analyze the feature information of EEG, as shown in <a href="#tab1">Table 1</a>, EEG signals are generally classified into &#x03B4;-band (1&#x2013;4&#x2009;Hz), &#x03B8;-band (4&#x2013;8&#x2009;Hz), &#x03B1;-band (8&#x2013;13&#x2009;Hz), &#x03B2;-1-band (13&#x2013;22&#x2009;Hz), &#x03B2;-2-band (22&#x2013;30&#x2009;Hz) and &#x03B3;-band (30&#x2013;64&#x2009;Hz) (<a href="#ref3">Alarcao and Fonseca, 2019</a>), and the frequency bands of the bands are divided into slightly different bands by different researchers.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Table 1</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg" name="table1" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg" alt="www.frontiersin.org" id="tab1" loading="lazy">
  </picture>
</a>
<p><b>Table 1</b>. Bands of EEG signals.
</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15">Research has shown that the five bands of EEG signals mentioned above are directly or indirectly related to human emotions. While early studies suggested that the &#x03B4;-band was not connected to people&#x2019;s emotions, recent research has found that &#x03B4; wave is closely associated with the emotional state of individuals following emotional regulation and holds promise for use in areas such as music therapy for emotion regulation (<a href="#ref46">Lapomarda et al., 2022</a>).</p>
<p class="mb15">The acquisition method of EEG signals is mainly categorized into invasive and non-invasive techniques. Invasive measurements require surgical implantation of electrodes to obtain clearer EEG signals, but this method is traumatized to the human body and difficult to widely apply, which is mainly used in clinical medical treatment. Non-invasive is to fit the electrodes to the surface of the head to collect brain signals.</p>
<p class="mb0">Previous research has demonstrated the significance of the limbic system (as in <a href="#fig2">Figure 2A</a>) in regulating human emotions, making it a pivotal area of interest in the field of emotion research (<a href="#ref75">Rolls, 2015</a>). To obtain more comprehensive brain signals, the internationally recognized 10/20 system, shown in <a href="#fig2">Figure 2B</a>, is generally used in the arrangement of electrodes, i.e., the actual distance of adjacent electrodes is 10% or 20% of the distance of the brain skull (<a href="#ref86">Silverman, 1963</a>). In the field of emotion recognition, multi-channel EEG acquisition is commonly utilized, featuring electrode channels ranging from 36 to 64, and a sampling frequency of 500 or 1,000&#x2009;Hz (<a href="#ref102">Wu et al., 2024</a>). Traditional EEG acquisition system devices are often cumbersome and expensive, which hinders their widespread adoption and use. With the advancement of open source technologies like OpenBCI as in <a href="#fig2">Figure 2C</a> and other EEG acquisition devices, more affordable, user-friendly, and portable options have emerged. Recently, these devices have become increasingly popular in EEG emotion recognition research (<a href="#ref4">Aldridge et al., 2019</a>). To investigate the mechanisms of timely emotional response, both the stimulus source and the acquisition system are typically equipped with time-synchronization devices (<a href="#ref71">Pei et al., 2024</a>).</p>
<div class="DottedLine"></div> <div class="Imageheaders">Figure 2</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg" name="figure2" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg" alt="www.frontiersin.org" id="fig2" loading="lazy">
  </picture>
</a>
<p><b>Figure 2</b>. <b>(A)</b> The limbic system for emotion, <b>(B)</b> the international 10/20 system with nine cortical regions labeled with different colors, <b>(C)</b> structure of a typical EEG system from OpenBCI.</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb0">In music emotion recognition research, non-invasive acquisition schemes are commonly employed. In recent years, wireless wearable non-invasive EEG measurement devices have greatly facilitated EEG-based emotion recognition research (<a href="#ref6">Apicella et al., 2022a</a>; <a href="#ref79">Saganowski et al., 2023</a>). The emergence of these novel EEG acquisition protocols has significant implications for expanding the scope of EEG emotional applications.</p>
<h3>2.2 EEG signal bands corresponding to different emotions</h3>
<p class="mb15">The brain exhibits diverse EEG response patterns for different emotions, and establishing the relationship between EEG signal bands and various emotional states is a crucial foundation for developing effective classification and recognition models. This correspondence serves as one of the key scientific challenges in the domain of artificial intelligence-based recognition of music-induced emotions. Around 1980, researchers found that EEG&#x2019;s characteristic signals correlate with human emotional states (<a href="#ref24">Davidson and Fox, 1982</a>), as in <a href="#tab1">Table 1</a>. Subsequently, researchers have investigated the relationship between distinct brainwave frequency bands and diverse emotional states. In 2001, Louis A. Schmidt et al. presented that emotions within valence can be distinguished by evaluating the asymmetry and overall power of &#x03B1;-band (8&#x2013;13&#x2009;Hz) from frontal brain EEG signals (<a href="#ref84">Schmidt and Trainor, 2001</a>). In 2007, Daniela Sammler et al. conducted a systematic analysis of the correlation between various EEG frequency bands and emotions. Their findings revealed that &#x03B8;-band (4&#x2013;8&#x2009;Hz) power in the prefrontal lobe is more prominent during happy music stimulation, indicating its significance in emotional processing (<a href="#ref81">Sammler et al., 2007</a>). With the continuous advancement of EEG analysis technology, it has become increasingly apparent that the intricate nature of the brain&#x2019;s emotional processes makes it challenging to establish precise correlations between different emotions and signals derived from a single brain region or waveform. Certainly, in some specific scenarios, researchers continue to explore and identify the most prominent EEG frequency bands to simplify the challenges associated with emotion recognition.</p>
<p class="mb0">The application of artificial intelligence in the emotional recognition of music-induced electroencephalography (EEG) holds significant value in two primary aspects. On one hand, the utilization of AI algorithms assists researchers in discerning and selecting the appropriate frequency bands amidst a multitude of options. On the other hand, the deployment of AI algorithms facilitates the exploration of additional effective frequency bands, enhancing the depth and breadth of research in this domain. With the development of artificial intelligence and deep learning technologies, emotion recognition by utilizing various frequency band features from different brain regions has emerged as a prominent and contemporary approach (<a href="#ref71">Pei et al., 2024</a>; <a href="#ref105">Xu et al., 2024</a>). Machine learning based Support Vector Machine (SVM) (<a href="#ref9">Bagherzadeh et al., 2023</a>), Na&#x000EF;ve Bayes (NB) (<a href="#ref66">Oktavia et al., 2019</a>), and K Nearest Neighbors (KNN) (<a href="#ref83">Sari et al., 2023</a>) classifier methods have been applied in this field. Deep learning based classification methods such as Convolutional Neural Networks (CNN) (<a href="#ref107">Yang et al., 2019</a>), Recurrent Neural Networks (RNN) (<a href="#ref115">Zhong et al., 2023</a>), Long-Short-Term Memory (LSTM) (<a href="#ref25">Du et al., 2022</a>), and other classification methods have also been used in EEG recognition studies.</p> <a id="h4" name="h4"></a>
<h2>3 Preprocessing and feature extraction of EEG signals</h2>
<p class="mb0">Extracting effective emotional state information from raw EEG signals is a highly challenging task, given that the signal is a multi-frequency non-stationary signal. EEG preprocessing and feature extraction are essential steps in the recognition algorithms for effective analysis and interpretation of the EEG signals. The purpose of preprocessing EEG signals is to eliminate human motion interference and environmental noise that are unrelated to emotion pattern recognition. This is essential for enhancing the accuracy and robustness of the recognition algorithm.</p>
<h3>3.1 EEG preprocessing</h3>
<p class="mb15">Noise removal is a crucial objective of EEG signal preprocessing. EEG signals are often vulnerable to interference from various sources such as environmental electromagnetic signals (~50&#x2013;60&#x2009;Hz), eye movements (~4&#x2009;Hz), electrocardiogram signals (ECG, ~1.2&#x2009;Hz), and so on.</p>
<p class="mb0">The removal of these noises can significantly enhance the robustness of the EEG model. Usually, these disturbing signals can be filtered out with band-pass filters, wavelet packet filtering, or independent component analysis (ICA) methods as in <a href="#tab2">Table 2</a>. However, researchers have different opinions regarding the signal filtering methods in preprocessing. Some argue that these methods do not eliminate interfering noise, while others believe that these techniques remove noise at the expense of potentially discarding valuable EEG information. To further improve the denoising performance, the artifact subspace reconstruction (ASR) method can be applied to remove the artificial signals. What&#x2019;s more, the average value of overall electrodes can be applied to subtract from each channel to reduce the system noise (<a href="#ref41">Katsigiannis and Ramzan, 2018</a>). Compared to classical machine learning algorithms, deep learning classification methods for emotion recognition are less influenced by the effects of preprocessing techniques.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Table 2</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg" name="table2" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg" alt="www.frontiersin.org" id="tab2" loading="lazy">
  </picture>
</a>
<p><b>Table 2</b>. Benefits and drawbacks of EEG preprocessing methods.
</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb0">The most popular open source toolbox for EEG preprocessing is EEGLAB running in the MATLAB environment (<a href="#ref60">Mart&#x000ED;nez-Saez et al., 2024</a>; <a href="#ref71">Pei et al., 2024</a>; <a href="#ref102">Wu et al., 2024</a>). This interactive toolbox can be applied to process continuous and event-related EEG signals. Moreover, the artifacts from eye movements can be removed with the run independent component analysis (RunICA) algorithm incorporated in EEGLAB based on the independent component analysis (ICA) method (<a href="#ref102">Wu et al., 2024</a>). The expansion of artificial intelligence has led to the integration of EEG signal preprocessing algorithms into a growing array of commercial AI development platforms, including Python, Brainstorm, and Curry.</p>
<h3>3.2 Time domain feature extraction</h3>
<p class="mb15">Emotional changes in the brain can be influenced by musical stimulation, leading to observable effects on EEG signals. These EEG signals exhibit various time-dependent features, which can be analyzed in the time domain. Time-domain features provide intuitive insights and are relatively easy to obtain. Some categories of time-domain features in EEG analysis include event-related potentials (ERPs), statistical features (such as mean, average, standard deviation, skewness, kurtosis, etc.), rise and fall times, and burst suppression (<a href="#ref89">Stancin et al., 2021</a>; <a href="#ref49">Li et al., 2022b</a>).</p>
<p class="mb0">Time domain features can intuitively capture changes in brain states following music-induced emotions. The active regions corresponding to these emotions can typically be promptly identified through intuitive brain area distribution maps, as in <a href="#fig3">Figure 3</a>, offering valuable insights for the improvement of recognition algorithms. Time domain features are generally preferred in emotion recognition research.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Figure 3</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg" name="figure3" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg" alt="www.frontiersin.org" id="fig3" loading="lazy">
  </picture>
</a>
<p><b>Figure 3</b>. Topographical maps of EEG signals for different types of music (<a href="#ref104">Xu J. et al., 2023</a>).</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15">Moreover, event-related potentials (ERPs) are specific patterns of EEG activity that are time-locked to particular sensory, cognitive, or motor events (<a href="#ref61">Martins et al., 2022</a>). They reflect the brain&#x2019;s response to stimuli and provide valuable information about cognitive processes, which is very helpful in studying the dynamic processes of emotion change with music stimuli. Rise and fall times refer to the duration it takes for the EEG signal to rise from its baseline to its peak (rise time) or fall back to the baseline (fall time). These measures provide insights into the speed of neural activation or deactivation. Currently, there is a relatively limited body of research on the speed, duration, and recovery time of human emotions stimulated by music. It is important to dedicate attention to these aspects in future studies to gain a deeper understanding of the relevant phenomena with the time domain feature of rise and fall times.</p>
<p class="mb0">By examining these time-domain features of EEG signals, researchers can gain a better understanding of the temporal dynamics of brain activity related to emotional responses to music. The deployment of artificial intelligence algorithms enables the real-time identification of emotions induced by music via EEG signals. Making well-informed choices and applying time-domain features effectively is essential for advancing these studies.</p>
<h3>3.3 Frequency domain feature extraction</h3>
<p class="mb15">As crucial parameters in EEG emotion recognition algorithms, frequency domain features offer more intricate emotion-related information, including the distribution of energy across different frequency bands. For instance, the energy distribution in high-frequency bands (such as &#x03B2;-band and &#x03B3;-band waves) tends to increase during pleasurable and exciting emotional states (<a href="#ref50">Li et al., 2018</a>). Analyzing the phase synchronization degree of signals can provide insights into changes in information &#x03B8;-band wave patterns between brain regions during different emotional states. For example, theta synchronization between the frontal and temporal lobes is associated with pleasant emotions (<a href="#ref8">Ara and Marco Pallar&#x000E9;s, 2020</a>). Frequency domain features allow for the analysis of interactions between various brain regions. By calculating correlation features between different brain regions at different frequencies, changes in information exchange patterns between brain regions during different emotional states can be observed (<a href="#ref58">Maffei, 2020</a>). Based on the inter-correlation maps of &#x03B4;, &#x03B1; and &#x03B3;-band waves stimulated by six different scenarios, the widest topographical distribution is &#x03B4;-band, while the narrowest is &#x03B1;-band (<a href="#ref58">Maffei, 2020</a>).</p>
<p class="mb0">Various techniques are commonly employed for extracting frequency domain features as in <a href="#tab3">Table 3</a>. These include the following methods: Fourier transform, wavelet transform, independent component analysis, and matrix decomposition (<a href="#ref96">Torres et al., 2020</a>; <a href="#ref117">Zhou et al., 2022</a>; <a href="#ref51">Li et al., 2023</a>; <a href="#ref59">Mahmoud et al., 2023</a>). Fourier transform is utilized to convert a time domain signal into a frequency domain signal, providing spectral information such as frequency components and amplitude details (<a href="#ref59">Mahmoud et al., 2023</a>). Frequency domain feature extraction techniques based on the Fourier transform encompass power spectral density (PSD), average power spectral density (APSD), and related features. PSD is usually evaluated within a specific frequency band, considered the most commonly applied feature for classical emotion classifiers (<a href="#ref105">Xu et al., 2024</a>).</p>
<div class="DottedLine"></div> <div class="Imageheaders">Table 3</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg" name="table3" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg" alt="www.frontiersin.org" id="tab3" loading="lazy">
  </picture>
</a>
<p><b>Table 3</b>. Advantages and disadvantages of frequency domain feature extraction methods.
</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15">Wavelet transform offers a more versatile and multi-scale approach to signal analysis, delivering both frequency and time information (<a href="#ref9">Bagherzadeh et al., 2023</a>). Frequency domain feature extraction methods associated with wavelet transform involve wavelet packet decomposition (WPD), wavelet packet energy features, and similar characteristics. Independent component analysis serves as a signal decomposition method grounded in independence assumptions, yielding independent frequency domain components post-decomposition (<a href="#ref85">Shu et al., 2018</a>). Frequency domain feature extraction techniques stemming from independent component analysis include frequency band energy distribution, phase synchronization degree, and more. Matrix decomposition is an algebraic signal decomposition method that disentangles the original signal into distinct frequency domain components (<a href="#ref39">Hossain et al., 2023</a>). These techniques enable the extraction of diverse frequency domain features such as spectral characteristics, phase synchronization degrees, correlation features, and so forth. In emotion classification applications, a tailored selection and adjustment of methods and feature combinations can be made based on specific requirements.</p>
<p class="mb0">The capabilities of artificial intelligence algorithms in mining large-scale data sets not only enable the automatic extraction of frequency characteristics from EEG signals but also reveal the underlying connections between frequency domain signals and emotions.</p>
<h3>3.4 Time-frequency domain feature extraction</h3>
<p class="mb0">Time-frequency feature extraction methods involve analyzing EEG signal changes in both time and frequency to extract characteristic parameters that capture the dynamic nature of the signal (<a href="#ref9">Bagherzadeh et al., 2023</a>). Common techniques of time-frequency domain features include wavelet transform (<a href="#ref43">Khare and Bajaj, 2021</a>) and short-time Fourier transform (STFT) (<a href="#ref71">Pei et al., 2024</a>). These methods enable the extraction of information across various time scales and frequency ranges, unveiling how signals evolve and frequency as in <a href="#fig4">Figure 4</a>, which also has been applied by our group (<a href="#ref48">Li et al., 2022a</a>).</p>
<div class="DottedLine"></div> <div class="Imageheaders">Figure 4</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg" name="figure4" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg" alt="www.frontiersin.org" id="fig4" loading="lazy">
  </picture>
</a>
<p><b>Figure 4</b>. Time-frequency plots before and after stimulation were used by the authors&#x2019; research group.</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15">By extracting time-frequency features, a more comprehensive description of the signal&#x2019;s dynamic characteristics can be achieved, laying the groundwork for subsequent signal processing and emotion classification analysis.</p>
<p class="mb0">Time-frequency plots typically encompass a vast array of data points, representing a high-dimensional dataset. The application of artificial intelligence algorithms can automatically discern time-frequency patterns associated with various emotions. This capacity for autonomous learning and data mining enhances the efficacy and reliability of time-frequency plots in the identification of emotions induced by music.</p>
<h3>3.5 Other advanced features</h3>
<p class="mb15">The development of new emotion-recognition features has been significantly influenced by researchers&#x2019; profound insights into the brain&#x2019;s response to emotions.</p>
<p class="mb0">Prior physiological and psychological studies have demonstrated that emotions, being intricate mental states, can be discerned by detecting the status of connections between brain regions. In recent years, scholars have advocated for the establishment of a network of brain regions using phase-locked values and the extraction of features from multiple brain functional connectivity networks through the application of the Hilbert transform. These graph features are then fused to facilitate emotion recognition (<a href="#ref47">Li et al., 2019</a>). Based on this concept, researchers have introduced a novel feature called asPLV (averaged sub-frequency phase locking value), which is derived from the Morlet transform method. This feature effectively mitigates the impact of the brain&#x2019;s inherent frequency oscillations induced by the cognitive processing of emotional fluctuations, thereby enhancing the accuracy of recognizing mood changes induced by music. The calculation process for asPLV is outlined as in <a href="#tab4">Table 4</a>.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Table 4</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg" name="table4" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg" alt="www.frontiersin.org" id="tab4" loading="lazy">
  </picture>
</a>
<p><b>Table 4</b>. Typical extraction process flow of asPLV.
</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15">In recent years, scholars have discovered that the spatiotemporal characteristics of EEG play a crucial role in emotion recognition. Many studies have introduced novel spatiotemporal features based on self-attention mechanisms (<a href="#ref116">Zhou and Lian, 2023</a>). As our comprehension of the neural mechanisms underlying emotional responses deepens, these new features are critical for enhancing the accuracy of emotion recognition.</p>
<p class="mb0">Other than these commonly applied features already discussed, artificial intelligence algorithms excel in processing multidimensional data, enabling the discovery of innovative feature metrics. These algorithms hold great promise in identifying individual-specific traits, and crafting features that are sensitive to the distinctive attributes of each individual.</p> <a id="h5" name="h5"></a>
<h2>4 Emotion data source and modeling</h2>
<p class="mb0">Auto-emotion recognition can be realized by integrating various data sources and emotion models. This is important for the development of music-induced emotion recognition and its application areas. In the realm of music-induced emotion recognition, emotional data sources form the foundation for acquiring emotion related in sights, while models serve as the essential tools for processing and analyzing this valuable information (<a href="#ref79">Saganowski et al., 2023</a>; <a href="#ref104">Xu J. et al., 2023</a>).</p>
<h3>4.1 Data sources for music-evoked emotion classification</h3>
<p class="mb0">In recent years, in order to promote research on music-induced emotions, a series of databases of music-triggered emotions have been established, with emotion labels provided by psychologists. Although these databases can be used for music-triggered emotion research, they lack a unified criterion. Based on the EEG method of emotion discrimination, researchers also have established emotion databases containing EEG signals. These open source databases are not only important resources for conducting research on music-triggered emotions, but can also be used to evaluate the performance of different EEG algorithms. <a href="#tab5">Table 5</a> shows some common open source music emotion databases and their characteristics.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Table 5</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg" name="table5" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg" alt="www.frontiersin.org" id="tab5" loading="lazy">
  </picture>
</a>
<p><b>Table 5</b>. Open-source music emotion databases for music-induced emotion classification.
</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15"><b>The AMG1608 database</b> is a database containing acoustic features extracted from 1,608 music clips of 30s as well as emotion annotations provided by 665 subjects, consisting of 345 females and 320 males. The database used a dimensional emotion model with validity and arousal (VA) as the coordinates in the emotion annotation, and the subjects annotated the emotional state of each music clip. The dataset contains two subsets of emotion annotations from National Taiwan University and Amazon Turkish Robotics and is characterized by a large amount of data capable of being publicly accessible and can be used for music emotion recognition research.</p>
<p class="mb15"><b>The CAL500 database</b> is composed of 500 Western songs&#x2019; clips written by 500 different artists. For the emotion annotation of the music, 174 music-related semantic keywords were used, and at least three subjects annotated keywords for each song. These annotated words were also post-processed algorithmically to constitute a vector of annotated words and weights, ensuring the reliability of the annotation labels. The dataset is able to satisfy the fine granularity and differentiation required in music emotion recognition research.</p>
<p class="mb15"><b>The DEAM database</b>, which labels musical emotions in terms of valence and arousal (VA) coordinates, has 1,802 songs licensed under the Creative Commons (CC) license. This music library contains categories such as rock, pop, soul, blues, electronic, classical, hip-hop, international, experimental, ethnic, jazz, country, and pop. The emotion annotations for these songs were made by 21 active teams from all over the world, and these annotations were statistically processed to form a database that can be used for music emotion research.</p>
<p class="mb15"><b>The emoMusic database</b> contains 1,000 audio tracks in MP3 format licensed under the Creative Commons (CC) License in eight different genres: blues, electronica, rock, classical, folk, jazz, country, and pop, with 125 tracks in each genre. The emotion labeling of the music was evaluated using valence and arousal (VA) model, where valence indicates positive and negative emotions and arousal indicates emotional intensity. The database collects time-varying (per second) continuous VA rating data, with each song containing at least 10 thematic annotations. The database can be utilized for the conduct of research related to music emotion annotation and other related studies.</p>
<p class="mb15"><b>The Emotify database</b> contains 100 pieces of music from each of the four genres of classical, rock, pop, and electronic music randomly selected from a collection of music containing 241 different albums by 140 performers. The database used the Geneva Emotional Music Scale (GEMS), in which subjects labeled the emotions of the music using a Likert scale using a scale of 1&#x2013;5. The database provides case studies and information on the effects of other factors on evoked emotions (gender, mood, music preference).</p>
<p class="mb15"><b>The DEAP database</b> is a music emotion recognition database based on an EEG emotion recognition method, which was built together by a consortium of four universities from the UK, the Netherlands, and Sweden, and records EEG and physiological information from 32 subjects who watched a series of forty 1-min music video clips. The database was selected as a semi-automatic stimulus selection method based on emotional labeling is open access to academics and can facilitate research related to emotional stimulation in modern music.</p>
<p class="mb15"><b>The IADS database</b> is the International Emotionally Digitized Sound Database, which is divided into two distinct phases. The initial Phase I database, established in 1999, contains a modest collection of data that has seen limited use in contemporary times. In contrast, Phase II is an expansive compilation of 167 digitally captured ambient sounds that are frequently encountered in everyday life, such as the joyful laughter of an infant, the rhythmic sounds of cooking, and the dramatic rumble of thunderstorms, with each sound clip precisely lasting 6&#x2009;s. The collection is meticulously annotated, with each piece of digital audio being evaluated by participants through a self-assessment approach that utilizes the affective dimensions of the Valence-Arousal (VA) model.</p>
<p class="mb0">At present, these databases of music emotions are mainly based on foreign music libraries, the situation is related to the importance of music in the relevant regions, and the establishment of music databases based on Chinese musical compositions is yet to be carried out. Due to the complexity of the signal measurement and classification of EEG in the early stage, there are fewer studies for EEG music-induced emotion recognition. Enabled by artificial intelligence, EEG-based music emotion recognition can help to expand the establishment of databases as soon as possible, and it can help more researchers to apply the established databases. Currently, besides the above common music-induced emotion databases, many researchers also use their own designed libraries to carry out personalized research in the study of music-induced emotions with EEG. As artificial intelligence technology advances, it facilitates the integration of EEG data with other modalities of data, thereby enriching the dimensions of information within the database. The application of data augmentation techniques helps to enhance the generalization capability of models built from the database. With the progression of research in EEG-induced emotion recognition, artificial intelligence can also assist in the automatic consolidation and updating of databases, providing technical support for the construction of more comprehensive, accurate, and holistic datasets.</p>
<h3>4.2 Emotion classification models</h3>
<p class="mb0">To address the challenge of quantifying the emotions elicited by music, researchers have developed a variety of models specifically designed to capture the nuances of music-induced emotions. These models aim to provide a structured approach to understanding and measuring the complex emotional responses that music can evoke. The classical music emotion models for analysis are shown in <a href="#fig5">Figure 5</a>, including the Hevner model (<a href="#ref38">Hevner, 1936</a>), the Pleasure Arousal Dominance (PAD) model (<a href="#ref76">Russell, 1980</a>), and the Thayer model (<a href="#ref95">Thayer and McNally, 1992</a>). With the development of artificial intelligence technology, some algorithm-based emotion classification approaches also have been proposed.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Figure 5</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg" name="figure5" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg" alt="www.frontiersin.org" id="fig5" loading="lazy">
  </picture>
</a>
<p><b>Figure 5</b>. Schematic diagrams of common emotion recognition models. <b>(A)</b> Hevner&#x2019;s model, <b>(B)</b> PAD model, <b>(C)</b> Thayer&#x2019;s model.</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15">In computerized categorization studies of musical emotions, the first psychological Hevner emotion classification model was proposed in 1936 (<a href="#ref38">Hevner, 1936</a>). This model classifies music emotion states into eight categories: Solemn, Sad, Dreamy, Quiet, Graceful, Happy, Excited, and Powerful as in <a href="#fig5">Figure 5A</a>. Each category can be further subdivided into more detailed and extensive emotion words, with a total of 67. This emotion classification model was set up considering musicology and psychology and is more abundant in the selection of emotion keywords, which is helpful for the research of emotion recognition in musical works. Hevner is a discrete emotion classification model that is often used as an emotion label for songs in music-induced emotion recognition research. However, due to the large number of labeling categories of the model and the relatively low variability of physiological properties of some categories, this model is seldom applied in EEG-based music emotion recognition studies, but it can be considered in relevant studies for featured music.</p>
<p class="mb15">Among the classification models of musical emotions, PAD is a three-dimensional measurement model that was proposed in 1980. As in <a href="#fig5">Figure 5B</a>, this model establishes three main dimensions of Pleasure-Displeasure, Arousal-Nonarousal, and Dominance-Submissiveness, which indicate the direction of the emotion, the degree of neurophysiological activation, and the degree of individual feeling, respectively. This categorization provides a continuous quantitative evaluation method, which has been widely used in psychological and emotional brand evaluation (<a href="#ref108">Yang et al., 2020</a>) but has not been used much in the actual evaluation of musical emotions. Thayer&#x2019;s model is a two-dimensional model that suggests different emotions are classified based on two-dimensional underlying dimensions, i.e., Energy awakening and Tension awakening. Using stress as the horizontal coordinate and energy as the vertical coordinate, emotions are categorized into four zones: vitality, anxiety, contentment, and depression as in <a href="#fig5">Figure 5C</a>. This model is proposed based on a psychological perspective and describes the music mood according to quantitative thinking, which is often used to classify the mood of audiophile music in MP3 and WAV formats (<a href="#ref13">Brotzer et al., 2019</a>). Since this model has fewer classification dimensions compared with other mentioned models and is more visible on the emotional response, they are well suitable to be used for EEG recognition of music-induced emotions. Besides the above classical emotion classification models, some researchers have also used probability distribution (<a href="#ref44">Kim et al., 2022</a>), neural network method (<a href="#ref106">Yang, 2021</a>), linear regression (<a href="#ref35">Griffiths et al., 2021</a>), and inverse word pairs (<a href="#ref54">Liu et al., 2019</a>) approaches to characterize the emotions of music. The probability distribution method describes the emotions corresponding to the song in the emotion description space, which gives a more comprehensive and intuitive description of the song. The ranking is utilized to order the emotion descriptions of songs according to the degree of relevance of the emotions expressed, which is convenient to provide a quick description method. The antonym pairs can give a relatively objective description of the mood of the music. Several researchers have currently extended discrete and multidimensional models for music emotion description based on these ideas.</p>
<p class="mb0">These new classification models are related to the development of emotion categorization algorithms and have a large potential for application in the field of EEG music-induced emotions. Different emotion models can be used to describe the classification of emotions in different states, meanwhile, there are some intersections between these different models. For different practical applications, people need to choose the appropriate emotion classification models according to the research scenarios and artificial intelligence algorithms.</p>
<h3>4.3 Emotional intensity model</h3>
<p class="mb0">Emotional intensity models were applied to quantitatively delineate the depth of emotions experienced by individuals in specific circumstances, forming the cornerstone for emotion recognition and prediction models. The discourse on quantifying emotional intensity emerged around 1990, advocating for a shift from solely focusing on subjective emotional aspects to incorporating physiological and observable behaviors as metrics of intensity (<a href="#ref76">Russell, 1980</a>; <a href="#ref88">Sonnemans and Frijda, 1994</a>). In 1994, Frijida introduced a five-dimensional model for scrutinizing subjective emotions, encompassing dimensions such as the frequency and intensity of re-collected and re-experienced emotions, latency and duration of emotions, intensity of actions and propensities, as well as actual behaviors, beliefs, and behavioral changes (<a href="#ref88">Sonnemans and Frijda, 1994</a>). In recent years, researchers have explored the use of emotional intensity modeling to study the instantaneous dynamic processes in the brain under external stimuli, as in <a href="#fig6">Figure 6A</a> (<a href="#ref31">Gan et al., 2024</a>). This innovative approach provides a new approach to the study of the neural mechanisms and processes of music-induced emotions.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Figure 6</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg" name="figure6" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg" alt="www.frontiersin.org" id="fig6" loading="lazy">
  </picture>
</a>
<p><b>Figure 6</b>. <b>(A)</b> The instantaneous emotional intensity of stimulated emotion dynamic process (<a href="#ref31">Gan et al., 2024</a>), <b>(B)</b> schematic of a fine-grained emotional division.</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15">Despite offering a theoretical framework for objectively describing emotional states, the model&#x2019;s impact was limited due to the scarcity of physiological emotion measures at that time.</p>
<p class="mb0">A prevalent theoretical framework in recent years for elucidating emotional intensity is <a href="#ref76">Russell, 1980</a> proposition that emotional experiences can be depicted in a two-dimensional space defined by emotional valence (positive vs. negative emotions) and arousal levels (high vs. low) (<a href="#ref76">Russell, 1980</a>). Building upon this framework, researchers have delved into refining each dimension to achieve a nuanced portrayal of emotions illustrated in <a href="#fig6">Figure 6B</a>, laying the groundwork for leveraging artificial intelligence in digitally characterizing emotions (<a href="#ref74">Reisenzein, 1994</a>; <a href="#ref113">Zhang et al., 2023</a>). Physiological emotional intensity indices such as EEG, ECG, and MRI are not only valuable for emotion recognition but also serve as essential tools for studying the dynamic processes and physiological mechanisms underlying music-induced emotional changes (<a href="#ref98">Ueno and Shimada, 2023</a>; <a href="#ref102">Wu et al., 2024</a>).</p> <a id="h6" name="h6"></a>
<h2>5 Artificial intelligence algorithms for EEG emotion recognition</h2>
<p class="mb0">Music-induced EEG-based emotion classification research can be considered an artificial intelligence classification task, where the selection of appropriate classification algorithms is a crucial element in the current research on EEG-based emotion classification. These algorithms are not only the topicality of emotion classification research in EEG but also serve as an important foundation for further research into music-induced emotions (<a href="#ref42">Khabiri et al., 2023</a>).</p>
<h3>5.1 Classical machine learning algorithms</h3>
<p class="mb0">Based on the EEG feature signals of music-induced emotions, various classical machine learning classification methods have been commonly used to achieve relatively good classification accuracy. These methods include classical classifiers such as Bayes classifier (BC) (<a href="#ref45">Koelstra et al., 2012</a>), linear regression (LR) (<a href="#ref114">Zheng and Lu, 2015</a>), support vector machines (SVM) (<a href="#ref9">Bagherzadeh et al., 2023</a>), K Nearest Neighbor (KNN) (<a href="#ref42">Khabiri et al., 2023</a>), and random forests (RF) (<a href="#ref71">Pei et al., 2024</a>), as in <a href="#tab6">Table 6</a>.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Table 6</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg" name="table6" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg" alt="www.frontiersin.org" id="tab6" loading="lazy">
  </picture>
</a>
<p><b>Table 6</b>. Several classical machine learning methods applied in music-induced emotion classification.
</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb15">These algorithms have been extensively employed in the field of emotion classification research and have shown promising results in accurately classifying mu-sic-induced emotions (<a href="#ref22">Dadebayev et al., 2022</a>). One of the commonly used supervised classification algorithms for music sentiment is the K Nearest Neighbor (KNN) algorithm. KNN, as a supervised learning algorithm, is highly versatile and easy to understand. It is robust to outliers and has a simple principle. However, the computational results of the KNN algorithm can be influenced by the training set samples as well as the value of K, which represents the number of nearest neighbors considered for classification. It is important to carefully select the appropriate value of K and ensure the representativeness and quality of the training set to achieve accurate classification results in music sentiment analysis. Another commonly used classical classifier for music sentiment analysis is the Support Vector Machine (SVM). When using SVM for classification, the choice of the kernel function has a significant impact on its performance. By mapping the features nonlinearly to a high-dimensional space using the kernel function, SVM improves the robustness of the music emotion recognition algorithm (<a href="#ref15">Cai et al., 2022</a>). SVM is particularly effective in handling high-dimensional data, making it suitable for achieving better classification results in music EEG emotion recognition compared to KNN.</p>
<p class="mb0">Classical machine learning algorithms exhibit strong interpretability, high data efficiency, and low computational resource requirements in music emotion recognition research. These characteristics are highly desirable for studying the neural mechanisms of music-induced mood changes. However, in practical applications of music emotions, these models often suffer from poor generalization performance and require improved accuracy.</p>
<h3>5.2 Deep learning algorithms</h3>
<p class="mb15">Although machine learning algorithms have been used for emotion recognition and have shown improvements, there are still challenges such as feature extraction, stability, and accuracy. However, the emergence of deep learning methods in recent years has provided a promising approach for EEG-based music emotion recognition research. Deep learning algorithms, characterized by their strong learning ability, have shown great potential in this field.</p>
<p class="mb0">One notable deep learning algorithm applied in EEG-based music emotion recognition research is Convolutional Neural Networks (CNN). CNN extends the network structure of artificial neural networks by incorporating convolutional layers and pooling layers between the input layer and the fully connected layer. This architecture allows CNN to automatically learn and extract relevant features from the input data, making it suitable for analyzing complex patterns in EEG signals. By leveraging deep learning techniques, researchers can enhance the performance of music emotion recognition systems. Deep learning algorithms can effectively handle the high-dimensional and time-varying nature of EEG signals, leading to improved accuracy and stability in emotion recognition tasks. Moreover, with the ability to capture hierarchical representations, CNN can capture both local and global features in EEG data, enabling a more comprehensive analysis of music-induced emotions (<a href="#ref64">Moctezuma et al., 2022</a>; <a href="#ref59">Mahmoud et al., 2023</a>). With the development of deep learning algorithms, a variety of deep learning models have been developed and applied to EEG-based music-induced emotion recognition, including recurrent neural networks (RNN) (<a href="#ref23">Dar et al., 2022</a>), long and short-term memory networks (LSTM) (<a href="#ref25">Du et al., 2022</a>), gated recurrent neural networks (GRNN) (<a href="#ref18">Weerakody et al., 2021</a>) and autoencoder (AE) (<a href="#ref55">Liu et al., 2020</a>). The properties and applications of these reported deep learning algorithms are summarized in the following <a href="#tab7">Table 7</a>.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Table 7</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg" name="table7" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg" alt="www.frontiersin.org" id="tab7" loading="lazy">
  </picture>
</a>
<p><b>Table 7</b>. Properties of typical deep learning methods applied in music-induced emotion classification.
</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<p class="mb0">The deep learning algorithms employed in EEG recognition of music-induced emotions demonstrate excellent generalization capabilities and data insensitivity, essential for the practical application of such emotions. While deep learning algorithms typically lack interpretability, recent advancements like GRNN (<a href="#ref18">Weerakody et al., 2021</a>) and RNN (<a href="#ref23">Dar et al., 2022</a>) can effectively capture the temporal aspects of EEG data, offering a novel approach to investigating the transient characteristics of music-induced emotions.</p>
<h3>5.3 Model optimization and fusion strategies</h3>
<p class="mb15">Previous studies have demonstrated that classical machine learning algorithms as well as deep learning algorithms each possess their unique strengths and weaknesses in EEG-based music-induced emotion recognition research. To address the research and application requirements in related domains, researchers have investigated fusion strategies involving diverse algorithms.</p>
<p class="mb15">To enhance the precision of emotion recognition, researchers have delved into a hybrid deep learning framework combining gated recurrent unit (GRU) and CNN to leverage the strengths of both methodologies. The conventional GRU model is excellent in handling time series data, while the CNN model is adept at capturing spatial features within the data. During the implementation phase, researchers opted to retain all feature information outputted by the GRU and extract spatial information from the temporal features using the CNN model. Ultimately, they achieved a recognition average accuracy of 87.04% (<a href="#ref103">Xu G. et al., 2023</a>). Based on the brain&#x2019;s functional network of emotional activity, researchers proposed a multi-feature fusion method combining energy activation, spatial distribution, and brain functional connectivity network features. In the study, the SVM model-based fusion of power activation features of differential entropy (DE), spatial features of common spatial patterns (CSP), five frequency features, and phase synchronization features of EEG phase-locked values (PLV) achieved classification results with an average accuracy around 85% (<a href="#ref68">Pan et al., 2022</a>).</p>
<p class="mb0">To realize the fusion between different machine learning algorithms, it can be achieved by combining multiple basic classifiers for better performance, fusing different algorithmic training models for model fusion of multiple ones, and also by joint training of multiple neural network models for fusion of different algorithms, etc. In addition to these fusion approaches mentioned above, some researchers have also considered about the optimization method from a music-induced emotions perspective. A pentameter-based EEG music model was proposed. The model constructs a multi-channel EEG sensor network and records the EEG of individuals in various emotional states to establish a mapping library of EEG and emotions. Subsequently, the music pentameter model is employed to adaptively segment the time-domain EEG signal, transforming the EEG signal. The time-frequency features of the EEG signal, such as amplitude, contour, and signal frequency, are quantitatively represented in a normalized musical space (<a href="#ref52">Li and Zheng, 2023</a>). The arithmetic modeling process is described as in <a href="#fig7">Figure 7</a>.</p>
<div class="DottedLine"></div> <div class="Imageheaders">Figure 7</div> <div class="FigureDesc"><a href="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg" name="figure7" target="_blank">
  <picture>
    <source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg" media="(max-width: 563px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg" media="(max-width: 1024px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg" media="(max-width: 1441px)"><source type="image/webp" srcset="https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg" media=""><source type="image/jpg" srcset="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg" media=""> <img src="https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg" alt="www.frontiersin.org" id="fig7" loading="lazy">
  </picture>
</a>
<p><b>Figure 7</b>. EEG musical staff model process flow chart.</p></div> <div class="clear"></div> <div class="DottedLine"></div>
<h3>5.4 Algorithm comparison and evaluation</h3>
<p class="mb0">Evaluating these various algorithms used for music-induced emotion EEG is very difficult. There is no clear consensus on an optimal algorithm, and several metrics can be employed to evaluate the selected algorithmic model to satisfy the requirements for various applications. The accuracy of emotion recognition is a fundamental evaluation metric, reflecting the model&#x2019;s performance in correctly predicting samples compared to the total number of samples and overall classifying emotions. Precision and recall, as evaluation metrics for binary classification problems, aid in assessing the model&#x2019;s performance across different categories. With the expansion of applications and the development of algorithms, developing new criteria is also an important part of future research.</p> <a id="h7" name="h7"></a>
<h2>6 Application examples and analysis</h2>
<p class="mb0">EEG-based music emotion recognition has emerged as a multidisciplinary research area at the intersection of medicine, psychology, and computer science. This field explores the use of EEG signals to detect and classify the emotional responses evoked by music. The insights gained from EEG-based music emotion recognition have profound implications across various domains.</p>
<h3>6.1 Music therapy</h3>
<p class="mb0">Music therapy is a therapeutic approach that harnesses the power of music and musical activities to promote both physical and mental well-being. The use of music for healing purposes has a long history, dating back to ancient civilizations like Greece, Rome, Egypt, and China. In more recent times, the formal recognition of music as a legitimate form of therapy began with the establishment of the American Music Therapy Association in 1944 (<a href="#ref92">Taylor, 1981</a>). This marked a significant milestone in acknowledging music therapy as a valid and effective treatment modality within modern society. To broaden the scope of music therapy, the American Music Therapy Association took a significant step in 2005 by introducing the Research Strategic Priority (RSP) program. The primary objective of this initiative is to delve into the physiological evidence supporting the effectiveness of music therapy in both practical and theoretical contexts. In 2013, a team of researchers from Finland conducted a study to investigate the impact of music on the activity of frontotemporal areas during resting state in individuals with depression. The study utilized EEG-based recognition of music-induced emotions as a methodological approach (<a href="#ref29">Fachner et al., 2013</a>). In 2018, a team of Spanish researchers conducted a study to evaluate the emotional response to music in patients with advanced cancer using EEG signals. The study aimed to demonstrate the positive emotional impact of music therapy on these patients (<a href="#ref73">Ramirez et al., 2018</a>). In 2020, a team of Canadian researchers conducted a study to explore the potential of music-induced emotions in alleviating psycho-cognitive symptoms of Alzheimer&#x2019;s disease. The study involved combining EEG analysis methods to investigate how music activates the brain system, reduces negative emotions, and increases positive emotions. By analyzing EEG signals, the researchers were able to assess the emotional states induced by music. They found that music had a significant impact on the participants&#x2019; emotional well-being, with the potential to reduce negative emotions and increase positive emotions (<a href="#ref14">Byrns et al., 2020</a>).</p>
<h3>6.2 Neuroscience</h3>
<p class="mb15">Brain science is a rapidly growing field of research that offers valuable insights into human thinking, behavior, and consciousness. One area of study within brain science is the investigation of how music stimulates the brain, which has been recognized as a notable example of this research. In 1992, French and German scientists conducted a groundbreaking EEG analysis study to examine the effects of music stimulation on the brain. The study revealed a fascinating phenomenon: different types of music had varying impacts on the intensity of EEG signals across different frequency bands (<a href="#ref90">Steinberg et al., 1992</a>). In 2016, a team of Indian researchers conducted a study using EEG to investigate the effects of Hindustani music on brain activity in a relaxed state. The results of the study revealed that Hindustani music had a significant effect on the listeners&#x2019; arousal levels in all activities stimulated by the music. The EEG analysis indicated an increase in brain activity in response to the music, suggesting that it had a stimulating effect on the listeners (<a href="#ref11">Banerjee et al., 2016</a>). In 2019, a group of Indian scholars delved into research on the reverse inversion of brain sounds by utilizing Hindustani classical music. They recognized the profound emotional impact of this music and sought to explore the correlation between EEG signals and musical stimuli. By leveraging the real-time recording capability of EEG, researchers from the fields of psychology and neurology conducted studies to analyze the neural mechanisms underlying the stimulation of music, both in positive and negative contexts. These investigations have significantly contributed to the advancement of brain science research (<a href="#ref82">Sanyal et al., 2019</a>).</p>
<p class="mb0">In the early stages, EEG, as a direct signal of brain activity, was employed by neuroscientists to conduct exploratory studies on functional brain regions associated with impaired musical ability caused by brain dysfunction. This utilization of EEG monitoring technology has played a pivotal role in advancing our understanding of the brain&#x2019;s mechanisms involved in music processing (<a href="#ref99">Vuust et al., 2022</a>). These initial findings laid the technical groundwork for subsequent research on EEG-based music emotion recognition. With a focus on music-induced emotions, researchers have endeavored to further investigate the realm of music-induced emotions using EEG technology (<a href="#ref34">Gomez-Canon et al., 2021</a>). From the perspective of music therapy, the utilization of EEG signals offers direct evidence regarding the process of music-induced emotions. By analyzing EEG signals from various brain regions corresponding to different emotions, researchers can obtain more detailed physiological information that aids in the interpretation of the brain mechanisms involved in music therapy. This application of EEG signals provides valuable insights into understanding the effects of music on emotional states and enhances our knowledge of the therapeutic potential of music (<a href="#ref14">Byrns et al., 2020</a>; <a href="#ref30">Fedotchev et al., 2022</a>).</p>
<h3>6.3 Others</h3>
<p class="mb15">Emotions play a crucial role in human experiences, behaviors, health, and social interactions. Music, a language of the human mind, has the power to vividly and imaginatively express various emotions such as happiness, sadness, and more, and can greatly influence listeners&#x2019; emotional state. In recent years, there has been significant progress in understanding music-induced emotions and their psychological and neurological mechanisms.</p>
<p class="mb15">In clinical medicine, this research can contribute to the development of personalized music therapy interventions for mental health disorders, neurorehabilitation, and stress management. It can also aid in diagnosing and monitoring emotional disorders such as depression and anxiety. In the realm of brain science, EEG-based music emotion recognition provides valuable insights into the neural mechanisms underlying emotional processing and music perception. These findings can enhance our understanding of how the brain responds to music and its impact on emotional well-being. Moreover, in the field of music information, this research can improve music recommendation systems, enhance user experiences, and facilitate music healing approaches. By tailoring music selections based on an individual&#x2019;s emotional responses, music platforms can offer personalized and therapeutic listening experiences. Overall, EEG-based music emotion recognition holds immense potential for diverse applications in fields like clinical medicine, brain science, and music information. It represents a promising avenue for advancing our understanding of the complex relationship between music and emotions and harnessing music&#x2019;s therapeutic benefits.</p>
<p class="mb0">Furthermore, for some long-term music healing processes, the real-time sensitivity of EEG to emotional signals induced by music stimulation can provide evidence for the effectiveness of certain therapeutic methods. This evidence can facilitate the development, correction, and smooth dissemination of related therapeutic techniques. By monitoring changes in EEG signals throughout the music therapy process, researchers can evaluate the effectiveness of different therapeutic methods and fine-tune them accordingly. This approach enhances the precision and efficacy of music therapy, allowing for optimized treatment plans that cater to individual needs (<a href="#ref14">Byrns et al., 2020</a>). For music researchers, the individual variability in EEG emotion detection allows for personalized categorization and annotation of musical emotions. This capability is crucial not only for music composition and information retrieval but also for guiding the development of more immersive multimedia environments. By leveraging EEG data to understand how individuals uniquely experience and respond to musical emotions, researchers can enhance the creation of tailored musical experiences and enrich the design of multimedia environments that resonate with diverse emotional responses (<a href="#ref110">Yu et al., 2022</a>).</p> <a id="h8" name="h8"></a>
<h2>7 Discussion and conclusions</h2>
<p class="mb15">Based on the mentioned model, researchers were able to carry out systematic research on the study of the emotional impacts of the same music on different listeners, the study of the emotional impact of various types of music on the same listener, and the key parameters of music-stimulated emotions. Previous researchers have conducted various studies in terms of music-induced emotion classification models, music-induced datasets, training and classification of emotion models, and so on.</p>
<p class="mb15">As an interdisciplinary challenge, research on EEG-based music-induced emotion recognition has emerged as a valuable approach for real-time and effective assessment of emotional responses to music. This innovative research not only offers new technical tools for studying music-related emotions but also provides a controllable research paradigm applicable to brain science and other fields. In recent years, researchers from various disciplines have made significant progress in addressing this complex issue. By approaching the problem from different perspectives, they have achieved notable results. However, during these investigations, several limitations have also been identified.</p>
<p class="mb15">Compared to other signals commonly used for music emotion recognition, such as audio signals, facial expressions, heart rate, and respiration, EEG signals have distinct advantages. EEG signals belong to physiological signals of the central nervous system, which are typically not under conscious control. Consequently, they can provide information about the current emotional state of an individual that cannot be deliberately concealed. Furthermore, EEG signals offer several benefits when compared to other methods of detecting physiological signals of the central nervous system. EEG is a relatively mature technology that has been extensively studied and validated. It is also portable, non-invasive, and cost-effective, making it practical for use in various research and real-world settings.</p>
<p class="mb15">As EEG monitoring hardware and recognition algorithm software technology continue to evolve, the advantages of personalization, real-time analysis, and the convenience of using EEG to recognize music-induced emotions will be further explored in various application fields. The growing sophistication of EEG technology opens up new possibilities for research and practical implementation of music-based therapies, multimedia environments, and personalized music experiences. As such, the continued development and refinement of EEG-based music emotion recognition has the potential to revolutionize our understanding of the impact of music on human emotions and behavior. Advancements in EEG monitoring hardware and recognition algorithm software technology have opened up new avenues for exploring the potential applications of EEG-based music-induced emotion recognition. With these technological improvements, the advantages of personalization, real-time, and convenience in recognizing music-induced emotions through EEG can be further explored in various fields.</p>
<p class="mb15">At present, the labeling basis of the training set in EEG emotion recognition algorithms largely relies on psychological scales and emotion labels from databases. However, these conventional labeling methods are inherently subjective and discrete. Therefore, there is a pressing need for extensive research to establish a standardized library of emotions based on EEG signals themselves. To address this challenge, musicologists from diverse cultural backgrounds have embarked on initial research into the emotional labeling of music within their respective cultures. As the accuracy of EEG signals for emotion recognition continues to improve, there has been increasing mention of establishing direct EEG signal discrimination for personalized emotion recognition. This advancement holds promise for enhancing our understanding of how individuals from different cultural backgrounds experience and interpret emotions in music, paving the way for more nuanced and culturally sensitive approaches to music emotion recognition (<a href="#ref26">Du et al., 2023</a>).</p>
<p class="mb15">Improving the accuracy of music-induced emotion recognition can be a challenging problem that demands long-term research, and the advent of deep learning algorithms in recent years has provided a more effective means of addressing this challenge compared to traditional machine learning approaches. With deep learning algorithms, researchers can leverage large amounts of data to train neural networks that can learn to recognize complex patterns and relationships in music-induced emotions. This approach has shown great promise in improving the accuracy of music emotion recognition, allowing researchers to gain deeper insights into how music affects human emotions and behavior. However, ongoing research and development are still required to further refine and optimize these algorithms for use in practical applications (<a href="#ref70">Pandey and Seeja, 2022</a>). As artificial intelligence algorithms continue to undergo continuous optimization and enhancement, new concepts, approaches, and research findings will undoubtedly emerge, offering fresh perspectives and advancing the field of music-induced emotion recognition.</p>
<p class="mb0">The research and development of music-induced emotion recognition based on EEG relies on the continuous expansion of the application field for such technology. Currently, there are some notable examples of music-induced emotion applications in clinical treatment (<a href="#ref14">Byrns et al., 2020</a>), neuroscience (<a href="#ref57">Luo et al., 2023</a>), and music information retrieval (<a href="#ref51">Li et al., 2023</a>). However, there is still a need for further development of related technical products that can be scaled up and made accessible to the general public, allowing them to better understand and benefit from this technology. This requires ongoing efforts to bridge the gap between research and practical implementation, fostering the creation of user-friendly tools and platforms that can effectively harness the potential of music-induced emotion recognition for broader applications and public engagement. In the realm of consumer applications, there is still much to be explored regarding the potential combination of EEG and personalized music to develop emotional regulation technology and products for users.</p> <a id="h9" name="h9"></a>
<h2>Author contributions</h2>
<p class="mb0">YS: Conceptualization, Data curation, Funding acquisition, Investigation, Methodology, Writing &#x2013; original draft, Writing &#x2013; review &#x00026; editing. YL: Resources, Supervision, Validation, Writing &#x2013; original draft. YX: Data curation, Resources, Supervision, Validation, Writing &#x2013; review &#x00026; editing. JM: Formal analysis, Visualization, Writing &#x2013; original draft. DL: Conceptualization, Formal analysis, Project administration, Writing &#x2013; original draft, Writing &#x2013; review &#x00026; editing.</p> <a id="h10" name="h10"></a>
<h2>Funding</h2>
<p class="mb0">The author(s) declare that financial support was received for the research, authorship, and/or publication of this article. This study was supported by the Zhejiang International Studies University&#x2019;s key project &#x201C;A comparative study of Chinese and English art education from the perspective of nationalization&#x201D; (No. 090500112016), the first-class curriculum construction project from the Department of Education of Zhejiang Province (No. 080830302022), and Zhejiang Natural Science Foundation project (No. LQ21E060006).</p> <a id="h11" name="h11"></a>
<h2>Conflict of interest</h2>
<p class="mb0">The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p> <a id="h12" name="h12"></a>
<h2>Publisher&#x2019;s note</h2>
<p class="mb0">All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p> <a id="h13" name="h13"></a>
<h2>References</h2>
<div class="References">
<p class="ReferencesCopy1"><a name="ref1" id="ref1"></a>Aftanas, L. I., Reva, N. V., Savotina, L. N., and Makhnev, V. P. (2006). Neurophysiological correlates of induced discrete emotions in humans: an individually oriented analysis. <i>Neurosci. Behav. Physiol.</i> 36, 119&#x2013;130. doi: 10.1007/s11055-005-0170-6 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/16380825" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1007/s11055-005-0170-6" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=L.+I.+Aftanas&#x00026;author=N.+V.+Reva&#x00026;author=L.+N.+Savotina&#x00026;author=V.+P.+Makhnev&#x00026;publication_year=2006&#x00026;title=Neurophysiological+correlates+of+induced+discrete+emotions+in+humans:+an+individually+oriented+analysis&#x00026;journal=Neurosci.+Behav.+Physiol.&#x00026;volume=36&#x00026;pages=119-130" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref2" id="ref2"></a>Ahmad, J., Ellis, C., Leech, R., Voytek, B., Garces, P., Jones, E., et al. (2022). From mechanisms to markers: novel noninvasive EEG proxy markers of the neural excitation and inhibition system in humans. <i>Transl. Psychiatry</i> 12:467. doi: 10.1038/s41398-022-02218-z </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/36344497" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1038/s41398-022-02218-z" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Ahmad&#x00026;author=C.+Ellis&#x00026;author=R.+Leech&#x00026;author=B.+Voytek&#x00026;author=P.+Garces&#x00026;author=E.+Jones&#x00026;publication_year=2022&#x00026;title=From+mechanisms+to+markers:+novel+noninvasive+EEG+proxy+markers+of+the+neural+excitation+and+inhibition+system+in+humans&#x00026;journal=Transl.+Psychiatry&#x00026;volume=12&#x00026;pages=467" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref3" id="ref3"></a>Alarcao, S. M., and Fonseca, M. J. (2019). Emotions recognition using EEG signals: a survey. <i>IEEE Trans. Affect. Comput.</i> 10, 374&#x2013;393. doi: 10.1109/TAFFC.2017.2714671</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TAFFC.2017.2714671" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+M.+Alarcao&#x00026;author=M.+J.+Fonseca&#x00026;publication_year=2019&#x00026;title=Emotions+recognition+using+EEG+signals:+a+survey&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=10&#x00026;pages=374-393" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref4" id="ref4"></a>Aldridge, A., Barnes, E., Bethel, C. L., Carruth, D. W., Kocturova, M., Pleva, M., et al. (2019). &#x201C;Accessible electroencephalograms (EEGs): a comparative review with OpenBCI&#x2019;s ultracortex mark IV headset&#x201D; in <i>2019 29th international conference radioelektronika (RADIOELEKTRONIKA)</i> (Pardubice, Czech Republic: IEEE), 1&#x2013;6.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?author=A.+Aldridge&#x00026;author=E.+Barnes&#x00026;author=C.+L.+Bethel&#x00026;author=D.+W.+Carruth&#x00026;author=M.+Kocturova&#x00026;author=M.+Pleva&#x00026;publication_year=2019&#x00026;title=Accessible+electroencephalograms+(EEGs):+a+comparative+review+with+OpenBCI&#x2019;s+ultracortex+mark+IV+headset&#x00026;journal=2019+29th+international+conference+radioelektronika+(RADIOELEKTRONIKA)&#x00026;pages=1-6" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref5" id="ref5"></a>Aljanaki, A., Yang, Y.-H., and Soleymani, M. (2017). Developing a benchmark for emotional analysis of music. <i>PLoS One</i> 12:e0173392. doi: 10.1371/journal.pone.0173392 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/28282400" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1371/journal.pone.0173392" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Aljanaki&#x00026;author=Y.-H.+Yang&#x00026;author=M.+Soleymani&#x00026;publication_year=2017&#x00026;title=Developing+a+benchmark+for+emotional+analysis+of+music&#x00026;journal=PLoS+One&#x00026;volume=12&#x00026;pages=e0173392" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref6" id="ref6"></a>Apicella, A., Arpaia, P., Frosolone, M., Improta, G., Moccaldi, N., and Pollastro, A. (2022a). EEG-based measurement system for monitoring student engagement in learning 4.0. <i>Sci. Rep.</i> 12:5857. doi: 10.1038/s41598-022-09578-y </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/35393470" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1038/s41598-022-09578-y" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Apicella&#x00026;author=P.+Arpaia&#x00026;author=M.+Frosolone&#x00026;author=G.+Improta&#x00026;author=N.+Moccaldi&#x00026;author=A.+Pollastro&#x00026;publication_year=2022a&#x00026;title=EEG-based+measurement+system+for+monitoring+student+engagement+in+learning+4.0&#x00026;journal=Sci.+Rep.&#x00026;volume=12&#x00026;pages=5857" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref7" id="ref7"></a>Apicella, A., Arpaia, P., Isgro, F., Mastrati, G., and Moccaldi, N. (2022b). A survey on EEG-based solutions for emotion recognition with a low number of channels. <i>IEEE Access</i> 10, 117411&#x2013;117428. doi: 10.1109/ACCESS.2022.3219844</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/ACCESS.2022.3219844" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Apicella&#x00026;author=P.+Arpaia&#x00026;author=F.+Isgro&#x00026;author=G.+Mastrati&#x00026;author=N.+Moccaldi&#x00026;publication_year=2022b&#x00026;title=A+survey+on+EEG-based+solutions+for+emotion+recognition+with+a+low+number+of+channels&#x00026;journal=IEEE+Access&#x00026;volume=10&#x00026;pages=117411-117428" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref8" id="ref8"></a>Ara, A., and Marco-Pallar&#x000E9;s, J. (2020). Fronto-temporal theta phase-synchronization underlies music-evoked pleasantness. <i>NeuroImage</i> 212:116665. doi: 10.1016/j.neuroimage.2020.116665 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/32087373" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.neuroimage.2020.116665" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Ara&#x00026;author=J.+Marco-Pallar&#x000E9;s&#x00026;publication_year=2020&#x00026;title=Fronto-temporal+theta+phase-synchronization+underlies+music-evoked+pleasantness&#x00026;journal=NeuroImage&#x00026;volume=212&#x00026;pages=116665" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref9" id="ref9"></a>Bagherzadeh, S., Maghooli, K., Shalbaf, A., and Maghsoudi, A. (2023). A hybrid EEG-based emotion recognition approach using wavelet convolutional neural networks and support vector machine. <i>Basic Clin. Neurosci.</i> 14, 87&#x2013;102. doi: 10.32598/bcn.2021.3133.1 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/37346875" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.32598/bcn.2021.3133.1" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Bagherzadeh&#x00026;author=K.+Maghooli&#x00026;author=A.+Shalbaf&#x00026;author=A.+Maghsoudi&#x00026;publication_year=2023&#x00026;title=A+hybrid+EEG-based+emotion+recognition+approach+using+wavelet+convolutional+neural+networks+and+support+vector+machine&#x00026;journal=Basic+Clin.+Neurosci.&#x00026;volume=14&#x00026;pages=87-102" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref10" id="ref10"></a>Balasubramanian, G., Kanagasabai, A., Mohan, J., and Seshadri, N. P. G. (2018). Music induced emotion using wavelet packet decomposition&#x2014;an EEG study. <i>Biomed. Signal Process. Control</i> 42, 115&#x2013;128. doi: 10.1016/j.bspc.2018.01.015</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.bspc.2018.01.015" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Balasubramanian&#x00026;author=A.+Kanagasabai&#x00026;author=J.+Mohan&#x00026;author=N.+P.+G.+Seshadri&#x00026;publication_year=2018&#x00026;title=Music+induced+emotion+using+wavelet+packet+decomposition&#x2014;an+EEG+study&#x00026;journal=Biomed.+Signal+Process.+Control&#x00026;volume=42&#x00026;pages=115-128" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref11" id="ref11"></a>Banerjee, A., Sanyal, S., Patranabis, A., Banerjee, K., Guhathakurta, T., Sengupta, R., et al. (2016). Study on brain dynamics by non linear analysis of music induced EEG signals. <i>Phys. A Stat. Mech. Appl.</i> 444, 110&#x2013;120. doi: 10.1016/j.physa.2015.10.030</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.physa.2015.10.030" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Banerjee&#x00026;author=S.+Sanyal&#x00026;author=A.+Patranabis&#x00026;author=K.+Banerjee&#x00026;author=T.+Guhathakurta&#x00026;author=R.+Sengupta&#x00026;publication_year=2016&#x00026;title=Study+on+brain+dynamics+by+non+linear+analysis+of+music+induced+EEG+signals&#x00026;journal=Phys.+A+Stat.+Mech.+Appl.&#x00026;volume=444&#x00026;pages=110-120" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref12" id="ref12"></a>Bergee, M. J., and Weingarten, K. M. (2021). Multilevel models of the relationship between music achievement and reading and math achievement. <i>J. Res. Music. Educ.</i> 68, 398&#x2013;418. doi: 10.1177/0022429420941432</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1177/0022429420941432" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+J.+Bergee&#x00026;author=K.+M.+Weingarten&#x00026;publication_year=2021&#x00026;title=Multilevel+models+of+the+relationship+between+music+achievement+and+reading+and+math+achievement&#x00026;journal=J.+Res.+Music.+Educ.&#x00026;volume=68&#x00026;pages=398-418" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref13" id="ref13"></a>Brotzer, J. M., Mosqueda, E. R., and Gorro, K. (2019). Predicting emotion in music through audio pattern analysis. <i>IOP Conf. Ser. Mater. Sci. Eng.</i> 482:12021. doi: 10.1088/1757-899X/482/1/012021</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1088/1757-899X/482/1/012021" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+M.+Brotzer&#x00026;author=E.+R.+Mosqueda&#x00026;author=K.+Gorro&#x00026;publication_year=2019&#x00026;title=Predicting+emotion+in+music+through+audio+pattern+analysis&#x00026;journal=IOP+Conf.+Ser.+Mater.+Sci.+Eng.&#x00026;volume=482&#x00026;pages=12021" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref14" id="ref14"></a>Byrns, A., Abdessalem, H. B., Cuesta, M., Bruneau, M.-A., Belleville, S., and Frasson, C. (2020). EEG analysis of the contribution of music therapy and virtual reality to the improvement of cognition in Alzheimer&#x2019;s disease. <i>J. Biomed. Sci. Eng.</i> 13, 187&#x2013;201. doi: 10.4236/jbise.2020.138018</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.4236/jbise.2020.138018" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Byrns&#x00026;author=H.+B.+Abdessalem&#x00026;author=M.+Cuesta&#x00026;author=M.-A.+Bruneau&#x00026;author=S.+Belleville&#x00026;author=C.+Frasson&#x00026;publication_year=2020&#x00026;title=EEG+analysis+of+the+contribution+of+music+therapy+and+virtual+reality+to+the+improvement+of+cognition+in+Alzheimer&#x2019;s+disease&#x00026;journal=J.+Biomed.+Sci.+Eng.&#x00026;volume=13&#x00026;pages=187-201" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref15" id="ref15"></a>Cai, Q., Cui, G.-C., and Wang, H.-X. (2022). EEG-based emotion recognition using multiple kernel learning. <i>Mach. Intell. Res.</i> 19, 472&#x2013;484. doi: 10.1007/s11633-022-1352-1</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1007/s11633-022-1352-1" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Q.+Cai&#x00026;author=G.-C.+Cui&#x00026;author=H.-X.+Wang&#x00026;publication_year=2022&#x00026;title=EEG-based+emotion+recognition+using+multiple+kernel+learning&#x00026;journal=Mach.+Intell.+Res.&#x00026;volume=19&#x00026;pages=472-484" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref16" id="ref16"></a>Carpente, J., Casenhiser, D. M., Kelliher, M., Mulholland, J., Sluder, H. L., Crean, A., et al. (2022). The impact of imitation on engagement in minimally verbal children with autism during improvisational music therapy. <i>Nord. J. Music. Ther.</i> 31, 44&#x2013;62. doi: 10.1080/08098131.2021.1924843</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1080/08098131.2021.1924843" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Carpente&#x00026;author=D.+M.+Casenhiser&#x00026;author=M.+Kelliher&#x00026;author=J.+Mulholland&#x00026;author=H.+L.+Sluder&#x00026;author=A.+Crean&#x00026;publication_year=2022&#x00026;title=The+impact+of+imitation+on+engagement+in+minimally+verbal+children+with+autism+during+improvisational+music+therapy&#x00026;journal=Nord.+J.+Music.+Ther.&#x00026;volume=31&#x00026;pages=44-62" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref17" id="ref17"></a>Chen, Y.-A., Yang, Y.-H., Wang, J.-C., and Chen, H. (2015). The AMG1608 dataset for music emotion recognition. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), (South Brisbane, Queensland, Australia: IEEE), 693&#x2013;697.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?author=Y.-A.+Chen&#x00026;author=Y.-H.+Yang&#x00026;author=J.-C.+Wang&#x00026;author=H.+Chen&#x00026;publication_year=2015&#x00026;title=The+AMG1608+dataset+for+music+emotion+recognition&#x00026;pages=693-697" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref19" id="ref19"></a>Colin, C., Prince, V., Bensoussan, J.-L., and Picot, M.-C. (2023). Music therapy for health workers to reduce stress, mental workload and anxiety: a systematic review. <i>J. Public Health</i> 45, e532&#x2013;e541. doi: 10.1093/pubmed/fdad059 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/37147921" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1093/pubmed/fdad059" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+Colin&#x00026;author=V.+Prince&#x00026;author=J.-L.+Bensoussan&#x00026;author=M.-C.+Picot&#x00026;publication_year=2023&#x00026;title=Music+therapy+for+health+workers+to+reduce+stress+mental+workload+and+anxiety:+a+systematic+review&#x00026;journal=J.+Public+Health&#x00026;volume=45&#x00026;pages=e532-e541" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref20" id="ref20"></a>Contreras-Molina, M., Rueda-N&#x000FA;&#x000F1;ez, A., P&#x000E9;rez-Collado, M. L., and Garc&#x000ED;a-Maestro, A. (2021). Effect of music therapy on anxiety and pain in the critical polytraumatised patient. <i>Enferm. Intensiva</i> 32, 79&#x2013;87. doi: 10.1016/j.enfie.2020.03.005 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/34099268" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.enfie.2020.03.005" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Contreras-Molina&#x00026;author=A.+Rueda-N&#x000FA;&#x000F1;ez&#x00026;author=M.+L.+P&#x000E9;rez-Collado&#x00026;author=A.+Garc&#x000ED;a-Maestro&#x00026;publication_year=2021&#x00026;title=Effect+of+music+therapy+on+anxiety+and+pain+in+the+critical+polytraumatised+patient&#x00026;journal=Enferm.+Intensiva&#x00026;volume=32&#x00026;pages=79-87" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref21" id="ref21"></a>Cui, X., Wu, Y., Wu, J., You, Z., Xiahou, J., and Ouyang, M. (2022). A review: music-emotion recognition and analysis based on EEG signals. <i>Front. Neuroinform.</i> 16:997282. doi: 10.3389/fninf.2022.997282 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/36387584" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fninf.2022.997282" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=X.+Cui&#x00026;author=Y.+Wu&#x00026;author=J.+Wu&#x00026;author=Z.+You&#x00026;author=J.+Xiahou&#x00026;author=M.+Ouyang&#x00026;publication_year=2022&#x00026;title=A+review:+music-emotion+recognition+and+analysis+based+on+EEG+signals&#x00026;journal=Front.+Neuroinform.&#x00026;volume=16&#x00026;pages=997282" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref22" id="ref22"></a>Dadebayev, D., Goh, W. W., and Tan, E. X. (2022). EEG-based emotion recognition: review of commercial EEG devices and machine learning techniques. <i>J. King Saud Univ. Comput. Inf. Sci.</i> 34, 4385&#x2013;4401. doi: 10.1016/j.jksuci.2021.03.009</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.jksuci.2021.03.009" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Dadebayev&#x00026;author=W.+W.+Goh&#x00026;author=E.+X.+Tan&#x00026;publication_year=2022&#x00026;title=EEG-based+emotion+recognition:+review+of+commercial+EEG+devices+and+machine+learning+techniques&#x00026;journal=J.+King+Saud+Univ.+Comput.+Inf.+Sci.&#x00026;volume=34&#x00026;pages=4385-4401" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref23" id="ref23"></a>Dar, M., Akram, M., Yuvaraj, R., Khawaja, S., and Murugappan, M. (2022). EEG-based emotion charting for Parkinson&#x2019;s disease patients using convolutional recurrent neural networks and cross dataset learning. <i>Comput. Biol. Med.</i> 144:105327. doi: 10.1016/j.compbiomed.2022.105327 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/35303579" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.compbiomed.2022.105327" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Dar&#x00026;author=M.+Akram&#x00026;author=R.+Yuvaraj&#x00026;author=S.+Khawaja&#x00026;author=M.+Murugappan&#x00026;publication_year=2022&#x00026;title=EEG-based+emotion+charting+for+Parkinson&#x2019;s+disease+patients+using+convolutional+recurrent+neural+networks+and+cross+dataset+learning&#x00026;journal=Comput.+Biol.+Med.&#x00026;volume=144&#x00026;pages=105327" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref24" id="ref24"></a>Davidson, R. J., and Fox, N. A. (1982). Asymmetrical brain activity discriminates between positive and negative affective stimuli in human infants. <i>Science</i> 218, 1235&#x2013;1237. doi: 10.1126/science.7146906 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/7146906" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1126/science.7146906" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+J.+Davidson&#x00026;author=N.+A.+Fox&#x00026;publication_year=1982&#x00026;title=Asymmetrical+brain+activity+discriminates+between+positive+and+negative+affective+stimuli+in+human+infants&#x00026;journal=Science&#x00026;volume=218&#x00026;pages=1235-1237" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref25" id="ref25"></a>Du, X., Ma, C., Zhang, G., Li, J., Lai, Y.-K., Zhao, G., et al. (2022). An efficient LSTM network for emotion recognition from multichannel EEG signals. <i>IEEE Trans. Affect. Comput.</i> 13, 1528&#x2013;1540. doi: 10.1109/TAFFC.2020.3013711</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TAFFC.2020.3013711" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=X.+Du&#x00026;author=C.+Ma&#x00026;author=G.+Zhang&#x00026;author=J.+Li&#x00026;author=Y.-K.+Lai&#x00026;author=G.+Zhao&#x00026;publication_year=2022&#x00026;title=An+efficient+LSTM+network+for+emotion+recognition+from+multichannel+EEG+signals&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=13&#x00026;pages=1528-1540" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref26" id="ref26"></a>Du, R., Zhu, S., Ni, H., Mao, T., Li, J., and Wei, R. (2023). Valence-arousal classification of emotion evoked by Chinese ancient-style music using 1D-CNN-BiLSTM model on EEG signals for college students. <i>Multimed. Tools Appl.</i> 82, 15439&#x2013;15456. doi: 10.1007/s11042-022-14011-7 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/36213341" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1007/s11042-022-14011-7" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+Du&#x00026;author=S.+Zhu&#x00026;author=H.+Ni&#x00026;author=T.+Mao&#x00026;author=J.+Li&#x00026;author=R.+Wei&#x00026;publication_year=2023&#x00026;title=Valence-arousal+classification+of+emotion+evoked+by+Chinese+ancient-style+music+using+1D-CNN-BiLSTM+model+on+EEG+signals+for+college+students&#x00026;journal=Multimed.+Tools+Appl.&#x00026;volume=82&#x00026;pages=15439-15456" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref27" id="ref27"></a>Egger, M., Ley, M., and Hanke, S. (2019). Emotion recognition from physiological signal analysis: a review. <i>Electron. Notes Theor. Comput. Sci.</i> 343, 35&#x2013;55. doi: 10.1016/j.entcs.2019.04.009</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.entcs.2019.04.009" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Egger&#x00026;author=M.+Ley&#x00026;author=S.+Hanke&#x00026;publication_year=2019&#x00026;title=Emotion+recognition+from+physiological+signal+analysis:+a+review&#x00026;journal=Electron.+Notes+Theor.+Comput.+Sci.&#x00026;volume=343&#x00026;pages=35-55" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref28" id="ref28"></a>Er, M. B., &#x000C7;i&#x011F;, H., and Aydilek, &#x0130;. B. (2021). A new approach to recognition of human emotions using brain signals and music stimuli. <i>Appl. Acoust.</i> 175:107840. doi: 10.1016/j.apacoust.2020.107840</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.apacoust.2020.107840" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+B.+Er&#x00026;author=H.+&#x000C7;i&#x011F;&#x00026;author=&#x0130;.+B.+Aydilek&#x00026;publication_year=2021&#x00026;title=A+new+approach+to+recognition+of+human+emotions+using+brain+signals+and+music+stimuli&#x00026;journal=Appl.+Acoust.&#x00026;volume=175&#x00026;pages=107840" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref29" id="ref29"></a>Fachner, J., Gold, C., and Erkkil&#x000E4;, J. (2013). Music therapy modulates fronto-temporal activity in rest-EEG in depressed clients. <i>Brain Topogr.</i> 26, 338&#x2013;354. doi: 10.1007/s10548-012-0254-x </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/22983820" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1007/s10548-012-0254-x" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Fachner&#x00026;author=C.+Gold&#x00026;author=J.+Erkkil&#x000E4;&#x00026;publication_year=2013&#x00026;title=Music+therapy+modulates+fronto-temporal+activity+in+rest-EEG+in+depressed+clients&#x00026;journal=Brain+Topogr.&#x00026;volume=26&#x00026;pages=338-354" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref30" id="ref30"></a>Fedotchev, A., Parin, S., Polevaya, S., and Zemlianaia, A. (2022). EEG-based musical neurointerfaces in the correction of stress-induced states. <i>Brain-Comput. Interfaces</i> 9, 1&#x2013;6. doi: 10.1080/2326263X.2021.1964874</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1080/2326263X.2021.1964874" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Fedotchev&#x00026;author=S.+Parin&#x00026;author=S.+Polevaya&#x00026;author=A.+Zemlianaia&#x00026;publication_year=2022&#x00026;title=EEG-based+musical+neurointerfaces+in+the+correction+of+stress-induced+states&#x00026;journal=Brain-Comput.+Interfaces&#x00026;volume=9&#x00026;pages=1-6" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref31" id="ref31"></a>Gan, K., Li, R., Zhang, J., Sun, Z., and Yin, Z. (2024). Instantaneous estimation of momentary affective responses using neurophysiological signals and a spatiotemporal emotional intensity regression network. <i>Neural Netw.</i> 172:106080. doi: 10.1016/j.neunet.2023.12.034 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/38160622" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.neunet.2023.12.034" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=K.+Gan&#x00026;author=R.+Li&#x00026;author=J.+Zhang&#x00026;author=Z.+Sun&#x00026;author=Z.+Yin&#x00026;publication_year=2024&#x00026;title=Instantaneous+estimation+of+momentary+affective+responses+using+neurophysiological+signals+and+a+spatiotemporal+emotional+intensity+regression+network&#x00026;journal=Neural+Netw.&#x00026;volume=172&#x00026;pages=106080" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref32" id="ref32"></a>Geipel, J., Koenig, J., Hillecke, T. K., and Resch, F. (2022). Short-term music therapy treatment for adolescents with depression &#x2013; a pilot study. <i>Art. Psychother.</i> 77:101874. doi: 10.1016/j.aip.2021.101874</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.aip.2021.101874" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Geipel&#x00026;author=J.+Koenig&#x00026;author=T.+K.+Hillecke&#x00026;author=F.+Resch&#x00026;publication_year=2022&#x00026;title=Short-term+music+therapy+treatment+for+adolescents+with+depression+&#x2013;+a+pilot+study&#x00026;journal=Art.+Psychother.&#x00026;volume=77&#x00026;pages=101874" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref33" id="ref33"></a>Geretsegger, M., Fusar-Poli, L., Elefant, C., M&#x000F6;ssler, K. A., Vitale, G., and Gold, C. (2022). Music therapy for autistic people. <i>Cochrane Database Syst. Rev.</i> 2022:CD004381. doi: 10.1002/14651858.CD004381.pub4 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/35532041" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1002/14651858.CD004381.pub4" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Geretsegger&#x00026;author=L.+Fusar-Poli&#x00026;author=C.+Elefant&#x00026;author=K.+A.+M&#x000F6;ssler&#x00026;author=G.+Vitale&#x00026;author=C.+Gold&#x00026;publication_year=2022&#x00026;title=Music+therapy+for+autistic+people&#x00026;journal=Cochrane+Database+Syst.+Rev.&#x00026;volume=2022&#x00026;pages=CD004381" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref34" id="ref34"></a>Gomez-Canon, J. S., Cano, E., Eerola, T., Herrera, P., Hu, X., Yang, Y.-H., et al. (2021). Music emotion recognition: toward new, robust standards in personalized and context-sensitive applications. <i>IEEE Signal Process. Mag.</i> 38, 106&#x2013;114. doi: 10.1109/MSP.2021.3106232</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/MSP.2021.3106232" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+S.+Gomez-Canon&#x00026;author=E.+Cano&#x00026;author=T.+Eerola&#x00026;author=P.+Herrera&#x00026;author=X.+Hu&#x00026;author=Y.-H.+Yang&#x00026;publication_year=2021&#x00026;title=Music+emotion+recognition:+toward+new+robust+standards+in+personalized+and+context-sensitive+applications&#x00026;journal=IEEE+Signal+Process.+Mag.&#x00026;volume=38&#x00026;pages=106-114" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref35" id="ref35"></a>Griffiths, D., Cunningham, S., Weinel, J., and Picking, R. (2021). A multi-genre model for music emotion recognition using linear regressors. <i>J. New Music Res.</i> 50, 355&#x2013;372. doi: 10.1080/09298215.2021.1977336</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1080/09298215.2021.1977336" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Griffiths&#x00026;author=S.+Cunningham&#x00026;author=J.+Weinel&#x00026;author=R.+Picking&#x00026;publication_year=2021&#x00026;title=A+multi-genre+model+for+music+emotion+recognition+using+linear+regressors&#x00026;journal=J.+New+Music+Res.&#x00026;volume=50&#x00026;pages=355-372" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref36" id="ref36"></a>Guo, S., Lu, J., Wang, Y., Li, Y., Huang, B., Zhang, Y., et al. (2020). Sad music modulates pain perception: an EEG study. <i>J. Pain Res.</i> 13, 2003&#x2013;2012. doi: 10.2147/JPR.S264188 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/32848448" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.2147/JPR.S264188" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Guo&#x00026;author=J.+Lu&#x00026;author=Y.+Wang&#x00026;author=Y.+Li&#x00026;author=B.+Huang&#x00026;author=Y.+Zhang&#x00026;publication_year=2020&#x00026;title=Sad+music+modulates+pain+perception:+an+EEG+study&#x00026;journal=J.+Pain+Res.&#x00026;volume=13&#x00026;pages=2003-2012" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref37" id="ref37"></a>Hartmann, M., Mavrolampados, A., Toiviainen, P., Saarikallio, S., Foubert, K., Brabant, O., et al. (2023). Musical interaction in music therapy for depression treatment. <i>Psychol. Music</i> 51, 33&#x2013;50. doi: 10.1177/03057356221084368</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1177/03057356221084368" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Hartmann&#x00026;author=A.+Mavrolampados&#x00026;author=P.+Toiviainen&#x00026;author=S.+Saarikallio&#x00026;author=K.+Foubert&#x00026;author=O.+Brabant&#x00026;publication_year=2023&#x00026;title=Musical+interaction+in+music+therapy+for+depression+treatment&#x00026;journal=Psychol.+Music&#x00026;volume=51&#x00026;pages=33-50" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref38" id="ref38"></a>Hevner, K. (1936). Experimental studies of the elements of expression in music. <i>Am. J. Psychol.</i> 48:246. doi: 10.2307/1415746</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.2307/1415746" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=K.+Hevner&#x00026;publication_year=1936&#x00026;title=Experimental+studies+of+the+elements+of+expression+in+music&#x00026;journal=Am.+J.+Psychol.&#x00026;volume=48&#x00026;pages=246" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref39" id="ref39"></a>Hossain, S., Rahman, M., Chakrabarty, A., Rashid, M., Kuwana, A., and Kobayashi, H. (2023). Emotional state classification from MUSIC-based features of multichannel EEG signals. <i>Bioengineering</i> 10:99. doi: 10.3390/bioengineering10010099 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/36671671" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/bioengineering10010099" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Hossain&#x00026;author=M.+Rahman&#x00026;author=A.+Chakrabarty&#x00026;author=M.+Rashid&#x00026;author=A.+Kuwana&#x00026;author=H.+Kobayashi&#x00026;publication_year=2023&#x00026;title=Emotional+state+classification+from+MUSIC-based+features+of+multichannel+EEG+signals&#x00026;journal=Bioengineering&#x00026;volume=10&#x00026;pages=99" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref40" id="ref40"></a>Humphreys, J. T. (1998). Musical aptitude testing: from James McKeen Cattell to Carl Emil seashore. <i>Res. Stud. Music Educ.</i> 10, 42&#x2013;53. doi: 10.1177/1321103X9801000104</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1177/1321103X9801000104" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+T.+Humphreys&#x00026;publication_year=1998&#x00026;title=Musical+aptitude+testing:+from+James+McKeen+Cattell+to+Carl+Emil+seashore&#x00026;journal=Res.+Stud.+Music+Educ.&#x00026;volume=10&#x00026;pages=42-53" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref41" id="ref41"></a>Katsigiannis, S., and Ramzan, N. (2018). DREAMER: a database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices. <i>IEEE J. Biomed. Health Inform.</i> 22, 98&#x2013;107. doi: 10.1109/JBHI.2017.2688239 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/28368836" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1109/JBHI.2017.2688239" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Katsigiannis&#x00026;author=N.+Ramzan&#x00026;publication_year=2018&#x00026;title=DREAMER:+a+database+for+emotion+recognition+through+EEG+and+ECG+signals+from+wireless+low-cost+off-the-shelf+devices&#x00026;journal=IEEE+J.+Biomed.+Health+Inform.&#x00026;volume=22&#x00026;pages=98-107" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref42" id="ref42"></a>Khabiri, H., Naseh Talebi, M., Kamran, M. F., Akbari, S., Zarrin, F., and Mohandesi, F. (2023). Music-induced emotion recognition based on feature reduction using PCA from EEG signals. <i>Front. Biomed. Technol.</i> 11, 59&#x2013;68. doi: 10.18502/fbt.v11i1.14512</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.18502/fbt.v11i1.14512" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Khabiri&#x00026;author=M.+Naseh+Talebi&#x00026;author=M.+F.+Kamran&#x00026;author=S.+Akbari&#x00026;author=F.+Zarrin&#x00026;author=F.+Mohandesi&#x00026;publication_year=2023&#x00026;title=Music-induced+emotion+recognition+based+on+feature+reduction+using+PCA+from+EEG+signals&#x00026;journal=Front.+Biomed.+Technol.&#x00026;volume=11&#x00026;pages=59-68" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref43" id="ref43"></a>Khare, S. K., and Bajaj, V. (2021). Time&#x2013;frequency representation and convolutional neural network-based emotion recognition. <i>IEEE Trans. Neural Networks Learn. Syst.</i> 32, 2901&#x2013;2909. doi: 10.1109/TNNLS.2020.3008938 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/32735536" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TNNLS.2020.3008938" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+K.+Khare&#x00026;author=V.+Bajaj&#x00026;publication_year=2021&#x00026;title=Time&#x2013;frequency+representation+and+convolutional+neural+network-based+emotion+recognition&#x00026;journal=IEEE+Trans.+Neural+Networks+Learn.+Syst.&#x00026;volume=32&#x00026;pages=2901-2909" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref44" id="ref44"></a>Kim, H., Zhang, D., Kim, L., and Im, C.-H. (2022). Classification of individual&#x2019;s discrete emotions reflected in facial microexpressions using electroencephalogram and facial electromyogram. <i>Expert Syst. Appl.</i> 188:116101. doi: 10.1016/j.eswa.2021.116101</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.eswa.2021.116101" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Kim&#x00026;author=D.+Zhang&#x00026;author=L.+Kim&#x00026;author=C.-H.+Im&#x00026;publication_year=2022&#x00026;title=Classification+of+individual&#x2019;s+discrete+emotions+reflected+in+facial+microexpressions+using+electroencephalogram+and+facial+electromyogram&#x00026;journal=Expert+Syst.+Appl.&#x00026;volume=188&#x00026;pages=116101" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref45" id="ref45"></a>Koelstra, S., Muhl, C., Soleymani, M., Lee, J.-S., Yazdani, A., Ebrahimi, T., et al. (2012). DEAP: a database for emotion analysis using physiological signals. <i>IEEE Trans. Affect. Comput.</i> 3, 18&#x2013;31. doi: 10.1109/T-AFFC.2011.15</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/T-AFFC.2011.15" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Koelstra&#x00026;author=C.+Muhl&#x00026;author=M.+Soleymani&#x00026;author=J.-S.+Lee&#x00026;author=A.+Yazdani&#x00026;author=T.+Ebrahimi&#x00026;publication_year=2012&#x00026;title=DEAP:+a+database+for+emotion+analysis+using+physiological+signals&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=3&#x00026;pages=18-31" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref46" id="ref46"></a>Lapomarda, G., Valer, S., Job, R., and Grecucci, A. (2022). Built to last: theta and delta changes in resting-state EEG activity after regulating emotions. <i>Brain Behav.</i> 12:e2597. doi: 10.1002/brb3.2597 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/35560984" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1002/brb3.2597" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Lapomarda&#x00026;author=S.+Valer&#x00026;author=R.+Job&#x00026;author=A.+Grecucci&#x00026;publication_year=2022&#x00026;title=Built+to+last:+theta+and+delta+changes+in+resting-state+EEG+activity+after+regulating+emotions&#x00026;journal=Brain+Behav.&#x00026;volume=12&#x00026;pages=e2597" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref47" id="ref47"></a>Li, P., Liu, H., Si, Y., Li, C., Li, F., Zhu, X., et al. (2019). EEG based emotion recognition by combining functional connectivity network and local activations. <i>IEEE Trans. Biomed. Eng.</i> 66, 2869&#x2013;2881. doi: 10.1109/TBME.2019.2897651 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/30735981" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TBME.2019.2897651" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+Li&#x00026;author=H.+Liu&#x00026;author=Y.+Si&#x00026;author=C.+Li&#x00026;author=F.+Li&#x00026;author=X.+Zhu&#x00026;publication_year=2019&#x00026;title=EEG+based+emotion+recognition+by+combining+functional+connectivity+network+and+local+activations&#x00026;journal=IEEE+Trans.+Biomed.+Eng.&#x00026;volume=66&#x00026;pages=2869-2881" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref48" id="ref48"></a>Li, D., Ruan, Y., Zheng, F., Lijuan, S., and Lin, Q. (2022a). Effect of Taiji post-standing on the brain analyzed with EEG signals. <i>J. Taiji Sci.</i> 1, 2&#x2013;7. doi: 10.57612/2022.jts.01.01</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.57612/2022.jts.01.01" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Li&#x00026;author=Y.+Ruan&#x00026;author=F.+Zheng&#x00026;author=S.+Lijuan&#x00026;author=Q.+Lin&#x00026;publication_year=2022a&#x00026;title=Effect+of+Taiji+post-standing+on+the+brain+analyzed+with+EEG+signals&#x00026;journal=J.+Taiji+Sci.&#x00026;volume=1&#x00026;pages=2-7" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref49" id="ref49"></a>Li, D., Ruan, Y., Zheng, F., Su, Y., and Lin, Q. (2022b). Fast sleep stage classification using cascaded support vector machines with single-channel EEG signals. <i>Sensors</i> 22:9914. doi: 10.3390/s22249914 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/36560286" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s22249914" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Li&#x00026;author=Y.+Ruan&#x00026;author=F.+Zheng&#x00026;author=Y.+Su&#x00026;author=Q.+Lin&#x00026;publication_year=2022b&#x00026;title=Fast+sleep+stage+classification+using+cascaded+support+vector+machines+with+single-channel+EEG+signals&#x00026;journal=Sensors&#x00026;volume=22&#x00026;pages=9914" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref50" id="ref50"></a>Li, M., Xu, H., Liu, X., and Lu, S. (2018). Emotion recognition from multichannel EEG signals using K-nearest neighbor classification. <i>Technol. Health Care</i> 26, 509&#x2013;519. doi: 10.3233/THC-174836 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/29758974" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3233/THC-174836" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Li&#x00026;author=H.+Xu&#x00026;author=X.+Liu&#x00026;author=S.+Lu&#x00026;publication_year=2018&#x00026;title=Emotion+recognition+from+multichannel+EEG+signals+using+K-nearest+neighbor+classification&#x00026;journal=Technol.+Health+Care&#x00026;volume=26&#x00026;pages=509-519" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref51" id="ref51"></a>Li, X., Zhang, Y., Tiwari, P., Song, D., Hu, B., Yang, M., et al. (2023). EEG based emotion recognition: a tutorial and review. <i>ACM Comput. Surv.</i> 55, 1&#x2013;57. doi: 10.1145/3524499</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1145/3524499" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=X.+Li&#x00026;author=Y.+Zhang&#x00026;author=P.+Tiwari&#x00026;author=D.+Song&#x00026;author=B.+Hu&#x00026;author=M.+Yang&#x00026;publication_year=2023&#x00026;title=EEG+based+emotion+recognition:+a+tutorial+and+review&#x00026;journal=ACM+Comput.+Surv.&#x00026;volume=55&#x00026;pages=1-57" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref52" id="ref52"></a>Li, Y., and Zheng, W. (2023). EEG processing in emotion recognition: inspired from a musical staff. <i>Multimed. Tools Appl.</i> 82, 4161&#x2013;4180. doi: 10.1007/s11042-022-13405-x</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1007/s11042-022-13405-x" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Y.+Li&#x00026;author=W.+Zheng&#x00026;publication_year=2023&#x00026;title=EEG+processing+in+emotion+recognition:+inspired+from+a+musical+staff&#x00026;journal=Multimed.+Tools+Appl.&#x00026;volume=82&#x00026;pages=4161-4180" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref53" id="ref53"></a>Liang, J., Tian, X., and Yang, W. (2021). Application of music therapy in general surgical treatment. <i>Biomed. Res. Int.</i> 2021, 1&#x2013;4. doi: 10.1155/2021/6169183 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/34621896" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1155/2021/6169183" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Liang&#x00026;author=X.+Tian&#x00026;author=W.+Yang&#x00026;publication_year=2021&#x00026;title=Application+of+music+therapy+in+general+surgical+treatment&#x00026;journal=Biomed.+Res.+Int.&#x00026;volume=2021&#x00026;pages=1-4" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref54" id="ref54"></a>Liu, H., Fang, Y., and Huang, Q. (2019). Music emotion recognition using a variant of recurrent neural network., in Proceedings of the 2018 international conference on mathematics, modeling, simulation and statistics application (MMSSA 2018), (Shanghai, China: Atlantis Press).</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?author=H.+Liu&#x00026;author=Y.+Fang&#x00026;author=Q.+Huang&#x00026;publication_year=2019&#x00026;title=Music+emotion+recognition+using+a+variant+of+recurrent+neural+network&#x00026;" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref55" id="ref55"></a>Liu, J., Wu, G., Luo, Y., Qiu, S., Yang, S., Li, W., et al. (2020). EEG-based emotion classification using a deep neural network and sparse autoencoder. <i>Front. Syst. Neurosci.</i> 14:43. doi: 10.3389/fnsys.2020.00043 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/32982703" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnsys.2020.00043" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Liu&#x00026;author=G.+Wu&#x00026;author=Y.+Luo&#x00026;author=S.+Qiu&#x00026;author=S.+Yang&#x00026;author=W.+Li&#x00026;publication_year=2020&#x00026;title=EEG-based+emotion+classification+using+a+deep+neural+network+and+sparse+autoencoder&#x00026;journal=Front.+Syst.+Neurosci.&#x00026;volume=14&#x00026;pages=43" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref56" id="ref56"></a>Lu, G., Jia, R., Liang, D., Yu, J., Wu, Z., and Chen, C. (2021). Effects of music therapy on anxiety: a meta-analysis of randomized controlled trials. <i>Psychiatry Res.</i> 304:114137. doi: 10.1016/j.psychres.2021.114137</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.psychres.2021.114137" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Lu&#x00026;author=R.+Jia&#x00026;author=D.+Liang&#x00026;author=J.+Yu&#x00026;author=Z.+Wu&#x00026;author=C.+Chen&#x00026;publication_year=2021&#x00026;title=Effects+of+music+therapy+on+anxiety:+a+meta-analysis+of+randomized+controlled+trials&#x00026;journal=Psychiatry+Res.&#x00026;volume=304&#x00026;pages=114137" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref57" id="ref57"></a>Luo, E., Pan, W., and Fan, X. (2023). Music, language, and autism: neurological insights for enhanced learning. <i>Int. J. Innov. Res. Med. Sci.</i> 8, 398&#x2013;408. doi: 10.23958/ijirms/vol08-i09/1743</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.23958/ijirms/vol08-i09/1743" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+Luo&#x00026;author=W.+Pan&#x00026;author=X.+Fan&#x00026;publication_year=2023&#x00026;title=Music+language+and+autism:+neurological+insights+for+enhanced+learning&#x00026;journal=Int.+J.+Innov.+Res.+Med.+Sci.&#x00026;volume=8&#x00026;pages=398-408" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref58" id="ref58"></a>Maffei, A. (2020). Spectrally resolved EEG intersubject correlation reveals distinct cortical oscillatory patterns during free-viewing of affective scenes. <i>Psychophysiology</i> 57:e13652. doi: 10.1111/psyp.13652 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/33460185" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1111/psyp.13652" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Maffei&#x00026;publication_year=2020&#x00026;title=Spectrally+resolved+EEG+intersubject+correlation+reveals+distinct+cortical+oscillatory+patterns+during+free-viewing+of+affective+scenes&#x00026;journal=Psychophysiology&#x00026;volume=57&#x00026;pages=e13652" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref59" id="ref59"></a>Mahmoud, A., Amin, K., Al Rahhal, M., Elkilani, W., Mekhalfi, M., and Ibrahim, M. (2023). A CNN approach for emotion recognition via EEG. <i>Symmetry</i> 15:1822. doi: 10.3390/sym15101822</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.3390/sym15101822" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Mahmoud&#x00026;author=K.+Amin&#x00026;author=M.+Al+Rahhal&#x00026;author=W.+Elkilani&#x00026;author=M.+Mekhalfi&#x00026;author=M.+Ibrahim&#x00026;publication_year=2023&#x00026;title=A+CNN+approach+for+emotion+recognition+via+EEG&#x00026;journal=Symmetry&#x00026;volume=15&#x00026;pages=1822" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref60" id="ref60"></a>Mart&#x000ED;nez-Saez, M., Ros, L., L&#x000F3;pez-Cano, M., Nieto, M., Navarro, B., and Latorre, J. (2024). Effect of popular songs from the reminiscence bump as autobiographical memory cues in aging: a preliminary study using EEG. <i>Front. Neurosci.</i> 17:1300751. doi: 10.3389/fnins.2023.1300751 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/38264494" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2023.1300751" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Mart&#x000ED;nez-Saez&#x00026;author=L.+Ros&#x00026;author=M.+L&#x000F3;pez-Cano&#x00026;author=M.+Nieto&#x00026;author=B.+Navarro&#x00026;author=J.+Latorre&#x00026;publication_year=2024&#x00026;title=Effect+of+popular+songs+from+the+reminiscence+bump+as+autobiographical+memory+cues+in+aging:+a+preliminary+study+using+EEG&#x00026;journal=Front.+Neurosci.&#x00026;volume=17&#x00026;pages=1300751" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref61" id="ref61"></a>Martins, I., Lima, C., and Pinheiro, A. (2022). Enhanced salience of musical sounds in singers and instrumentalists. <i>Cogn. Affect. Behav. Ne.</i> 22, 1044&#x2013;1062. doi: 10.3758/s13415-022-01007-x </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/35501427" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3758/s13415-022-01007-x" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=I.+Martins&#x00026;author=C.+Lima&#x00026;author=A.+Pinheiro&#x00026;publication_year=2022&#x00026;title=Enhanced+salience+of+musical+sounds+in+singers+and+instrumentalists&#x00026;journal=Cogn.+Affect.+Behav.+Ne.&#x00026;volume=22&#x00026;pages=1044-1062" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref62" id="ref62"></a>Metfessel, M. (1950). Carl emil seashore, 1866-1949. <i>Science</i> 111, 713&#x2013;717. doi: 10.1126/science.111.2896.713 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/15431064" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1126/science.111.2896.713" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Metfessel&#x00026;publication_year=1950&#x00026;title=Carl+emil+seashore+1866-1949&#x00026;journal=Science&#x00026;volume=111&#x00026;pages=713-717" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref63" id="ref63"></a>Micallef Grimaud, A., and Eerola, T. (2022). An interactive approach to emotional expression through musical cues. <i>Music. Sci.</i> 5:205920432110617. doi: 10.1177/20592043211061745</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1177/20592043211061745" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Micallef+Grimaud&#x00026;author=T.+Eerola&#x00026;publication_year=2022&#x00026;title=An+interactive+approach+to+emotional+expression+through+musical+cues&#x00026;journal=Music.+Sci.&#x00026;volume=5&#x00026;pages=205920432110617" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref64" id="ref64"></a>Moctezuma, L., Abe, T., and Molinas, M. (2022). Two-dimensional CNN-based distinction of human emotions from EEG channels selected by multi-objective evolutionary algorithm. <i>Sci. Rep.</i> 12:3523. doi: 10.1038/s41598-022-07517-5 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/35241745" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1038/s41598-022-07517-5" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=L.+Moctezuma&#x00026;author=T.+Abe&#x00026;author=M.+Molinas&#x00026;publication_year=2022&#x00026;title=Two-dimensional+CNN-based+distinction+of+human+emotions+from+EEG+channels+selected+by+multi-objective+evolutionary+algorithm&#x00026;journal=Sci.+Rep.&#x00026;volume=12&#x00026;pages=3523" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref65" id="ref65"></a>Nawaz, R., Ng, J. T., Nisar, H., and Voon, Y. V. (2019). Can background music help to relieve stress? An EEG analysis., in 2019 IEEE international conference on signal and image processing applications (ICSIPA), Kuala Lumpur, Malaysia: IEEE, 68&#x2013;72.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?author=R.+Nawaz&#x00026;author=J.+T.+Ng&#x00026;author=H.+Nisar&#x00026;author=Y.+V.+Voon&#x00026;publication_year=2019&#x00026;title=Can+background+music+help+to+relieve+stress?+An+EEG+analysis&#x00026;pages=68-72" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref66" id="ref66"></a>Oktavia, N. Y., Wibawa, A. D., Pane, E. S., and Purnomo, M. H. (2019). &#x201C;Human emotion classification based on EEG signals using na&#x000EF;ve bayes method&#x201D; in <i>2019 international seminar on application for technology of information and communication (iSemantic)</i> (Semarang, Indonesia: IEEE), 319&#x2013;324.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?author=N.+Y.+Oktavia&#x00026;author=A.+D.+Wibawa&#x00026;author=E.+S.+Pane&#x00026;author=M.+H.+Purnomo&#x00026;publication_year=2019&#x00026;title=Human+emotion+classification+based+on+EEG+signals+using+na&#x000EF;ve+bayes+method&#x00026;journal=2019+international+seminar+on+application+for+technology+of+information+and+communication+(iSemantic)&#x00026;pages=319-324" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref67" id="ref67"></a>Palazzi, A., Meschini, R., and Piccinini, C. A. (2021). NICU music therapy effects on maternal mental health and preterm infant&#x2019;s emotional arousal. <i>Infant Ment. Health J.</i> 42, 672&#x2013;689. doi: 10.1002/imhj.21938 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/34378804" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1002/imhj.21938" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Palazzi&#x00026;author=R.+Meschini&#x00026;author=C.+A.+Piccinini&#x00026;publication_year=2021&#x00026;title=NICU+music+therapy+effects+on+maternal+mental+health+and+preterm+infant&#x2019;s+emotional+arousal&#x00026;journal=Infant+Ment.+Health+J.&#x00026;volume=42&#x00026;pages=672-689" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref68" id="ref68"></a>Pan, J., Yang, F., Qiu, L., and Huang, H. (2022). Fusion of EEG-based activation, spatial, and connection patterns for fear emotion recognition. <i>Comput. Intell. Neurosci.</i> 2022, 1&#x2013;11. doi: 10.1155/2022/3854513 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/35463262" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1155/2022/3854513" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Pan&#x00026;author=F.+Yang&#x00026;author=L.+Qiu&#x00026;author=H.+Huang&#x00026;publication_year=2022&#x00026;title=Fusion+of+EEG-based+activation+spatial+and+connection+patterns+for+fear+emotion+recognition&#x00026;journal=Comput.+Intell.+Neurosci.&#x00026;volume=2022&#x00026;pages=1-11" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref69" id="ref69"></a>Panda, R., Malheiro, R., and Paiva, R. P. (2023). Audio features for music emotion recognition: a survey. <i>IEEE Trans. Affect. Comput.</i> 14, 68&#x2013;88. doi: 10.1109/TAFFC.2020.3032373</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TAFFC.2020.3032373" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+Panda&#x00026;author=R.+Malheiro&#x00026;author=R.+P.+Paiva&#x00026;publication_year=2023&#x00026;title=Audio+features+for+music+emotion+recognition:+a+survey&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=14&#x00026;pages=68-88" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref70" id="ref70"></a>Pandey, P., and Seeja, K. R. (2022). Subject independent emotion recognition from EEG using VMD and deep learning. <i>J. King Saud Univ. Comput. Inf. Sci.</i> 34, 1730&#x2013;1738. doi: 10.1016/j.jksuci.2019.11.003</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.jksuci.2019.11.003" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+Pandey&#x00026;author=K.+R.+Seeja&#x00026;publication_year=2022&#x00026;title=Subject+independent+emotion+recognition+from+EEG+using+VMD+and+deep+learning&#x00026;journal=J.+King+Saud+Univ.+Comput.+Inf.+Sci.&#x00026;volume=34&#x00026;pages=1730-1738" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref71" id="ref71"></a>Pei, G., Shang, Q., Hua, S., Li, T., and Jin, J. (2024). EEG-based affective computing in virtual reality with a balancing of the computational efficiency and recognition accuracy. <i>Comput. Hum. Behav.</i> 152:108085. doi: 10.1016/j.chb.2023.108085</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.chb.2023.108085" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Pei&#x00026;author=Q.+Shang&#x00026;author=S.+Hua&#x00026;author=T.+Li&#x00026;author=J.+Jin&#x00026;publication_year=2024&#x00026;title=EEG-based+affective+computing+in+virtual+reality+with+a+balancing+of+the+computational+efficiency+and+recognition+accuracy&#x00026;journal=Comput.+Hum.+Behav.&#x00026;volume=152&#x00026;pages=108085" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref72" id="ref72"></a>Perlovsky, L. (2012). Cognitive function, origin, and evolution of musical emotions. <i>Music. Sci.</i> 16, 185&#x2013;199. doi: 10.1177/1029864912448327</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1177/1029864912448327" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=L.+Perlovsky&#x00026;publication_year=2012&#x00026;title=Cognitive+function+origin+and+evolution+of+musical+emotions&#x00026;journal=Music.+Sci.&#x00026;volume=16&#x00026;pages=185-199" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref73" id="ref73"></a>Ramirez, R., Planas, J., Escude, N., Mercade, J., and Farriols, C. (2018). EEG-based analysis of the emotional effect of music therapy on palliative care cancer patients. <i>Front. Psychol.</i> 9:254. doi: 10.3389/fpsyg.2018.00254</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.3389/fpsyg.2018.00254" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+Ramirez&#x00026;author=J.+Planas&#x00026;author=N.+Escude&#x00026;author=J.+Mercade&#x00026;author=C.+Farriols&#x00026;publication_year=2018&#x00026;title=EEG-based+analysis+of+the+emotional+effect+of+music+therapy+on+palliative+care+cancer+patients&#x00026;journal=Front.+Psychol.&#x00026;volume=9&#x00026;pages=254" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref74" id="ref74"></a>Reisenzein, R. (1994). Pleasure-arousal theory and the intensity of emotions. <i>J. Pers. Soc. Psychol.</i> 67, 525&#x2013;539. doi: 10.1037/0022-3514.67.3.525</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1037/0022-3514.67.3.525" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+Reisenzein&#x00026;publication_year=1994&#x00026;title=Pleasure-arousal+theory+and+the+intensity+of+emotions&#x00026;journal=J.+Pers.+Soc.+Psychol.&#x00026;volume=67&#x00026;pages=525-539" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref75" id="ref75"></a>Rolls, E. T. (2015). Limbic systems for emotion and for memory, but no single limbic system. <i>Cortex</i> 62, 119&#x2013;157. doi: 10.1016/j.cortex.2013.12.005</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.cortex.2013.12.005" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+T.+Rolls&#x00026;publication_year=2015&#x00026;title=Limbic+systems+for+emotion+and+for+memory+but+no+single+limbic+system&#x00026;journal=Cortex&#x00026;volume=62&#x00026;pages=119-157" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref76" id="ref76"></a>Russell, J. A. (1980). A circumplex model of affect. <i>J. Pers. Soc. Psychol.</i> 39, 1161&#x2013;1178. doi: 10.1037/h0077714</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1037/h0077714" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+A.+Russell&#x00026;publication_year=1980&#x00026;title=A+circumplex+model+of+affect&#x00026;journal=J.+Pers.+Soc.+Psychol.&#x00026;volume=39&#x00026;pages=1161-1178" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref77" id="ref77"></a>Ruth, N., and Schramm, H. (2021). Effects of prosocial lyrics and musical production elements on emotions, thoughts and behavior. <i>Psychol. Music</i> 49, 759&#x2013;776. doi: 10.1177/0305735620902534</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1177/0305735620902534" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=N.+Ruth&#x00026;author=H.+Schramm&#x00026;publication_year=2021&#x00026;title=Effects+of+prosocial+lyrics+and+musical+production+elements+on+emotions+thoughts+and+behavior&#x00026;journal=Psychol.+Music&#x00026;volume=49&#x00026;pages=759-776" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref78" id="ref78"></a>Ryczkowska, A. (2022). Positive mood induction through music: the significance of listener age and musical timbre. <i>Psychol. Music</i> 50, 1961&#x2013;1975. doi: 10.1177/03057356221081164</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1177/03057356221081164" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Ryczkowska&#x00026;publication_year=2022&#x00026;title=Positive+mood+induction+through+music:+the+significance+of+listener+age+and+musical+timbre&#x00026;journal=Psychol.+Music&#x00026;volume=50&#x00026;pages=1961-1975" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref79" id="ref79"></a>Saganowski, S., Perz, B., Polak, A. G., and Kazienko, P. (2023). Emotion recognition for everyday life using physiological signals from wearables: a systematic literature review. <i>IEEE Trans. Affect. Comput.</i> 14, 1876&#x2013;1897. doi: 10.1109/TAFFC.2022.3176135</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TAFFC.2022.3176135" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Saganowski&#x00026;author=B.+Perz&#x00026;author=A.+G.+Polak&#x00026;author=P.+Kazienko&#x00026;publication_year=2023&#x00026;title=Emotion+recognition+for+everyday+life+using+physiological+signals+from+wearables:+a+systematic+literature+review&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=14&#x00026;pages=1876-1897" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref80" id="ref80"></a>Salakka, I., Pitk&#x000E4;niemi, A., Pentik&#x000E4;inen, E., Mikkonen, K., Saari, P., Toiviainen, P., et al. (2021). What makes music memorable? Relationships between acoustic musical features and music-evoked emotions and memories in older adults. <i>PLoS One</i> 16:e0251692. doi: 10.1371/journal.pone.0251692</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1371/journal.pone.0251692" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=I.+Salakka&#x00026;author=A.+Pitk&#x000E4;niemi&#x00026;author=E.+Pentik&#x000E4;inen&#x00026;author=K.+Mikkonen&#x00026;author=P.+Saari&#x00026;author=P.+Toiviainen&#x00026;publication_year=2021&#x00026;title=What+makes+music+memorable?+Relationships+between+acoustic+musical+features+and+music-evoked+emotions+and+memories+in+older+adults&#x00026;journal=PLoS+One&#x00026;volume=16&#x00026;pages=e0251692" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref81" id="ref81"></a>Sammler, D., Grigutsch, M., Fritz, T., and Koelsch, S. (2007). Music and emotion: electrophysiological correlates of the processing of pleasant and unpleasant music. <i>Psychophysiology</i> 44, 293&#x2013;304. doi: 10.1111/j.1469-8986.2007.00497.x</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1111/j.1469-8986.2007.00497.x" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Sammler&#x00026;author=M.+Grigutsch&#x00026;author=T.+Fritz&#x00026;author=S.+Koelsch&#x00026;publication_year=2007&#x00026;title=Music+and+emotion:+electrophysiological+correlates+of+the+processing+of+pleasant+and+unpleasant+music&#x00026;journal=Psychophysiology&#x00026;volume=44&#x00026;pages=293-304" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref82" id="ref82"></a>Sanyal, S., Nag, S., Banerjee, A., Sengupta, R., and Ghosh, D. (2019). Music of brain and music on brain: a novel EEG sonification approach. <i>Cogn. Neurodyn.</i> 13, 13&#x2013;31. doi: 10.1007/s11571-018-9502-4 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/30728868" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1007/s11571-018-9502-4" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Sanyal&#x00026;author=S.+Nag&#x00026;author=A.+Banerjee&#x00026;author=R.+Sengupta&#x00026;author=D.+Ghosh&#x00026;publication_year=2019&#x00026;title=Music+of+brain+and+music+on+brain:+a+novel+EEG+sonification+approach&#x00026;journal=Cogn.+Neurodyn.&#x00026;volume=13&#x00026;pages=13-31" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref83" id="ref83"></a>Sari, D. A. L., Kusumaningrum, T. D., and Kusumoputro, B. (2023). Non-linear EEG based emotional classification using k-nearest neighbor and weighted k-nearest neighbor with variation of features selection methods. <i>AIP Conf. Proc.</i> 2654:020004. doi: 10.1063/5.0116377</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1063/5.0116377" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+A.+L.+Sari&#x00026;author=T.+D.+Kusumaningrum&#x00026;author=B.+Kusumoputro&#x00026;publication_year=2023&#x00026;title=Non-linear+EEG+based+emotional+classification+using+k-nearest+neighbor+and+weighted+k-nearest+neighbor+with+variation+of+features+selection+methods&#x00026;journal=AIP+Conf.+Proc.&#x00026;volume=2654&#x00026;pages=020004" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref84" id="ref84"></a>Schmidt, L. A., and Trainor, L. J. (2001). Frontal brain electrical activity (EEG) distinguishes valence and intensity of musical emotions. <i>Cogn. Emot.</i> 15, 487&#x2013;500. doi: 10.1080/02699930126048</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1080/02699930126048" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=L.+A.+Schmidt&#x00026;author=L.+J.+Trainor&#x00026;publication_year=2001&#x00026;title=Frontal+brain+electrical+activity+(EEG)+distinguishes+valence+and+intensity+of+musical+emotions&#x00026;journal=Cogn.+Emot.&#x00026;volume=15&#x00026;pages=487-500" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref85" id="ref85"></a>Shu, L., Xie, J., Yang, M., Li, Z., Li, Z., Liao, D., et al. (2018). A review of emotion recognition using physiological signals. <i>Sensors</i> 18:2074. doi: 10.3390/s18072074 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/29958457" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s18072074" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=L.+Shu&#x00026;author=J.+Xie&#x00026;author=M.+Yang&#x00026;author=Z.+Li&#x00026;author=Z.+Li&#x00026;author=D.+Liao&#x00026;publication_year=2018&#x00026;title=A+review+of+emotion+recognition+using+physiological+signals&#x00026;journal=Sensors&#x00026;volume=18&#x00026;pages=2074" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref86" id="ref86"></a>Silverman, D. (1963). The rationale and history of the 10-20 system of the international federation. <i>Am. J. EEG Technol.</i> 3, 17&#x2013;22. doi: 10.1080/00029238.1963.11080602</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1080/00029238.1963.11080602" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Silverman&#x00026;publication_year=1963&#x00026;title=The+rationale+and+history+of+the+10-20+system+of+the+international+federation&#x00026;journal=Am.+J.+EEG+Technol.&#x00026;volume=3&#x00026;pages=17-22" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref87" id="ref87"></a>Soleymani, M., Caro, M. N., Schmidt, E. M., Sha, C.-Y., and Yang, Y.-H. (2013). 1000 songs for emotional analysis of music., in Proceedings of the 2nd ACM international workshop on crowdsourcing for multimedia, (Barcelona, Spain: ACM), 1&#x2013;6.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?author=M.+Soleymani&#x00026;author=M.+N.+Caro&#x00026;author=E.+M.+Schmidt&#x00026;author=C.-Y.+Sha&#x00026;author=Y.-H.+Yang&#x00026;publication_year=2013&#x00026;title=1000+songs+for+emotional+analysis+of+music&#x00026;pages=1-6" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref88" id="ref88"></a>Sonnemans, J., and Frijda, N. H. (1994). The structure of subjective emotional intensity. <i>Cogn. Emot.</i> 8, 329&#x2013;350. doi: 10.1080/02699939408408945</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1080/02699939408408945" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Sonnemans&#x00026;author=N.+H.+Frijda&#x00026;publication_year=1994&#x00026;title=The+structure+of+subjective+emotional+intensity&#x00026;journal=Cogn.+Emot.&#x00026;volume=8&#x00026;pages=329-350" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref89" id="ref89"></a>Stancin, I., Cifrek, M., and Jovic, A. (2021). A review of EEG signal features and their application in driver drowsiness detection systems. <i>Sensors</i> 21:3786. doi: 10.3390/s21113786 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/34070732" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s21113786" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=I.+Stancin&#x00026;author=M.+Cifrek&#x00026;author=A.+Jovic&#x00026;publication_year=2021&#x00026;title=A+review+of+EEG+signal+features+and+their+application+in+driver+drowsiness+detection+systems&#x00026;journal=Sensors&#x00026;volume=21&#x00026;pages=3786" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref90" id="ref90"></a>Steinberg, R., G&#x000FC;nther, W., Stiltz, I., and Rondot, P. (1992). EEG-mapping during music stimulation. <i>Psychomusicol. J. Res. Music Cogn.</i> 11, 157&#x2013;170. doi: 10.1037/h0094123</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1037/h0094123" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+Steinberg&#x00026;author=W.+G&#x000FC;nther&#x00026;author=I.+Stiltz&#x00026;author=P.+Rondot&#x00026;publication_year=1992&#x00026;title=EEG-mapping+during+music+stimulation&#x00026;journal=Psychomusicol.+J.+Res.+Music+Cogn.&#x00026;volume=11&#x00026;pages=157-170" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref91" id="ref91"></a>Tang, Z., Xia, D., Li, X., Wang, X., Ying, J., and Yang, H. (2023). Evaluation of the effect of music on idea generation using electrocardiography and electroencephalography signals. <i>Int. J. Technol. Des. Educ.</i> 33, 1607&#x2013;1625. doi: 10.1007/s10798-022-09782-x</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1007/s10798-022-09782-x" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Z.+Tang&#x00026;author=D.+Xia&#x00026;author=X.+Li&#x00026;author=X.+Wang&#x00026;author=J.+Ying&#x00026;author=H.+Yang&#x00026;publication_year=2023&#x00026;title=Evaluation+of+the+effect+of+music+on+idea+generation+using+electrocardiography+and+electroencephalography+signals&#x00026;journal=Int.+J.+Technol.+Des.+Educ.&#x00026;volume=33&#x00026;pages=1607-1625" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref92" id="ref92"></a>Taylor, D. B. (1981). Music in general hospital treatment from 1900 to 1950. <i>J. Music. Ther.</i> 18, 62&#x2013;73. doi: 10.1093/jmt/18.2.62 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/10298285" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1093/jmt/18.2.62" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+B.+Taylor&#x00026;publication_year=1981&#x00026;title=Music+in+general+hospital+treatment+from+1900+to+1950&#x00026;journal=J.+Music.+Ther.&#x00026;volume=18&#x00026;pages=62-73" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref93" id="ref93"></a>Tcherkassof, A., and Dupr&#x000E9;, D. (2021). The emotion&#x2013;facial expression link: evidence from human and automatic expression recognition. <i>Psychol. Res.</i> 85, 2954&#x2013;2969. doi: 10.1007/s00426-020-01448-4 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/33236175" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1007/s00426-020-01448-4" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Tcherkassof&#x00026;author=D.+Dupr&#x000E9;&#x00026;publication_year=2021&#x00026;title=The+emotion&#x2013;facial+expression+link:+evidence+from+human+and+automatic+expression+recognition&#x00026;journal=Psychol.+Res.&#x00026;volume=85&#x00026;pages=2954-2969" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref94" id="ref94"></a>Thaut, M. H., Francisco, G., and Hoemberg, V. (2021). Editorial: the clinical neuroscience of music: evidence based approaches and neurologic music therapy. <i>Front. Neurosci.</i> 15:740329. doi: 10.3389/fnins.2021.740329 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/34630025" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2021.740329" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+H.+Thaut&#x00026;author=G.+Francisco&#x00026;author=V.+Hoemberg&#x00026;publication_year=2021&#x00026;title=Editorial:+the+clinical+neuroscience+of+music:+evidence+based+approaches+and+neurologic+music+therapy&#x00026;journal=Front.+Neurosci.&#x00026;volume=15&#x00026;pages=740329" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref95" id="ref95"></a>Thayer, R. E., and McNally, R. J. (1992). The biopsychology of mood and arousal. <i>Cogn. Behav. Neurol.</i> 5:65.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?author=R.+E.+Thayer&#x00026;author=R.+J.+McNally&#x00026;publication_year=1992&#x00026;title=The+biopsychology+of+mood+and+arousal&#x00026;journal=Cogn.+Behav.+Neurol.&#x00026;volume=5&#x00026;pages=65" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref96" id="ref96"></a>Torres, E. P., Torres, E. A., Hern&#x000E1;ndez-&#x000C1;lvarez, M., and Yoo, S. G. (2020). EEG-based BCI emotion recognition: a survey. <i>Sensors</i> 20:5083. doi: 10.3390/s20185083 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/32906731" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s20185083" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+P.+Torres&#x00026;author=E.+A.+Torres&#x00026;author=M.+Hern&#x000E1;ndez-&#x000C1;lvarez&#x00026;author=S.+G.+Yoo&#x00026;publication_year=2020&#x00026;title=EEG-based+BCI+emotion+recognition:+a+survey&#x00026;journal=Sensors&#x00026;volume=20&#x00026;pages=5083" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref97" id="ref97"></a>Turnbull, D., Barrington, L., Torres, D., and Lanckriet, G. (2008). Semantic annotation and retrieval of music and sound effects. <i>IEEE Trans. Audio Speech Lang. Process.</i> 16, 467&#x2013;476. doi: 10.1109/TASL.2007.913750</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TASL.2007.913750" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Turnbull&#x00026;author=L.+Barrington&#x00026;author=D.+Torres&#x00026;author=G.+Lanckriet&#x00026;publication_year=2008&#x00026;title=Semantic+annotation+and+retrieval+of+music+and+sound+effects&#x00026;journal=IEEE+Trans.+Audio+Speech+Lang.+Process.&#x00026;volume=16&#x00026;pages=467-476" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref98" id="ref98"></a>Ueno, F., and Shimada, S. (2023). Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music. <i>Front. Hum. Neurosci.</i> 17:1225377. doi: 10.3389/fnhum.2023.1225377</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.3389/fnhum.2023.1225377" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=F.+Ueno&#x00026;author=S.+Shimada&#x00026;publication_year=2023&#x00026;title=Inter-subject+correlations+of+EEG+reflect+subjective+arousal+and+acoustic+features+of+music&#x00026;journal=Front.+Hum.+Neurosci.&#x00026;volume=17&#x00026;pages=1225377" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref99" id="ref99"></a>Vuust, P., Heggli, O. A., Friston, K. J., and Kringelbach, M. L. (2022). Music in the brain. <i>Nat. Rev. Neurosci.</i> 23, 287&#x2013;305. doi: 10.1038/s41583-022-00578-5</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1038/s41583-022-00578-5" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+Vuust&#x00026;author=O.+A.+Heggli&#x00026;author=K.+J.+Friston&#x00026;author=M.+L.+Kringelbach&#x00026;publication_year=2022&#x00026;title=Music+in+the+brain&#x00026;journal=Nat.+Rev.+Neurosci.&#x00026;volume=23&#x00026;pages=287-305" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref100" id="ref100"></a>Wang, X., Wei, Y., and Yang, D. (2022). Cross-cultural analysis of the correlation between musical elements and emotion. <i>Cognit. Comput. Syst.</i> 4, 116&#x2013;129. doi: 10.1049/ccs2.12032</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1049/ccs2.12032" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=X.+Wang&#x00026;author=Y.+Wei&#x00026;author=D.+Yang&#x00026;publication_year=2022&#x00026;title=Cross-cultural+analysis+of+the+correlation+between+musical+elements+and+emotion&#x00026;journal=Cognit.+Comput.+Syst.&#x00026;volume=4&#x00026;pages=116-129" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref18" id="ref18"></a>Weerakody, P. B., Wong, K. W., Wang, G., and Ela, W. (2021). A review of irregular time series data handling with gated recurrent neural networks. <i>Neurocomputing</i> 441, 161&#x2013;178. doi: 10.1016/j.neucom.2021.02.046</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.neucom.2021.02.046" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+B.+Weerakody&#x00026;author=K.+W.+Wong&#x00026;author=G.+Wang&#x00026;author=W.+Ela&#x00026;publication_year=2021&#x00026;title=A+review+of+irregular+time+series+data+handling+with+gated+recurrent+neural+networks&#x00026;journal=Neurocomputing&#x00026;volume=441&#x00026;pages=161-178" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref101" id="ref101"></a>Witkower, Z., Hill, A. K., Koster, J., and Tracy, J. L. (2021). Beyond face value: evidence for the universality of bodily expressions of emotion. <i>Affect. Sci.</i> 2, 221&#x2013;229. doi: 10.1007/s42761-021-00052-y </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/36059900" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1007/s42761-021-00052-y" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Z.+Witkower&#x00026;author=A.+K.+Hill&#x00026;author=J.+Koster&#x00026;author=J.+L.+Tracy&#x00026;publication_year=2021&#x00026;title=Beyond+face+value:+evidence+for+the+universality+of+bodily+expressions+of+emotion&#x00026;journal=Affect.+Sci.&#x00026;volume=2&#x00026;pages=221-229" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref102" id="ref102"></a>Wu, Q., Sun, L., Ding, N., and Yang, Y. (2024). Musical tension is affected by metrical structure dynamically and hierarchically. <i>Cogn. Neurodyn.</i> 18, 1955&#x2013;1976. doi: 10.1007/s11571-023-10058-w </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/39104669" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1007/s11571-023-10058-w" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Q.+Wu&#x00026;author=L.+Sun&#x00026;author=N.+Ding&#x00026;author=Y.+Yang&#x00026;publication_year=2024&#x00026;title=Musical+tension+is+affected+by+metrical+structure+dynamically+and+hierarchically&#x00026;journal=Cogn.+Neurodyn.&#x00026;volume=18&#x00026;pages=1955-1976" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref103" id="ref103"></a>Xu, G., Guo, W., and Wang, Y. (2023). Subject-independent EEG emotion recognition with hybrid spatio-temporal GRU-conv architecture. <i>Med. Biol. Eng. Comput.</i> 61, 61&#x2013;73. doi: 10.1007/s11517-022-02686-x </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/36322243" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1007/s11517-022-02686-x" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Xu&#x00026;author=W.+Guo&#x00026;author=Y.+Wang&#x00026;publication_year=2023&#x00026;title=Subject-independent+EEG+emotion+recognition+with+hybrid+spatio-temporal+GRU-conv+architecture&#x00026;journal=Med.+Biol.+Eng.+Comput.&#x00026;volume=61&#x00026;pages=61-73" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref104" id="ref104"></a>Xu, J., Hu, L., Qiao, R., Hu, Y., and Tian, Y. (2023). Music-emotion EEG coupling effects based on representational similarity. <i>J. Neurosci. Methods</i> 398:109959. doi: 10.1016/j.jneumeth.2023.109959 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/37661055" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.jneumeth.2023.109959" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Xu&#x00026;author=L.+Hu&#x00026;author=R.+Qiao&#x00026;author=Y.+Hu&#x00026;author=Y.+Tian&#x00026;publication_year=2023&#x00026;title=Music-emotion+EEG+coupling+effects+based+on+representational+similarity&#x00026;journal=J.+Neurosci.+Methods&#x00026;volume=398&#x00026;pages=109959" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref105" id="ref105"></a>Xu, J., Qian, W., Hu, L., Liao, G., and Tian, Y. (2024). EEG decoding for musical emotion with functional connectivity features. <i>Biomed. Signal Process. Control</i> 89:105744. doi: 10.1016/j.bspc.2023.105744</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.bspc.2023.105744" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Xu&#x00026;author=W.+Qian&#x00026;author=L.+Hu&#x00026;author=G.+Liao&#x00026;author=Y.+Tian&#x00026;publication_year=2024&#x00026;title=EEG+decoding+for+musical+emotion+with+functional+connectivity+features&#x00026;journal=Biomed.+Signal+Process.+Control&#x00026;volume=89&#x00026;pages=105744" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref106" id="ref106"></a>Yang, J. (2021). A novel music emotion recognition model using neural network technology. <i>Front. Psychol.</i> 12:760060. doi: 10.3389/fpsyg.2021.760060 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/34650499" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fpsyg.2021.760060" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+Yang&#x00026;publication_year=2021&#x00026;title=A+novel+music+emotion+recognition+model+using+neural+network+technology&#x00026;journal=Front.+Psychol.&#x00026;volume=12&#x00026;pages=760060" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref107" id="ref107"></a>Yang, H., Han, J., and Min, K. (2019). A multi-column CNN model for emotion recognition from EEG signals. <i>Sensors</i> 19:4736. doi: 10.3390/s19214736 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/31683608" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s19214736" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Yang&#x00026;author=J.+Han&#x00026;author=K.+Min&#x00026;publication_year=2019&#x00026;title=A+multi-column+CNN+model+for+emotion+recognition+from+EEG+signals&#x00026;journal=Sensors&#x00026;volume=19&#x00026;pages=4736" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref108" id="ref108"></a>Yang, K., Kim, H. M., and Zimmerman, J. (2020). Emotional branding on fashion brand websites: harnessing the pleasure-arousal-dominance (P-a-D) model. <i>J. Fash. Mark. Manag.</i> 24, 555&#x2013;570. doi: 10.1108/JFMM-03-2019-0055</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1108/JFMM-03-2019-0055" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=K.+Yang&#x00026;author=H.+M.+Kim&#x00026;author=J.+Zimmerman&#x00026;publication_year=2020&#x00026;title=Emotional+branding+on+fashion+brand+websites:+harnessing+the+pleasure-arousal-dominance+(P-a-D)+model&#x00026;journal=J.+Fash.+Mark.+Manag.&#x00026;volume=24&#x00026;pages=555-570" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref109" id="ref109"></a>Yang, W., Makita, K., Nakao, T., Kanayama, N., Machizawa, M. G., Sasaoka, T., et al. (2018). Affective auditory stimulus database: an expanded version of the international affective digitized sounds (IADS-E). <i>Behav. Res. Methods</i> 50, 1415&#x2013;1429. doi: 10.3758/s13428-018-1027-6 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/29520632" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3758/s13428-018-1027-6" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=W.+Yang&#x00026;author=K.+Makita&#x00026;author=T.+Nakao&#x00026;author=N.+Kanayama&#x00026;author=M.+G.+Machizawa&#x00026;author=T.+Sasaoka&#x00026;publication_year=2018&#x00026;title=Affective+auditory+stimulus+database:+an+expanded+version+of+the+international+affective+digitized+sounds+(IADS-E)&#x00026;journal=Behav.+Res.+Methods&#x00026;volume=50&#x00026;pages=1415-1429" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref110" id="ref110"></a>Yu, M., Xiao, S., Hua, M., Wang, H., Chen, X., Tian, F., et al. (2022). EEG-based emotion recognition in an immersive virtual reality environment: from local activity to brain network features. <i>Biomed. Signal Process. Control</i> 72:103349. doi: 10.1016/j.bspc.2021.103349</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.bspc.2021.103349" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Yu&#x00026;author=S.+Xiao&#x00026;author=M.+Hua&#x00026;author=H.+Wang&#x00026;author=X.+Chen&#x00026;author=F.+Tian&#x00026;publication_year=2022&#x00026;title=EEG-based+emotion+recognition+in+an+immersive+virtual+reality+environment:+from+local+activity+to+brain+network+features&#x00026;journal=Biomed.+Signal+Process.+Control&#x00026;volume=72&#x00026;pages=103349" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref111" id="ref111"></a>Zentner, M., Grandjean, D., and Scherer, K. R. (2008). Emotions evoked by the sound of music: characterization, classification, and measurement. <i>Emotion</i> 8, 494&#x2013;521. doi: 10.1037/1528-3542.8.4.494</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1037/1528-3542.8.4.494" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Zentner&#x00026;author=D.+Grandjean&#x00026;author=K.+R.+Scherer&#x00026;publication_year=2008&#x00026;title=Emotions+evoked+by+the+sound+of+music:+characterization+classification+and+measurement&#x00026;journal=Emotion&#x00026;volume=8&#x00026;pages=494-521" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref112" id="ref112"></a>Zhang, M., Ding, Y., Zhang, J., Jiang, X., Xu, N., Zhang, L., et al. (2022). Effect of group impromptu music therapy on emotional regulation and depressive symptoms of college students: a randomized controlled study. <i>Front. Psychol.</i> 13:851526. doi: 10.3389/fpsyg.2022.851526 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/35432107" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fpsyg.2022.851526" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Zhang&#x00026;author=Y.+Ding&#x00026;author=J.+Zhang&#x00026;author=X.+Jiang&#x00026;author=N.+Xu&#x00026;author=L.+Zhang&#x00026;publication_year=2022&#x00026;title=Effect+of+group+impromptu+music+therapy+on+emotional+regulation+and+depressive+symptoms+of+college+students:+a+randomized+controlled+study&#x00026;journal=Front.+Psychol.&#x00026;volume=13&#x00026;pages=851526" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref113" id="ref113"></a>Zhang, L., Xia, B., Wang, Y., Zhang, W., and Han, Y. (2023). A fine-grained approach for EEG-based emotion recognition using clustering and hybrid deep neural networks. <i>Electronics</i> 12:4717. doi: 10.3390/electronics12234717</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.3390/electronics12234717" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=L.+Zhang&#x00026;author=B.+Xia&#x00026;author=Y.+Wang&#x00026;author=W.+Zhang&#x00026;author=Y.+Han&#x00026;publication_year=2023&#x00026;title=A+fine-grained+approach+for+EEG-based+emotion+recognition+using+clustering+and+hybrid+deep+neural+networks&#x00026;journal=Electronics&#x00026;volume=12&#x00026;pages=4717" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref114" id="ref114"></a>Zheng, W.-L., and Lu, B.-L. (2015). Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks. <i>IEEE Trans. Auton. Ment. Dev.</i> 7, 162&#x2013;175. doi: 10.1109/TAMD.2015.2431497</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TAMD.2015.2431497" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=W.-L.+Zheng&#x00026;author=B.-L.+Lu&#x00026;publication_year=2015&#x00026;title=Investigating+critical+frequency+bands+and+channels+for+EEG-based+emotion+recognition+with+deep+neural+networks&#x00026;journal=IEEE+Trans.+Auton.+Ment.+Dev.&#x00026;volume=7&#x00026;pages=162-175" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a name="ref115" id="ref115"></a>Zhong, M., Yang, Q., Liu, Y., Zhen, B., Zhao, F., and Xie, B. (2023). EEG emotion recognition based on TQWT-features and hybrid convolutional recurrent neural network. <i>Biomed. Signal Process. Control</i> 79:104211. doi: 10.1016/j.bspc.2022.104211</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.bspc.2022.104211" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Zhong&#x00026;author=Q.+Yang&#x00026;author=Y.+Liu&#x00026;author=B.+Zhen&#x00026;author=F.+Zhao&#x00026;author=B.+Xie&#x00026;publication_year=2023&#x00026;title=EEG+emotion+recognition+based+on+TQWT-features+and+hybrid+convolutional+recurrent+neural+network&#x00026;journal=Biomed.+Signal+Process.+Control&#x00026;volume=79&#x00026;pages=104211" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref116" id="ref116"></a>Zhou, Y., and Lian, J. (2023). Identification of emotions evoked by music via spatial-temporal transformer in multi-channel EEG signals. <i>Front. Neurosci.</i> 17:1188696. doi: 10.3389/fnins.2023.1188696 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/37483354" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3389/fnins.2023.1188696" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Y.+Zhou&#x00026;author=J.+Lian&#x00026;publication_year=2023&#x00026;title=Identification+of+emotions+evoked+by+music+via+spatial-temporal+transformer+in+multi-channel+EEG+signals&#x00026;journal=Front.+Neurosci.&#x00026;volume=17&#x00026;pages=1188696" target="_blank">Google Scholar</a></p></div>
<div class="References">
<p class="ReferencesCopy1"><a name="ref117" id="ref117"></a>Zhou, T. H., Liang, W., Liu, H., Wang, L., Ryu, K. H., and Nam, K. W. (2022). EEG emotion recognition applied to the effect analysis of music on emotion changes in psychological healthcare. <i>Int. J. Environ. Res. Public Health</i> 20:378. doi: 10.3390/ijerph20010378 </p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/36612700" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/ijerph20010378" target="_blank">Crossref Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=T.+H.+Zhou&#x00026;author=W.+Liang&#x00026;author=H.+Liu&#x00026;author=L.+Wang&#x00026;author=K.+H.+Ryu&#x00026;author=K.+W.+Nam&#x00026;publication_year=2022&#x00026;title=EEG+emotion+recognition+applied+to+the+effect+analysis+of+music+on+emotion+changes+in+psychological+healthcare&#x00026;journal=Int.+J.+Environ.+Res.+Public+Health&#x00026;volume=20&#x00026;pages=378" target="_blank">Google Scholar</a></p></div>
</div> <div class="thinLineM20"></div> <div class="AbstractSummary">
<p><span>Keywords:</span> music-induced, emotion recognition, artificial intelligence, personalization, applications</p>
<p><span>Citation:</span> Su Y, Liu Y, Xiao Y, Ma J and Li D (2024) A review of artificial intelligence methods enabled music-evoked EEG emotion recognition and their applications. <i>Front. Neurosci</i>. 18:1400444. doi: 10.3389/fnins.2024.1400444</p>
<p class="timestamps"><span>Received:</span> 17 April 2024; <span>Accepted:</span> 14 August 2024;<br> <span>Published:</span> 04 September 2024.</p> <div>
<p>Edited by:</p> <a href="https://loop.frontiersin.org/people/29627/overview">Dan Zhang</a>, Tsinghua University, China</div> <div>
<p>Reviewed by:</p> <a href="https://loop.frontiersin.org/people/1583946/overview">Jun Jiang</a>, Shanghai Normal University, China<br> <a href="https://loop.frontiersin.org/people/2086612/overview">Wen Li</a>, Universiti Sains Malaysia, Malaysia</div>
<p><span>Copyright</span> &#x000A9; 2024 Su, Liu, Xiao, Ma and Li. This is an open-access article distributed under the terms of the <a rel="license" href="http://creativecommons.org/licenses/by/4.0/" target="_blank">Creative Commons Attribution License (CC BY)</a>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p>
<p><span>&#x0002A;Correspondence:</span> Dezhao Li, <a id="encmail">ZGxpYWVAY29ubmVjdC51c3QuaGs=</a></p> <div class="clear"></div> </div></div></div><p class="AbstractSummary__disclaimer"><span>Disclaimer: </span> All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. </p></div></div><!----></main><aside class="Layout__aside"><div class="ArticleDetails__wrapper"><div class="ArticleDetails__aside"><div class="ArticleDetails__aside__responsiveButtons"><div class="ActionsDropDown" id="FloatingButtonsEl"><button aria-label="Open dropdown" class="ActionsDropDown__button--type ActionsDropDown__button--icon ActionsDropDown__button--color ActionsDropDown__button" data-event="actionsDropDown-button-toggle"><span class="ActionsDropDown__button__label">Download article</span></button><div class="ActionsDropDown__menuWrapper"><!----><ul class="ActionsDropDown__menu"><!--[--><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/pdf" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-pdf">Download PDF</a></li><li><a href="http://www.readcube.com/articles/10.3389/fnins.2024.1400444" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-readCube">ReadCube</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/epub" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-epub">epub</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/xml" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-nlmXml">XML</a></li><!--]--></ul><button class="ActionsDropDown__mobileClose" aria-label="Close modal" data-event="actionsDropDown-button-close"></button></div></div><div class="ArticleDetails__aside__responsiveButtons__items"><span></span><div class="ArticleDetailsShare__responsive"><button class="ArticleDetailsShare__trigger" aria-label="Open share options"></button><div class="ArticleDetailsShare"><p class="ArticleDetailsShare__title">Share on</p><ul class="ArticleDetailsShare__list"><!--[--><li class="ArticleDetailsShare__item"><a href="https://www.twitter.com/share?url=https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full" target="_blank" class="ArticleDetailsShare__link--x ArticleDetailsShare__link" title="Share on X" aria-label="Share on X"></a></li><li class="ArticleDetailsShare__item"><a href="https://www.linkedin.com/share?url=https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full" target="_blank" class="ArticleDetailsShare__link--linkedin ArticleDetailsShare__link" title="Share on Linkedin" aria-label="Share on Linkedin"></a></li><li class="ArticleDetailsShare__item"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full" target="_blank" class="ArticleDetailsShare__link--facebook ArticleDetailsShare__link" title="Share on Facebook" aria-label="Share on Facebook"></a></li><!--]--></ul></div></div><div class="ActionsDropDown"><button aria-label="Open dropdown" class="ActionsDropDown__button--typeIconButton ActionsDropDown__button--iconQuote ActionsDropDown__button--color ActionsDropDown__button" data-event="actionsDropDown-button-toggle"><!----></button><div class="ActionsDropDown__menuWrapper"><div class="ActionsDropDown__mobileTitle">Export citation</div><ul class="ActionsDropDown__menu"><!--[--><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/endNote" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-endNote">EndNote</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/reference" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-referenceManager">Reference Manager</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/text" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-simpleTextFile">Simple Text file</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/bibTex" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-bibTex">BibTex</a></li><!--]--></ul><button class="ActionsDropDown__mobileClose" aria-label="Close modal" data-event="actionsDropDown-button-close"></button></div></div></div></div><div class="TotalViews"><div class="TotalViews__data"><div class="TotalViews__data__metrics"><div class="TotalViews__data__metrics__number">5,4K</div><div class="TotalViews__data__metrics__text"><div class="TotalViews__data__metrics__label">Total views</div></div></div><div class="TotalViews__data__metrics"><div class="TotalViews__data__metrics__number">1,3K</div><div class="TotalViews__data__metrics__text"><div class="TotalViews__data__metrics__label">Downloads</div></div></div><div class="TotalViews__data__metrics"><div class="TotalViews__data__metrics__number">6</div><div class="TotalViews__data__metrics__text"><div class="TotalViews__data__metrics__label">Citations</div></div></div><div class="ImpactMetricsInfoPopover"><button aria-label="Open impact metrics info" class="ImpactMetricsInfoPopover__button"></button><div class="ImpactMetricsInfoPopover__tooltip"><button aria-label="Close impact metrics info" class="ImpactMetricsInfoPopover__tooltip__closeButton"></button><div class="ImpactMetricsInfoPopover__tooltip__text"> Citation numbers are available from Dimensions </div></div></div></div><div class="TotalViews__viewImpactLink"><span class="Link__wrapper"><a class="Link Link--linkType Link--maincolor Link--medium Link--icon Link--chevronRight Link--right" aria-label="View article impact" href="http://loop-impact.frontiersin.org/impact/article/1400444#totalviews/views" target="_blank" data-event="customLink-link-a_viewArticleImpact"><span>View article impact</span></a></span></div><div class="TotalViews__altmetric"><div data-badge-popover="bottom" data-badge-type="donut" data-doi="10.3389/fnins.2024.1400444" data-condensed="true" data-link-target="new" class="altmetric-embed"></div><span class="Link__wrapper"><a class="Link Link--linkType Link--maincolor Link--medium Link--icon Link--chevronRight Link--right" aria-label="View altmetric score" href="https://www.altmetric.com/details/doi/10.3389/fnins.2024.1400444" target="_blank" data-event="customLink-link-a_viewAltmetricScore"><span>View altmetric score</span></a></span></div></div><div class="ArticleDetailsShare"><p class="ArticleDetailsShare__title">Share on</p><ul class="ArticleDetailsShare__list"><!--[--><li class="ArticleDetailsShare__item"><a href="https://www.twitter.com/share?url=https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full" target="_blank" class="ArticleDetailsShare__link--x ArticleDetailsShare__link" title="Share on X" aria-label="Share on X"></a></li><li class="ArticleDetailsShare__item"><a href="https://www.linkedin.com/share?url=https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full" target="_blank" class="ArticleDetailsShare__link--linkedin ArticleDetailsShare__link" title="Share on Linkedin" aria-label="Share on Linkedin"></a></li><li class="ArticleDetailsShare__item"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full" target="_blank" class="ArticleDetailsShare__link--facebook ArticleDetailsShare__link" title="Share on Facebook" aria-label="Share on Facebook"></a></li><!--]--></ul></div><div class="ArticleDetailsEditors"><div class="ArticleDetailsEditors__editors"><div class="ArticleDetailsEditors__title">Edited by</div><!--[--><a href="https://loop.frontiersin.org/people/29627/overview" data-event="editorInfo-a-danZhang" class="ArticleDetailsEditors__ediorInfo"><div class="Avatar Avatar--size-32 Avatar--text Avatar--grey"><!--[--><span class="notranslate">D</span><span class="notranslate">Z</span><!--]--></div><div class="ArticleDetailsEditors__ediorInfo__info"><div class="ArticleDetailsEditors__ediorInfo__name notranslate">Dan  Zhang</div><div class="ArticleDetailsEditors__ediorInfo__affiliation notranslate"></div></div></a><!--]--></div></div><div class="ArticleDetailsEditors"><div class="ArticleDetailsEditors__editors"><div class="ArticleDetailsEditors__title">Reviewed by</div><!--[--><a href="https://loop.frontiersin.org/people/1583946/overview" data-event="editorInfo-a-junJiang" class="ArticleDetailsEditors__ediorInfo"><div class="Avatar Avatar--size-32 Avatar--text Avatar--grey"><!--[--><span class="notranslate">J</span><span class="notranslate">J</span><!--]--></div><div class="ArticleDetailsEditors__ediorInfo__info"><div class="ArticleDetailsEditors__ediorInfo__name notranslate">Jun  Jiang</div><div class="ArticleDetailsEditors__ediorInfo__affiliation notranslate"></div></div></a><a href="https://loop.frontiersin.org/people/2086612/overview" data-event="editorInfo-a-wenLi" class="ArticleDetailsEditors__ediorInfo"><div class="Avatar Avatar--size-32 Avatar--text Avatar--grey"><!--[--><span class="notranslate">W</span><span class="notranslate">L</span><!--]--></div><div class="ArticleDetailsEditors__ediorInfo__info"><div class="ArticleDetailsEditors__ediorInfo__name notranslate">Wen  Li</div><div class="ArticleDetailsEditors__ediorInfo__affiliation notranslate"></div></div></a><!--]--></div></div><div class="ArticleDetailsGlossary ArticleDetailsGlossary--open"><button class="ArticleDetailsGlossary__header"><div class="ArticleDetailsGlossary__header__title">Table of contents</div><div class="ArticleDetailsGlossary__header__arrow"></div></button><div class="ArticleDetailsGlossary__content"><ul class="flyoutJournal"> <li><a href="#h1">Abstract</a></li> <li><a href="#h2">1 Introduction</a></li> <li><a href="#h3">2 EEG signal and emotions</a></li> <li><a href="#h4">3 Preprocessing and feature extraction of EEG signals</a></li> <li><a href="#h5">4 Emotion data source and modeling</a></li> <li><a href="#h6">5 Artificial intelligence algorithms for EEG emotion recognition</a></li> <li><a href="#h7">6 Application examples and analysis</a></li> <li><a href="#h8">7 Discussion and conclusions</a></li> <li><a href="#h9">Author contributions</a></li> <li><a href="#h10">Funding</a></li> <li><a href="#h11">Conflict of interest</a></li> <li><a href="#h12">Publisher&#x2019;s note</a></li> <li><a href="#h13">References</a></li> </ul></div></div><span></span><div class="ActionsDropDown"><button aria-label="Open dropdown" class="ActionsDropDown__button--typeOutline ActionsDropDown__button--iconQuote ActionsDropDown__button--color ActionsDropDown__button" data-event="actionsDropDown-button-toggle"><span class="ActionsDropDown__button__label">Export citation</span></button><div class="ActionsDropDown__menuWrapper"><!----><ul class="ActionsDropDown__menu"><!--[--><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/endNote" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-endNote">EndNote</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/reference" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-referenceManager">Reference Manager</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/text" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-simpleTextFile">Simple Text file</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/bibTex" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-bibTex">BibTex</a></li><!--]--></ul><button class="ActionsDropDown__mobileClose" aria-label="Close modal" data-event="actionsDropDown-button-close"></button></div></div><div class="CheckForUpdates"><button data-target="crossmark" class="CheckForUpdates__link" data-event="checkForUpdates-btn-openModal"><img class="CheckForUpdates__link__img" src="/ap-2024/images/crossmark.svg" alt="Crossmark icon"><div class="CheckForUpdates__link__text">Check for updates</div></button></div><div class="AnnouncementCard"><p class="AnnouncementCard__title">Frontiers&#39; impact</p><article class="CardA"><div class="CardA__wrapper CardA__wrapper--vertical"><figure class="FrontiersImage CardA__img"><picture class="FrontiersImage"><!--[--><source srcset="https://images-provider.frontiersin.org/api/ipx/w=440&amp;f=webp/https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-Impact-metrics-banner-for-article-pages.png" media="(max-width: 563px)"><source srcset="https://images-provider.frontiersin.org/api/ipx/s=320x400&amp;fit=outside&amp;f=webp/https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-Impact-metrics-banner-for-article-pages.png" media="(max-width: 1024px)"><source srcset="https://images-provider.frontiersin.org/api/ipx/s=268x280&amp;fit=outside&amp;f=webp/https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-Impact-metrics-banner-for-article-pages.png" media="(max-width: 1441px)"><source srcset="https://images-provider.frontiersin.org/api/ipx/s=366x408&amp;fit=outside&amp;f=webp/https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-Impact-metrics-banner-for-article-pages.png" media><source srcset="https://images-provider.frontiersin.org/api/ipx/s=366x408&amp;fit=outside&amp;f=jpg/https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-Impact-metrics-banner-for-article-pages.png" media><!--]--><img src="https://images-provider.frontiersin.org/api/ipx/s=366x408&amp;fit=outside&amp;f=jpg/https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-Impact-metrics-banner-for-article-pages.png" class="is-inside-mask" alt loading="eager"></picture><!----></figure><div class="CardA__info"><!--[--><h2 class="CardA__title">Articles published with Frontiers have received 12 million total citations</h2><p class="CardA__text">Your research is the real superpower - learn how we maximise its impact through our leading community journals</p><br><span class="Link__wrapper"><a class="Link Link--linkType Link--maincolor Link--small Link--icon Link--chevronRight Link--right" aria-label="Explore our impact metrics" href="https://www.frontiersin.org/about/impact" target="_self" data-event="customLink-linkType-a_exploreOurImpactMe"><span>Explore our impact metrics</span></a></span><!--]--></div></div></article></div><!----><!----></div><div><div class="Modal Modal--noFooter"><button aria-label="Close modal" class="Modal__overlay" data-event="modal-overlay-close"></button><div class="Modal__container"><div class="Modal__header"><p class="Modal__title">Supplementary Material</p><button aria-label="Close modal" class="Modal__close" data-event="modal-button-close"></button></div><div class="Modal__body"><!--[--><div class="SupplementalData"><ul class="SupplementalData__list"><!--[--><!--]--></ul></div><!--]--><!----></div><!----><!----></div></div></div><div><div class="FloatingButtons"><!----><div class="ActionsDropDown"><button aria-label="Open dropdown" class="ActionsDropDown__button--type ActionsDropDown__button--iconDownload ActionsDropDown__button--color ActionsDropDown__button" data-event="actionsDropDown-button-toggle"><span class="ActionsDropDown__button__label">Download article</span></button><div class="ActionsDropDown__menuWrapper"><div class="ActionsDropDown__mobileTitle">Download</div><ul class="ActionsDropDown__menu"><!--[--><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/pdf" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-pdf">Download PDF</a></li><li><a href="http://www.readcube.com/articles/10.3389/fnins.2024.1400444" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-readCube">ReadCube</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/epub" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-epub">epub</a></li><li><a href="/journals/neuroscience/articles/10.3389/fnins.2024.1400444/xml" target="_blank" rel="noopener noreferrer" class="ActionsDropDown__option" data-event="actionsDropDown-a-nlmXml">XML</a></li><!--]--></ul><button class="ActionsDropDown__mobileClose" aria-label="Close modal" data-event="actionsDropDown-button-close"></button></div></div><!----></div></div></div></aside></div><div class=""><!----></div><!--]--></div><!----><footer class="Footer"><div class="Footer__wrapper"><div class="Footer__sections"><ul class="Accordion"><!--[--><!--[--><li class="Accordion__item"><button class="Accordion__headline"><!----><!--[--><div class="Accordion__title">Guidelines</div><div class="Accordion__space"></div><!--]--><div class="Accordion__arrow"></div></button><div class="Accordion__content Accordion__content--fadeOut" style="height:0px;"><!--[--><ul><!--[--><li><a href="https://www.frontiersin.org/guidelines/author-guidelines" target="_self" data-event="footer-block_0-a_authorGuidelines">Author guidelines</a></li><li><a href="https://www.frontiersin.org/for-authors/author-services" target="_self" data-event="footer-block_0-a_servicesForAuthors">Services for authors</a></li><li><a href="https://www.frontiersin.org/guidelines/policies-and-publication-ethics" target="_self" data-event="footer-block_0-a_policiesAndPublicationE">Policies and publication ethics</a></li><li><a href="https://www.frontiersin.org/guidelines/editor-guidelines" target="_self" data-event="footer-block_0-a_editorGuidelines">Editor guidelines</a></li><li><a href="https://www.frontiersin.org/about/fee-policy" target="_self" data-event="footer-block_0-a_feePolicy">Fee policy</a></li><!--]--></ul><!--]--></div></li><li class="Accordion__item"><button class="Accordion__headline"><!----><!--[--><div class="Accordion__title">Explore</div><div class="Accordion__space"></div><!--]--><div class="Accordion__arrow"></div></button><div class="Accordion__content Accordion__content--fadeOut" style="height:0px;"><!--[--><ul><!--[--><li><a href="https://www.frontiersin.org/articles" target="_self" data-event="footer-block_1-a_articles">Articles</a></li><li><a href="https://www.frontiersin.org/research-topics" target="_self" data-event="footer-block_1-a_researchTopics">Research Topics </a></li><li><a href="https://www.frontiersin.org/journals" target="_self" data-event="footer-block_1-a_journals">Journals</a></li><li><a href="https://www.frontiersin.org/about/how-we-publish" target="_self" data-event="footer-block_1-a_howWePublish">How we publish</a></li><!--]--></ul><!--]--></div></li><li class="Accordion__item"><button class="Accordion__headline"><!----><!--[--><div class="Accordion__title">Outreach</div><div class="Accordion__space"></div><!--]--><div class="Accordion__arrow"></div></button><div class="Accordion__content Accordion__content--fadeOut" style="height:0px;"><!--[--><ul><!--[--><li><a href="https://forum.frontiersin.org/" target="_blank" data-event="footer-block_2-a_frontiersForum">Frontiers Forum </a></li><li><a href="https://policylabs.frontiersin.org/" target="_blank" data-event="footer-block_2-a_frontiersPolicyLabs">Frontiers Policy Labs </a></li><li><a href="https://kids.frontiersin.org/" target="_blank" data-event="footer-block_2-a_frontiersForYoungMinds">Frontiers for Young Minds</a></li><li><a href="https://www.frontiersin.org/about/frontiers-planet-prize" target="_self" data-event="footer-block_2-a_frontiersPlanetPrize">Frontiers Planet Prize</a></li><!--]--></ul><!--]--></div></li><li class="Accordion__item"><button class="Accordion__headline"><!----><!--[--><div class="Accordion__title">Connect</div><div class="Accordion__space"></div><!--]--><div class="Accordion__arrow"></div></button><div class="Accordion__content Accordion__content--fadeOut" style="height:0px;"><!--[--><ul><!--[--><li><a href="https://helpcenter.frontiersin.org" target="_blank" data-event="footer-block_3-a_helpCenter">Help center</a></li><li><a href="https://subscription-management.frontiersin.org/emails/preferences#block0" target="_blank" data-event="footer-block_3-a_emailsAndAlerts">Emails and alerts </a></li><li><a href="https://www.frontiersin.org/about/contact" target="_self" data-event="footer-block_3-a_contactUs">Contact us </a></li><li><a href="https://www.frontiersin.org/submission/submit" target="_self" data-event="footer-block_3-a_submit">Submit</a></li><li><a href="https://careers.frontiersin.org/" target="_blank" data-event="footer-block_3-a_careerOpportunities">Career opportunities</a></li><!--]--></ul><!--]--></div></li><!--]--><!--]--></ul><div class="Footer__socialLinks"><div class="Footer__socialLinks__title">Follow us</div><!--[--><span class="Link__wrapper"><a class="Link Link--linkType Link--grey Link--medium Link--icon Link--facebook Link--right" aria-label="Frontiers Facebook" href="https://www.facebook.com/Frontiersin" target="_blank" data-event="footer-facebook-a_true"><span></span></a></span><span class="Link__wrapper"><a class="Link Link--linkType Link--grey Link--medium Link--icon Link--twitter Link--right" aria-label="Frontiers Twitter" href="https://twitter.com/frontiersin" target="_blank" data-event="footer-twitter-a_true"><span></span></a></span><span class="Link__wrapper"><a class="Link Link--linkType Link--grey Link--medium Link--icon Link--linkedin Link--right" aria-label="Frontiers LinkedIn" href="https://www.linkedin.com/company/frontiers" target="_blank" data-event="footer-linkedIn-a_true"><span></span></a></span><span class="Link__wrapper"><a class="Link Link--linkType Link--grey Link--medium Link--icon Link--instagram Link--right" aria-label="Frontiers Instagram" href="https://www.instagram.com/frontiersin_" target="_blank" data-event="footer-instagram-a_true"><span></span></a></span><!--]--></div></div><div class="Footer__copyright"><div><span>© 2025 Frontiers Media S.A. All rights reserved</span></div><div><a href="https://www.frontiersin.org/legal/privacy-policy" target="_blank">Privacy policy</a><span> | </span><a href="https://www.frontiersin.org/legal/terms-and-conditions" target="_blank">Terms and conditions</a></div></div></div></footer><div class="SnackbarWrapper"><div class="SnackbarManager"><!--[--><!--]--></div></div></div><noscript><iframe
            src="https://tag-manager.frontiersin.org/ns.html?id=GTM-M322FV2&gtm_auth=owVbWxfaJr21yQv1fe1cAQ&gtm_preview=env-1&gtm_cookies_win=x"
            height="0"
            width="0"
            style="display: none; visibility: hidden">
          </iframe></noscript><!--]--></div><div id="teleports"></div><script type="application/json" id="__NUXT_DATA__" data-ssr="true">[["ShallowReactive",1],{"data":2,"state":4,"once":10,"_errors":11,"serverRendered":13,"path":14,"pinia":15},["ShallowReactive",3],{},["Reactive",5],{"$ssite-config":6},{"env":7,"name":8,"url":9},"production","Frontiers articles","https://article-pages-2024.frontiersin.org/",["Set"],["ShallowReactive",12],{},true,"/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full",["Reactive",16],{"main":17,"user":508,"article":509,"articleHub":753,"mainHeader":757},{"ibar":18,"footer":272,"newsletterComponent":-1,"snackbarItem":354,"toggleShowSnackbar":355,"contentfulJournal":356,"graphJournal":409,"settingsFeaturesSwitchers":413,"templateToggleBanner":414,"tenantConfig":474},{"tenantLogo":19,"journalLogo":19,"aboutUs":20,"submitUrl":113,"showSubmitButton":13,"journal":114,"sectionTerm":203,"aboutJournal":204,"mainLinks":253,"journalLinks":260,"helpCenterLink":269},"",[21,38,47,71,87],{"title":22,"links":23},"Who we are",[24,29,32,35],{"text":25,"url":26,"target":27,"ariaLabel":28},"Mission and values","https://www.frontiersin.org/about/mission","_self",null,{"text":30,"url":31,"target":27,"ariaLabel":28},"History","https://www.frontiersin.org/about/history",{"text":33,"url":34,"target":27,"ariaLabel":28},"Leadership","https://www.frontiersin.org/about/leadership",{"text":36,"url":37,"target":27,"ariaLabel":28},"Awards","https://www.frontiersin.org/about/awards",{"title":39,"links":40},"Impact and progress",[41,44],{"text":42,"url":43,"target":27,"ariaLabel":28},"Frontiers' impact","https://www.frontiersin.org/about/impact",{"text":45,"url":46,"target":27,"ariaLabel":28},"Our annual reports","https://www.frontiersin.org/about/annual-reports",{"title":48,"links":49},"Publishing model",[50,53,56,59,62,65,68],{"text":51,"url":52,"target":27,"ariaLabel":28},"How we publish","https://www.frontiersin.org/about/how-we-publish",{"text":54,"url":55,"target":27,"ariaLabel":28},"Open access","https://www.frontiersin.org/about/open-access",{"text":57,"url":58,"target":27,"ariaLabel":28},"Peer review","https://www.frontiersin.org/about/peer-review",{"text":60,"url":61,"target":27,"ariaLabel":28},"Research integrity","https://www.frontiersin.org/about/research-integrity",{"text":63,"url":64,"target":27,"ariaLabel":28},"Research Topics","https://www.frontiersin.org/about/research-topics",{"text":66,"url":67,"target":27,"ariaLabel":28},"FAIR² Data Management","https://www.frontiersin.org/about/fair-data-management",{"text":69,"url":70,"target":27,"ariaLabel":28},"Fee policy","https://www.frontiersin.org/about/fee-policy",{"title":72,"links":73},"Services",[74,78,81,84],{"text":75,"url":76,"target":77,"ariaLabel":28},"Societies","https://publishingpartnerships.frontiersin.org/","_blank",{"text":79,"url":80,"target":27,"ariaLabel":28},"National consortia","https://www.frontiersin.org/open-access-agreements/consortia",{"text":82,"url":83,"target":27,"ariaLabel":28},"Institutional partnerships","https://www.frontiersin.org/about/open-access-agreements",{"text":85,"url":86,"target":27,"ariaLabel":28},"Collaborators","https://www.frontiersin.org/about/collaborators",{"title":88,"links":89},"More from Frontiers",[90,94,97,101,105,109],{"text":91,"url":92,"target":77,"ariaLabel":93},"Frontiers Forum","https://forum.frontiersin.org/","this link will take you to the Frontiers Forum website",{"text":95,"url":96,"target":27,"ariaLabel":28},"Frontiers Planet Prize","https://www.frontiersin.org/about/frontiers-planet-prize",{"text":98,"url":99,"target":77,"ariaLabel":100},"Press office","https://pressoffice.frontiersin.org/","this link will take you to the Frontiers press office website",{"text":102,"url":103,"target":27,"ariaLabel":104},"Sustainability","https://www.frontiersin.org/about/sustainability","link to information about Frontiers' sustainability",{"text":106,"url":107,"target":77,"ariaLabel":108},"Career opportunities","https://careers.frontiersin.org/","this link will take you to the Frontiers careers website",{"text":110,"url":111,"target":27,"ariaLabel":112},"Contact us","https://www.frontiersin.org/about/contact","this link will take you to the help pages to contact our support team","https://www.frontiersin.org/submission/submit?domainid=1&fieldid=55&specialtyid=0&entitytype=2&entityid=1",{"id":115,"name":116,"slug":117,"sections":118},1,"Frontiers in Neuroscience","neuroscience",[119,123,127,131,135,139,143,147,151,155,159,163,167,171,175,179,183,187,191,195,199],{"id":120,"name":121,"slug":122},65,"Auditory Cognitive Neuroscience","auditory-cognitive-neuroscience",{"id":124,"name":125,"slug":126},157,"Autonomic Neuroscience","autonomic-neuroscience",{"id":128,"name":129,"slug":130},600,"Brain Imaging Methods","brain-imaging-methods",{"id":132,"name":133,"slug":134},33,"Decision Neuroscience","decision-neuroscience",{"id":136,"name":137,"slug":138},2416,"Gut-Brain Axis","gut-brain-axis",{"id":140,"name":141,"slug":142},1206,"Neural Technology","neural-technology",{"id":144,"name":145,"slug":146},73,"Neurodegeneration","neurodegeneration",{"id":148,"name":149,"slug":150},1944,"Neurodevelopment","neurodevelopment",{"id":152,"name":153,"slug":154},113,"Neuroendocrine Science","neuroendocrine-science",{"id":156,"name":157,"slug":158},818,"Neuroenergetics and Brain Health","neuroenergetics-and-brain-health",{"id":160,"name":161,"slug":162},25,"Neurogenesis","neurogenesis",{"id":164,"name":165,"slug":166},19,"Neurogenomics","neurogenomics",{"id":168,"name":169,"slug":170},31,"Neuromorphic Engineering","neuromorphic-engineering",{"id":172,"name":173,"slug":174},26,"Neuropharmacology","neuropharmacology",{"id":176,"name":177,"slug":178},23,"Neuroprosthetics","neuroprosthetics",{"id":180,"name":181,"slug":182},3022,"Neuroscience Methods and Techniques","neuroscience-methods-and-techniques",{"id":184,"name":185,"slug":186},41,"Perception Science","perception-science",{"id":188,"name":189,"slug":190},1409,"Sleep and Circadian Rhythms","sleep-and-circadian-rhythms",{"id":192,"name":193,"slug":194},57,"Social and Evolutionary Neuroscience","social-and-evolutionary-neuroscience",{"id":196,"name":197,"slug":198},2450,"Translational Neuroscience","translational-neuroscience",{"id":200,"name":201,"slug":202},2411,"Visual Neuroscience","visual-neuroscience","Sections",[205,229],{"title":206,"links":207},"Scope",[208,211,214,217,220,223,226],{"text":209,"url":210,"target":27,"ariaLabel":28},"Field chief editors","https://www.frontiersin.org/journals/neuroscience/about#about-editors",{"text":212,"url":213,"target":27,"ariaLabel":28},"Mission & scope","https://www.frontiersin.org/journals/neuroscience/about#about-scope",{"text":215,"url":216,"target":27,"ariaLabel":28},"Facts","https://www.frontiersin.org/journals/neuroscience/about#about-facts",{"text":218,"url":219,"target":27,"ariaLabel":28},"Journal sections","https://www.frontiersin.org/journals/neuroscience/about#about-submission",{"text":221,"url":222,"target":27,"ariaLabel":28},"Open access statement","https://www.frontiersin.org/journals/neuroscience/about#about-open",{"text":224,"url":225,"target":27,"ariaLabel":28},"Copyright statement","https://www.frontiersin.org/journals/neuroscience/about#copyright-statement",{"text":227,"url":228,"target":27,"ariaLabel":28},"Quality","https://www.frontiersin.org/journals/neuroscience/about#about-quality",{"title":230,"links":231},"For authors",[232,235,238,241,244,247,250],{"text":233,"url":234,"target":27,"ariaLabel":28},"Why submit?","https://www.frontiersin.org/journals/neuroscience/for-authors/why-submit",{"text":236,"url":237,"target":27,"ariaLabel":28},"Article types","https://www.frontiersin.org/journals/neuroscience/for-authors/article-types",{"text":239,"url":240,"target":27,"ariaLabel":28},"Author guidelines","https://www.frontiersin.org/journals/neuroscience/for-authors/author-guidelines",{"text":242,"url":243,"target":27,"ariaLabel":28},"Editor guidelines","https://www.frontiersin.org/journals/neuroscience/for-authors/editor-guidelines",{"text":245,"url":246,"target":27,"ariaLabel":28},"Publishing fees","https://www.frontiersin.org/journals/neuroscience/for-authors/publishing-fees",{"text":248,"url":249,"target":27,"ariaLabel":28},"Submission checklist","https://www.frontiersin.org/journals/neuroscience/for-authors/submission-checklist",{"text":251,"url":252,"target":27,"ariaLabel":28},"Contact editorial office","https://www.frontiersin.org/journals/neuroscience/for-authors/contact-editorial-office",[254,257],{"text":255,"url":256,"target":27,"ariaLabel":28},"All journals","https://www.frontiersin.org/journals",{"text":258,"url":259,"target":27,"ariaLabel":28},"All articles","https://www.frontiersin.org/articles",[261,264,266],{"text":262,"url":263,"target":27,"ariaLabel":28},"Articles","articles",{"text":63,"url":265,"target":27,"ariaLabel":28},"research-topics",{"text":267,"url":268,"target":27,"ariaLabel":28},"Editorial board","editors",{"text":270,"url":271,"target":77,"ariaLabel":270},"Help center","https://helpcenter.frontiersin.org",{"blocks":273,"socialLinks":327,"copyright":351,"termsAndConditionsUrl":352,"privacyPolicyUrl":353},[274,288,298,312],{"title":275,"links":276},"Guidelines",[277,279,282,285,287],{"text":239,"url":278,"target":27,"ariaLabel":28},"https://www.frontiersin.org/guidelines/author-guidelines",{"text":280,"url":281,"target":27,"ariaLabel":28},"Services for authors","https://www.frontiersin.org/for-authors/author-services",{"text":283,"url":284,"target":27,"ariaLabel":28},"Policies and publication ethics","https://www.frontiersin.org/guidelines/policies-and-publication-ethics",{"text":242,"url":286,"target":27,"ariaLabel":28},"https://www.frontiersin.org/guidelines/editor-guidelines",{"text":69,"url":70,"target":27,"ariaLabel":28},{"title":289,"links":290},"Explore",[291,292,295,297],{"text":262,"url":259,"target":27,"ariaLabel":28},{"text":293,"url":294,"target":27,"ariaLabel":28},"Research Topics ","https://www.frontiersin.org/research-topics",{"text":296,"url":256,"target":27,"ariaLabel":28},"Journals",{"text":51,"url":52,"target":27,"ariaLabel":28},{"title":299,"links":300},"Outreach",[301,304,307,311],{"text":302,"url":92,"target":77,"ariaLabel":303},"Frontiers Forum ","Frontiers Forum website",{"text":305,"url":306,"target":77,"ariaLabel":28},"Frontiers Policy Labs ","https://policylabs.frontiersin.org/",{"text":308,"url":309,"target":77,"ariaLabel":310},"Frontiers for Young Minds","https://kids.frontiersin.org/","Frontiers for Young Minds journal",{"text":95,"url":96,"target":27,"ariaLabel":28},{"title":313,"links":314},"Connect",[315,316,320,323,326],{"text":270,"url":271,"target":77,"ariaLabel":270},{"text":317,"url":318,"target":77,"ariaLabel":319},"Emails and alerts ","https://subscription-management.frontiersin.org/emails/preferences#block0","Subscribe to Frontiers emails",{"text":321,"url":111,"target":27,"ariaLabel":322},"Contact us ","Subscribe to newsletter",{"text":324,"url":325,"target":27,"ariaLabel":28},"Submit","https://www.frontiersin.org/submission/submit",{"text":106,"url":107,"target":77,"ariaLabel":28},[328,336,341,346],{"link":329,"type":332,"color":333,"icon":334,"size":335,"hiddenText":13},{"text":330,"url":331,"target":77,"ariaLabel":330},"Frontiers Facebook","https://www.facebook.com/Frontiersin","Link","Grey","Facebook","Medium",{"link":337,"type":332,"color":333,"icon":340,"size":335,"hiddenText":13},{"text":338,"url":339,"target":77,"ariaLabel":28},"Frontiers Twitter","https://twitter.com/frontiersin","Twitter",{"link":342,"type":332,"color":333,"icon":345,"size":335,"hiddenText":13},{"text":343,"url":344,"target":77,"ariaLabel":28},"Frontiers LinkedIn","https://www.linkedin.com/company/frontiers","LinkedIn",{"link":347,"type":332,"color":333,"icon":350,"size":335,"hiddenText":13},{"text":348,"url":349,"target":77,"ariaLabel":28},"Frontiers Instagram","https://www.instagram.com/frontiersin_","Instagram","Frontiers Media S.A. All rights reserved","https://www.frontiersin.org/legal/terms-and-conditions","https://www.frontiersin.org/legal/privacy-policy",{},false,{"__typename":357,"identifier":115,"name":116,"slug":117,"banner":358,"description":402,"mission":403,"palette":404,"impactFactor":405,"citeScore":406,"citations":407,"showTagline":28,"twitter":408},"Journal",[359],{"id":360,"src":361,"name":362,"type":363,"width":364,"height":365,"idHash":366,"archive":367,"brandId":368,"limited":367,"fileSize":369,"isPublic":115,"original":370,"copyright":19,"extension":371,"thumbnails":373,"dateCreated":381,"description":19,"orientation":382,"userCreated":383,"watermarked":367,"dateModified":381,"datePublished":384,"ecsArchiveFiles":385,"propertyOptions":386,"property_Channel":391,"property_Sub-Type":393,"property_Asset_Type":395,"activeOriginalFocusPoint":397,"property_Office_Department":400},"27A044CC-44D1-4E5E-A886893A3FE656F4","https://d2csxpduxe849s.cloudfront.net/media/E32629C6-9347-4F84-81FEAEF7BFA342B3/27A044CC-44D1-4E5E-A886893A3FE656F4/webimage-731DCAD7-7D53-4C7A-ABED6B270359A65F.png","FNINS_Main Visual_Purple_Website","image",6788,4865,"18e4eb8ad56b508b",0,"22C10171-81B3-4DA6-99342F272A32E8BB",12200641,"https://brand.frontiersin.org/m/18e4eb8ad56b508b/original/FNINS_Main-Visual_Purple_Website.jpg",[372],"jpg",{"mini":374,"thul":375,"webimage":361,"Guidelines":376,"WebsiteJpg_XL":377,"WebsiteWebP_L":378,"WebsiteWebP_M":379,"WebsiteWebP_XL":380},"https://d2csxpduxe849s.cloudfront.net/media/E32629C6-9347-4F84-81FEAEF7BFA342B3/27A044CC-44D1-4E5E-A886893A3FE656F4/mini-64E6EA95-B7B1-4005-AB72061E18A6F830.png","https://d2csxpduxe849s.cloudfront.net/media/E32629C6-9347-4F84-81FEAEF7BFA342B3/27A044CC-44D1-4E5E-A886893A3FE656F4/thul-38432C11-11F0-4DA5-B8C0EBF0A1603722.png","https://d2csxpduxe849s.cloudfront.net/media/E32629C6-9347-4F84-81FEAEF7BFA342B3/27A044CC-44D1-4E5E-A886893A3FE656F4/A72D78D9-73CD-4C75-910FB7C0D04825C0/Guidelines-FNINS_Main Visual_Purple_Website.png","https://d2csxpduxe849s.cloudfront.net/media/E32629C6-9347-4F84-81FEAEF7BFA342B3/27A044CC-44D1-4E5E-A886893A3FE656F4/A72D78D9-73CD-4C75-910FB7C0D04825C0/WebsiteJpg_XL-FNINS_Main Visual_Purple_Website.jpg","https://d2csxpduxe849s.cloudfront.net/media/E32629C6-9347-4F84-81FEAEF7BFA342B3/27A044CC-44D1-4E5E-A886893A3FE656F4/A72D78D9-73CD-4C75-910FB7C0D04825C0/WebsiteWebP_L-FNINS_Main Visual_Purple_Website.webp","https://d2csxpduxe849s.cloudfront.net/media/E32629C6-9347-4F84-81FEAEF7BFA342B3/27A044CC-44D1-4E5E-A886893A3FE656F4/A72D78D9-73CD-4C75-910FB7C0D04825C0/WebsiteWebP_M-FNINS_Main Visual_Purple_Website.webp","https://d2csxpduxe849s.cloudfront.net/media/E32629C6-9347-4F84-81FEAEF7BFA342B3/27A044CC-44D1-4E5E-A886893A3FE656F4/A72D78D9-73CD-4C75-910FB7C0D04825C0/WebsiteWebP_XL-FNINS_Main Visual_Purple_Website.webp","2022-06-27T10:00:29Z","landscape","Caroline Sutter","2022-06-27T09:27:09Z",[],[387,388,389,390],"414FB2D4-2283-43FD-BE14E534ECA67928","6C18119B-14BD-4951-B437696F4357BD33","7C692885-DB25-4858-B1FB4FF47B241E9B","D88C0047-EC30-4506-A7DF28A4D765E1CF",[392],"frontiersin_org",[394],"Main_Visual",[396],"Photography",{"x":398,"y":399},3394,2433,[401],"Publishing","Part of the most cited neuroscience journal series which explores the brain - from the new eras of causation and anatomical neurosciences to neuroeconomics and neuroenergetics.","\u003Cp>Frontiers in Neuroscience is a leading multidisciplinary journal that publishes research across a wide range of spectrum of specialities and disciplines in the field of neuroscience.\u003C/p>\n\n\u003Cp>Led by Field Chief Editor, Professor Idan Segev (Hebrew University of Jerusalem, Israel) and indexed in PubMed Central (PMC), Web of Science (SCIE), and Scopus among others, the journal provides a comprehensive understanding of brain functions, from genes to behavior, and aims to tighten the links between various domains of neuroscience and advance conceptual and technological developments in the field.\u003C/p>\n\n\u003Cp>Topics of interest include, but are not limited to:\u003C/p>\n\n\u003Cul>\n  \u003Cli>autonomic function and brain energy homeostasis\u003C/li>\n  \u003Cli>neural development and degeneration\u003C/li>\n  \u003Cli>neuroengineering including neuromorphic engineering and neuroprosthetics\u003C/li>\n  \u003Cli>novel brain imaging methods from the subcellular level (e.g. genetically encoded voltage- and ions indicators) to the whole brain level (e.g. large-scale micro-electrode arrays and direct imaging of neuronal activity at the cellular level by fMRI)\u003C/li>\n  \u003Cli>sensory perception and cognition.\u003C/li>\n  \u003Cli>translational neuroscience\u003C/li>\n\u003C/ul>\n\n\u003Cp>Studies on the convergence of novel molecular and optical techniques, and developments in anatomical methods, both at the whole-brain level (“connectome”) and the local circuit and synaptic level (“connectomics”) are of particular interest.  Approaches that allow us to link the structure of local circuits at specific brain regions to function are encouraged.\u003C/p>\n\n\u003Cp>The journal also welcomes submissions which support and advance the UN’s Sustainable Development Goals (SDGs), notably SDG 3: to ensure healthy lives and promote well-being for all at all ages. Studies that propose ways to promote mental health and prevent neurodegenerative diseases are of particular interest.\u003C/p>\n\n\u003Cp>Studies that focus on clinical trials, medical treatments, or the application of medical techniques without neuroscientific foundations are not within the scope of the journal. Similarly, studies that primarily investigate the effects of traditional medicine on health conditions, without exploring the underlying neurological mechanisms, fall outside of the journal scope.\u003C/p>\n\n\u003Cp>Frontiers’ journals require that manuscripts primarily comprising computational studies of public data, must include appropriate validation. Please refer to the \u003Ca href=\"https://www.frontiersin.org/guidelines/policies-and-publication-ethics#standards-for-research-methodology:~:text=complaints%20and%20allegations.-,Standards%20for%20research%20methodology,-Experiments\">Frontiers Standards for research methodology policy\u003C/a>, for more information. Manuscripts not adhering to these standards will not be considered.\u003Cp>\n\n\u003Cp>Frontiers in Neuroscience is committed to advancing developments in the field by allowing unrestricted access to research articles and communicating scientific knowledge to researchers and the public alike, to enhance scientific understanding and repairing the brain.\u003C/p>","purple","4.3","6.8","381571","@FrontNeurosci",{"id":115,"name":116,"slug":117,"abbreviation":410,"isOnline":13,"isOpenForSubmissions":13,"citeScore":411,"impactFactor":412},"fnins",6.6,3.2,{"displayTitlePillLabels":13,"displayRelatedArticlesBox":13,"showEditors":13,"showReviewers":13,"showLoopImpactLink":13,"enableFigshare":355,"useXmlImages":13},{"isPublic":13,"allowCompanyUsers":355,"whiteListEmails":415,"enableAllJournals":13,"whiteListJournals":437},[416,417,418,419,420,421,422,423,424,421,425,426,427,428,429,430,431,432,433,434,435,436],"Y2FybG9zLmxvcmNhQGZyb250aWVyc2luLm9yZw==","ZGFuaWVsLmx1ekBmcm9udGllcnNpbi5vcmc=","bGF1cmEudGVuYUBmcm9udGllcnNpbi5vcmc=","cmFmYWVsLnJpYW5jaG9AZnJvbnRpZXJzaW4ub3Jn","amFtZXMuc21hbGxib25lQGZyb250aWVyc2luLm9yZw==","ZnJhbi5tb3Jlbm9AZnJvbnRpZXJzaW4ub3Jn","c2FtdWVsLmZlcm5hbmRlekBmcm9udGllcnNpbi5vcmc=","YmFyYmFyYS5jYXJjYW5naXVAZnJvbnRpZXJzaW4ub3Jn","aXZhbi5zYW50YW1hcmlhQGZyb250aWVyc2luLm9yZw==","aHVlLnRyYW5AZnJvbnRpZXJzaW4ub3Jn","Z29uY2Fsby52YXJnYXNAZnJvbnRpZXJzaW4ub3Jn","bHVjaWUuc2VubkBmcm9udGllcnNpbi5vcmc=","bG9ybi5mcmFzZXJAZnJvbnRpZXJzaW4ub3Jn","ZWxzYS5jYXJyb25AZnJvbnRpZXJzaW4ub3Jn","bWFhcnRlbi52YW5kaWpja0Bmcm9udGllcnNpbi5vcmc=","YmluZGh1LmtyaXNobmFuQGZyb250aWVyc2luLm9yZw==","bWF0dGhldy5hdHR3YXRlcnNAZnJvbnRpZXJzaW4ub3Jn","Z2l1bGlhLnZhbHNlY2NoaUBmcm9udGllcnNpbi5vcmc=","ZmFiaWFuLmRlbGxhbW9ydGVAZnJvbnRpZXJzaW4ub3Jn","dmlqYXlhbi5wcEBwaXRzb2x1dGlvbnMuY29t","Z3VpbGxhdW1lLnNldXJhdEBmcm9udGllcnNpbi5vcmc=",[438,439,440,441,442,443,115,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473],2232,1729,2357,2456,2176,2333,1843,602,106,616,310,657,3123,1468,2137,628,1566,2726,2086,1490,451,141,1335,2655,1238,604,698,1547,176,1440,403,1239,755,2136,609,1534,{"spaceId":115,"name":115,"availableJournalPages":475,"announcement":478},[263,268,265,476,477],"volumes","about",{"__typename":479,"sys":480,"preHeader":42,"title":482,"description":483,"image":484,"link":506},"Announcement",{"id":481},"5ITtnTtQgAzPPGH6rX0AlM","Articles published with Frontiers have received 12 million total citations","Your research is the real superpower - learn how we maximise its impact through our leading community journals",[485],{"archive":367,"brandId":368,"copyright":28,"dateCreated":486,"dateModified":487,"datePublished":488,"description":28,"extension":489,"fileSize":491,"height":492,"id":493,"isPublic":367,"limited":367,"name":494,"orientation":382,"original":28,"thumbnails":495,"type":363,"watermarked":367,"width":502,"videoPreviewURLs":503,"tags":504,"textMetaproperties":505,"src":496},"2025-06-18T12:58:01Z","2025-06-18T14:15:34Z","2025-06-18T12:57:32Z",[490],"png",1365026,920,"7C5269C4-3C83-4591-951A74D15B95DAEA","Impact metrics banner for article pages",{"webimage":496,"thul":497,"mini":498,"WebsiteWebP_L":499,"WebsiteWebP_M":500,"Guidelines":501},"https://brand.frontiersin.org/m/59ee6275cb9a849c/webimage-Impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/thul-Impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/m/59ee6275cb9a849c/mini-Impact-metrics-banner-for-article-pages.png","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/WebsiteWebP_L/Impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/WebsiteWebP_M/Impact-metrics-banner-for-article-pages.webp","https://brand.frontiersin.org/asset/7c5269c4-3c83-4591-951a-74d15b95daea/Guidelines/Impact-metrics-banner-for-article-pages.png",1600,[],[],[],{"text":507,"url":43,"target":27,"ariaLabel":28},"Explore our impact metrics",{"loggedUserInfo":-1},{"currentArticle":510,"isPreviewPage":355,"hasSupplementalData":355,"showCrossmarkWidget":13,"articleTemplate":628,"currentArticlePageMetaInfo":629,"xmlParsedArticleContent":-1,"xmlParsingError":-1},{"id":511,"doi":512,"title":513,"acceptanceDate":514,"receptionDate":515,"publicationDate":516,"lastModifiedDate":517,"isPublished":13,"abstract":518,"researchTopic":28,"articleType":519,"stage":522,"keywords":524,"authors":530,"editors":567,"reviewers":575,"journal":589,"section":596,"impactMetrics":598,"volume":523,"articleVolume":602,"relatedArticles":603,"isPublishedV2":13,"contents":604,"files":607,"htmlZipFileName":627},1400444,"10.3389/fnins.2024.1400444","A review of artificial intelligence methods enabled music-evoked EEG emotion recognition and their applications","2024-08-14T09:33:19.000Z","2024-04-17T04:26:02.000Z","2024-09-04T00:00:00.000Z","2025-10-05T02:40:00.139Z","Music is an archaic form of emotional expression and arousal that can induce strong emotional experiences in listeners, which has important research and practical value in related fields such as emotion regulation. Among the various emotion recognition methods, the music-evoked emotion recognition method utilizing EEG signals provides real-time and direct brain response data, playing a crucial role in elucidating the neural mechanisms underlying music-induced emotions. Artificial intelligence technology has greatly facilitated the research on the recognition of music-evoked EEG emotions. AI algorithms have ushered in a new era for the extraction of characteristic frequency signals and the identification of novel feature signals. The robust computational capabilities of AI have provided fresh perspectives for the development of innovative quantitative models of emotions, tailored to various emotion recognition paradigms. The discourse surrounding AI algorithms in the context of emotional classification models is gaining momentum, with their applications in music therapy, neuroscience, and social activities increasingly coming under the spotlight. Through an in-depth analysis of the complete process of emotion recognition induced by music through electroencephalography (EEG) signals, we have systematically elucidated the influence of AI on pertinent research issues. This analysis offers a trove of innovative approaches that could pave the way for future research endeavors.",{"id":520,"name":521},27,"Review",{"id":523,"name":19},18,[525,526,527,528,529],"artificial intelligence","emotion recognition","music-induced","personalization","applications",[531,540,547,552,560],{"id":532,"firstName":533,"middleName":19,"lastName":534,"givenNames":535,"isCorresponding":355,"isProfilePublic":13,"userId":532,"email":-1,"affiliations":536},2684571,"Yan","Su","Yan ",[537],{"organizationName":538,"countryName":539,"cityName":19,"stateName":19,"zipCode":19},"School of Art, Zhejiang International Studies University","China",{"id":367,"firstName":541,"middleName":19,"lastName":542,"givenNames":543,"isCorresponding":355,"isProfilePublic":355,"userId":367,"email":-1,"affiliations":544},"Yong","Liu","Yong ",[545],{"organizationName":546,"countryName":539,"cityName":19,"stateName":19,"zipCode":19},"School of Education, Hangzhou Normal University",{"id":367,"firstName":533,"middleName":19,"lastName":548,"givenNames":535,"isCorresponding":355,"isProfilePublic":355,"userId":367,"email":-1,"affiliations":549},"Xiao",[550],{"organizationName":551,"countryName":539,"cityName":19,"stateName":19,"zipCode":19},"School of Arts and Media, Beijing Normal University",{"id":553,"firstName":554,"middleName":19,"lastName":555,"givenNames":556,"isCorresponding":355,"isProfilePublic":13,"userId":553,"email":-1,"affiliations":557},2685361,"Jiaqi","Ma","Jiaqi ",[558],{"organizationName":559,"countryName":539,"cityName":19,"stateName":19,"zipCode":19},"College of Science, Zhejiang University of Technology",{"id":561,"firstName":562,"middleName":19,"lastName":563,"givenNames":564,"isCorresponding":355,"isProfilePublic":13,"userId":561,"email":-1,"affiliations":565},506405,"Dezhao","Li","Dezhao ",[566],{"organizationName":559,"countryName":539,"cityName":19,"stateName":19,"zipCode":19},[568],{"id":569,"firstName":570,"middleName":19,"lastName":571,"givenNames":572,"isCorresponding":355,"isProfilePublic":13,"userId":569,"email":-1,"affiliations":573},29627,"Dan","Zhang","Dan ",[574],{"organizationName":19,"countryName":19,"cityName":19,"stateName":19,"zipCode":19},[576,583],{"id":577,"firstName":578,"middleName":19,"lastName":579,"givenNames":580,"isCorresponding":355,"isProfilePublic":13,"userId":577,"email":-1,"affiliations":581},1583946,"Jun","Jiang","Jun ",[582],{"organizationName":19,"countryName":19,"cityName":19,"stateName":19,"zipCode":19},{"id":584,"firstName":585,"middleName":19,"lastName":563,"givenNames":586,"isCorresponding":355,"isProfilePublic":13,"userId":584,"email":-1,"affiliations":587},2086612,"Wen","Wen ",[588],{"organizationName":19,"countryName":19,"cityName":19,"stateName":19,"zipCode":19},{"id":115,"slug":117,"name":116,"shortName":590,"electronicISSN":591,"field":592,"specialtyId":28,"journalSectionPaths":594},"Front. Neurosci.","1662-453X",{"id":593,"domainId":115},55,[595],{"section":596},{"id":120,"name":121,"slug":122,"specialtyId":597},86,{"views":599,"downloads":600,"citations":601},5426,1258,6,"Volume 18 - 2024",[],{"titleHtml":513,"fullTextHtml":605,"menuHtml":606},"\u003Cdiv class=\"JournalAbstract\"> \u003Ca id=\"h1\" name=\"h1\">\u003C/a>\n \u003Cdiv class=\"authors\">\u003Cspan class=\"author-wrapper notranslate\">\u003Ca href=\"https://loop.frontiersin.org/people/2684571\" class=\"user-id-2684571\">\u003Cimg class=\"pr5\" src=\"https://loop.frontiersin.org/images/profile/2684571/74\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\" alt=\"Yan Su\">Yan Su\u003C/a>\u003Csup>1\u003C/sup>\u003C/span>\u003Cspan class=\"author-wrapper notranslate\">\u003Cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"Yong Liu\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">Yong Liu\u003Csup>2\u003C/sup>\u003C/span>\u003Cspan class=\"author-wrapper notranslate\">\u003Cimg class=\"pr5\" src=\"https://loop.frontiersin.org/cdn/images/profile/default_32.jpg\" alt=\"Yan Xiao\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\">Yan Xiao\u003Csup>3\u003C/sup>\u003C/span>\u003Cspan class=\"author-wrapper notranslate\">\u003Ca href=\"https://loop.frontiersin.org/people/2685361\" class=\"user-id-2685361\">\u003Cimg class=\"pr5\" src=\"https://loop.frontiersin.org/images/profile/2685361/74\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\" alt=\"Jiaqi Ma\">Jiaqi Ma\u003C/a>\u003Csup>4\u003C/sup>\u003C/span>\u003Cspan class=\"author-wrapper notranslate\">\u003Ca href=\"https://loop.frontiersin.org/people/506405\" class=\"user-id-506405\">\u003Cimg class=\"pr5\" src=\"https://loop.frontiersin.org/images/profile/506405/74\" onerror=\"this.onerror=null;this.src='https://loop.frontiersin.org/cdn/images/profile/default_32.jpg';\" alt=\"Dezhao Li&#xA;\">Dezhao Li\u003C/a>\u003Csup>4\u003C/sup>\u003Csup>&#x0002A;\u003C/sup>\u003C/span>\u003C/div> \u003Cul class=\"notes\"> \u003Cli>\u003Cspan>\u003Csup>1\u003C/sup>\u003C/span>School of Art, Zhejiang International Studies University, Hangzhou, China\u003C/li> \u003Cli>\u003Cspan>\u003Csup>2\u003C/sup>\u003C/span>School of Education, Hangzhou Normal University, Hangzhou, China\u003C/li> \u003Cli>\u003Cspan>\u003Csup>3\u003C/sup>\u003C/span>School of Arts and Media, Beijing Normal University, Beijing, China\u003C/li> \u003Cli>\u003Cspan>\u003Csup>4\u003C/sup>\u003C/span>College of Science, Zhejiang University of Technology, Hangzhou, China\u003C/li> \u003C/ul>\n\u003Cp>Music is an archaic form of emotional expression and arousal that can induce strong emotional experiences in listeners, which has important research and practical value in related fields such as emotion regulation. Among the various emotion recognition methods, the music-evoked emotion recognition method utilizing EEG signals provides real-time and direct brain response data, playing a crucial role in elucidating the neural mechanisms underlying music-induced emotions. Artificial intelligence technology has greatly facilitated the research on the recognition of music-evoked EEG emotions. AI algorithms have ushered in a new era for the extraction of characteristic frequency signals and the identification of novel feature signals. The robust computational capabilities of AI have provided fresh perspectives for the development of innovative quantitative models of emotions, tailored to various emotion recognition paradigms. The discourse surrounding AI algorithms in the context of emotional classification models is gaining momentum, with their applications in music therapy, neuroscience, and social activities increasingly coming under the spotlight. Through an in-depth analysis of the complete process of emotion recognition induced by music through electroencephalography (EEG) signals, we have systematically elucidated the influence of AI on pertinent research issues. This analysis offers a trove of innovative approaches that could pave the way for future research endeavors.\u003C/p> \u003Cdiv class=\"clear\">\u003C/div> \u003C/div> \u003Cdiv class=\"JournalFullText\"> \u003Ca id=\"h2\" name=\"h2\">\u003C/a>\n\u003Ch2>1 Introduction\u003C/h2>\n\u003Cp class=\"mb15\">Music serves as a unique medium for people to express their emotions and also can arouse strong emotional responses. Previous studies have shown that the emotional changes induced by appropriate music can relieve listeners&#x2019; mental stress (\u003Ca href=\"#ref65\">Nawaz et al., 2019\u003C/a>; \u003Ca href=\"#ref19\">Colin et al., 2023\u003C/a>), promote emotional expression ability (\u003Ca href=\"#ref67\">Palazzi et al., 2021\u003C/a>; \u003Ca href=\"#ref63\">Micallef Grimaud and Eerola, 2022\u003C/a>; \u003Ca href=\"#ref112\">Zhang et al., 2022\u003C/a>), improve learning ability (\u003Ca href=\"#ref12\">Bergee and Weingarten, 2021\u003C/a>; \u003Ca href=\"#ref57\">Luo et al., 2023\u003C/a>), and so on. Moreover, it also can be applied in the regulation of mood-related disorders such as autism (\u003Ca href=\"#ref16\">Carpente et al., 2022\u003C/a>; \u003Ca href=\"#ref33\">Geretsegger et al., 2022\u003C/a>), depression (\u003Ca href=\"#ref32\">Geipel et al., 2022\u003C/a>; \u003Ca href=\"#ref37\">Hartmann et al., 2023\u003C/a>), and anxiety (\u003Ca href=\"#ref20\">Contreras-Molina et al., 2021\u003C/a>; \u003Ca href=\"#ref56\">Lu et al., 2021\u003C/a>). With the extensive applications of music-induced emotions in medical (\u003Ca href=\"#ref53\">Liang et al., 2021\u003C/a>), neuroscience (\u003Ca href=\"#ref94\">Thaut et al., 2021\u003C/a>; \u003Ca href=\"#ref30\">Fedotchev et al., 2022\u003C/a>), and music retrieval fields (\u003Ca href=\"#ref34\">Gomez-Canon et al., 2021\u003C/a>), the study of music-induced emotion recognition has received much attention in recent years.\u003C/p>\n\u003Cp class=\"mb0\">Empirical research on the effects of music on emotions has been discussed for more than three millennia (\u003Ca href=\"#ref72\">Perlovsky, 2012\u003C/a>), while modern evidence-based work on the effects of music on emotions has its roots in the early 20th century (\u003Ca href=\"#ref40\">Humphreys, 1998\u003C/a>). Western psychologists and musicians primarily conducted pioneering empirical research on music-induced emotions. A representative example is the experimental research conducted by the American psychologist and music educator Carl Emil Seashore on the emotional expression of music and the emotional impact of music on the listener, combining experiments and psychological tests and proposing the &#x201C;theory of musical expression,&#x201D; which emphasizes how elements such as melodies, rhythms, and harmonies of music affect people&#x2019;s emotional experiences, and lays the foundation for the subsequent development of related work (\u003Ca href=\"#ref62\">Metfessel, 1950\u003C/a>). With the development of psychology neuroscience and other fields, people gradually realized that the study of music&#x2019;s induction of emotions also requires an understanding of auditory perception, emotion discrimination, and neural mechanism, which is an interdisciplinary research work (\u003Ca href=\"#ref21\">Cui et al., 2022\u003C/a>; \u003Ca href=\"#ref78\">Ryczkowska, 2022\u003C/a>). Musicologists have mainly studied the influence of music on emotion induction from the perspective of different musical features of music (\u003Ca href=\"#ref69\">Panda et al., 2023\u003C/a>), including analyzing the influence of music on the listener&#x2019;s emotion from the perspective of musical elements (\u003Ca href=\"#ref77\">Ruth and Schramm, 2021\u003C/a>), quantitatively analyzing the emotional features of music to find the relationship between the features and emotion (\u003Ca href=\"#ref80\">Salakka et al., 2021\u003C/a>), developing emotion recognition algorithms based on musical features (\u003Ca href=\"#ref70\">Pandey and Seeja, 2022\u003C/a>), and exploring cross-cultural emotional understanding of and response to specific musical features (\u003Ca href=\"#ref100\">Wang et al., 2022\u003C/a>). These studies have made it possible to help people get a better understanding of the relationship between musical features and emotions, and provide theoretical support and practical guidance for the fields of music psychology, music therapy, and creativity, but researchers have also put forward different viewpoints on the individual differences in musical emotional responses and on the objective evaluative validity of emotions as in \u003Ca href=\"#fig1\">Figure 1\u003C/a>.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Figure 1\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg\" name=\"figure1\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg\" alt=\"www.frontiersin.org\" id=\"fig1\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Figure 1\u003C/b>. Research and applications of music-induced emotion classification with EEG.\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">With the development of brain science and technology, researchers have found that signals generated by the central nervous system, such as electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (MRI) are more objective and reliable in the field of emotion research (\u003Ca href=\"#ref3\">Alarcao and Fonseca, 2019\u003C/a>; \u003Ca href=\"#ref27\">Egger et al., 2019\u003C/a>; \u003Ca href=\"#ref79\">Saganowski et al., 2023\u003C/a>). Among various central nervous system signals, the monitoring of emotions using EEG signals is characterized by the convenience of noninvasive measurements, real-time measurements, and good objectivity. Research on emotion recognition based on EEG signals has been widely used in many disciplines in recent years and has received extensive attention from researchers as in \u003Ca href=\"#fig1\">Figure 1\u003C/a>. Artificial intelligence (AI) techniques that integrate EEG signals for identifying emotions elicited by music leverage AI&#x2019;s robust capabilities in data analytics, pattern recognition, and learning, alongside the distinctive benefits of EEG for real-time, non-invasive monitoring of brain activity. AI-enabled EEG recognition of music-induced emotions can accurately and in real-time identify emotions, which has broad applications in many areas including music therapy, education, entertainment, and so on.\u003C/p>\n\u003Cp class=\"mb15\">How to accurately identify music-induced emotions has always been a difficult research problem due to the subjectivity, abstractness, and individualized differences of music-induced emotions. Researchers have explored a variety of physiological signals to carry out emotion recognition studies, in which using the signal characteristics of facial expressions, researchers have classified emotions including fear, sadness, disgust, surprise, and joy, and the accuracy of obtaining emotion discrimination can be as high as 81% or more, but there are inconsistencies between different cultures in the understanding of facial expressions and the way of expression of facial expressions, which affect the generalizability of the results of the study (\u003Ca href=\"#ref93\">Tcherkassof and Dupr&#x000E9;, 2021\u003C/a>; \u003Ca href=\"#ref101\">Witkower et al., 2021\u003C/a>). Physiological parameters such as galvanic skin response, heart rate, temperature, blood pressure, and respiration rate have also been utilized for emotion recognition, but these methods are relatively inaccurate for emotion discrimination and highly influenced by other factors (\u003Ca href=\"#ref27\">Egger et al., 2019\u003C/a>; \u003Ca href=\"#ref79\">Saganowski et al., 2023\u003C/a>).\u003C/p>\n\u003Cp class=\"mb0\">In this study, the research methods, processes, and characteristics of EEG in music-induced emotion recognition have been analyzed. The potential future development directions of music-induced emotion based on EEG also have been discussed, which can promote the development of fundamental and application research on music-induced emotion.\u003C/p> \u003Ca id=\"h3\" name=\"h3\">\u003C/a>\n\u003Ch2>2 EEG signal and emotions\u003C/h2>\n\u003Cp class=\"mb0\">Measurement of EEG signals is capable of non-invasive, continuous recording of brain activity with a temporal resolution of a few milliseconds. Based on the characteristic waveform signals from different brain regions, EEG signals are widely used in cognitive neuroscience to research emotion regulation and processing, and the results of the related studies provide an important reference for further research on music-induced emotion recognition (\u003Ca href=\"#ref7\">Apicella et al., 2022b\u003C/a>; \u003Ca href=\"#ref70\">Pandey and Seeja, 2022\u003C/a>).\u003C/p>\n\u003Ch3>2.1 EEG signals and acquisition method\u003C/h3>\n\u003Cp class=\"mb0\">The activity of the central nervous system of the brain is closely related to human emotions, mainly realized through electrical communication between neurons (\u003Ca href=\"#ref2\">Ahmad et al., 2022\u003C/a>). When neurons are stimulated, the membrane potential will rise and fall to form weak electrical pulse signals and emotional changes can be monitored by recording and analyzing EEG signals. EEG signals have the characteristics of small amplitude (10&#x2013;100&#x2009;&#x03BC;V), many interference sources, and high uncertainty. To analyze the feature information of EEG, as shown in \u003Ca href=\"#tab1\">Table 1\u003C/a>, EEG signals are generally classified into &#x03B4;-band (1&#x2013;4&#x2009;Hz), &#x03B8;-band (4&#x2013;8&#x2009;Hz), &#x03B1;-band (8&#x2013;13&#x2009;Hz), &#x03B2;-1-band (13&#x2013;22&#x2009;Hz), &#x03B2;-2-band (22&#x2013;30&#x2009;Hz) and &#x03B3;-band (30&#x2013;64&#x2009;Hz) (\u003Ca href=\"#ref3\">Alarcao and Fonseca, 2019\u003C/a>), and the frequency bands of the bands are divided into slightly different bands by different researchers.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Table 1\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg\" name=\"table1\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t001.jpg\" alt=\"www.frontiersin.org\" id=\"tab1\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Table 1\u003C/b>. Bands of EEG signals.\n\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">Research has shown that the five bands of EEG signals mentioned above are directly or indirectly related to human emotions. While early studies suggested that the &#x03B4;-band was not connected to people&#x2019;s emotions, recent research has found that &#x03B4; wave is closely associated with the emotional state of individuals following emotional regulation and holds promise for use in areas such as music therapy for emotion regulation (\u003Ca href=\"#ref46\">Lapomarda et al., 2022\u003C/a>).\u003C/p>\n\u003Cp class=\"mb15\">The acquisition method of EEG signals is mainly categorized into invasive and non-invasive techniques. Invasive measurements require surgical implantation of electrodes to obtain clearer EEG signals, but this method is traumatized to the human body and difficult to widely apply, which is mainly used in clinical medical treatment. Non-invasive is to fit the electrodes to the surface of the head to collect brain signals.\u003C/p>\n\u003Cp class=\"mb0\">Previous research has demonstrated the significance of the limbic system (as in \u003Ca href=\"#fig2\">Figure 2A\u003C/a>) in regulating human emotions, making it a pivotal area of interest in the field of emotion research (\u003Ca href=\"#ref75\">Rolls, 2015\u003C/a>). To obtain more comprehensive brain signals, the internationally recognized 10/20 system, shown in \u003Ca href=\"#fig2\">Figure 2B\u003C/a>, is generally used in the arrangement of electrodes, i.e., the actual distance of adjacent electrodes is 10% or 20% of the distance of the brain skull (\u003Ca href=\"#ref86\">Silverman, 1963\u003C/a>). In the field of emotion recognition, multi-channel EEG acquisition is commonly utilized, featuring electrode channels ranging from 36 to 64, and a sampling frequency of 500 or 1,000&#x2009;Hz (\u003Ca href=\"#ref102\">Wu et al., 2024\u003C/a>). Traditional EEG acquisition system devices are often cumbersome and expensive, which hinders their widespread adoption and use. With the advancement of open source technologies like OpenBCI as in \u003Ca href=\"#fig2\">Figure 2C\u003C/a> and other EEG acquisition devices, more affordable, user-friendly, and portable options have emerged. Recently, these devices have become increasingly popular in EEG emotion recognition research (\u003Ca href=\"#ref4\">Aldridge et al., 2019\u003C/a>). To investigate the mechanisms of timely emotional response, both the stimulus source and the acquisition system are typically equipped with time-synchronization devices (\u003Ca href=\"#ref71\">Pei et al., 2024\u003C/a>).\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Figure 2\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg\" name=\"figure2\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g002.jpg\" alt=\"www.frontiersin.org\" id=\"fig2\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Figure 2\u003C/b>. \u003Cb>(A)\u003C/b> The limbic system for emotion, \u003Cb>(B)\u003C/b> the international 10/20 system with nine cortical regions labeled with different colors, \u003Cb>(C)\u003C/b> structure of a typical EEG system from OpenBCI.\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb0\">In music emotion recognition research, non-invasive acquisition schemes are commonly employed. In recent years, wireless wearable non-invasive EEG measurement devices have greatly facilitated EEG-based emotion recognition research (\u003Ca href=\"#ref6\">Apicella et al., 2022a\u003C/a>; \u003Ca href=\"#ref79\">Saganowski et al., 2023\u003C/a>). The emergence of these novel EEG acquisition protocols has significant implications for expanding the scope of EEG emotional applications.\u003C/p>\n\u003Ch3>2.2 EEG signal bands corresponding to different emotions\u003C/h3>\n\u003Cp class=\"mb15\">The brain exhibits diverse EEG response patterns for different emotions, and establishing the relationship between EEG signal bands and various emotional states is a crucial foundation for developing effective classification and recognition models. This correspondence serves as one of the key scientific challenges in the domain of artificial intelligence-based recognition of music-induced emotions. Around 1980, researchers found that EEG&#x2019;s characteristic signals correlate with human emotional states (\u003Ca href=\"#ref24\">Davidson and Fox, 1982\u003C/a>), as in \u003Ca href=\"#tab1\">Table 1\u003C/a>. Subsequently, researchers have investigated the relationship between distinct brainwave frequency bands and diverse emotional states. In 2001, Louis A. Schmidt et al. presented that emotions within valence can be distinguished by evaluating the asymmetry and overall power of &#x03B1;-band (8&#x2013;13&#x2009;Hz) from frontal brain EEG signals (\u003Ca href=\"#ref84\">Schmidt and Trainor, 2001\u003C/a>). In 2007, Daniela Sammler et al. conducted a systematic analysis of the correlation between various EEG frequency bands and emotions. Their findings revealed that &#x03B8;-band (4&#x2013;8&#x2009;Hz) power in the prefrontal lobe is more prominent during happy music stimulation, indicating its significance in emotional processing (\u003Ca href=\"#ref81\">Sammler et al., 2007\u003C/a>). With the continuous advancement of EEG analysis technology, it has become increasingly apparent that the intricate nature of the brain&#x2019;s emotional processes makes it challenging to establish precise correlations between different emotions and signals derived from a single brain region or waveform. Certainly, in some specific scenarios, researchers continue to explore and identify the most prominent EEG frequency bands to simplify the challenges associated with emotion recognition.\u003C/p>\n\u003Cp class=\"mb0\">The application of artificial intelligence in the emotional recognition of music-induced electroencephalography (EEG) holds significant value in two primary aspects. On one hand, the utilization of AI algorithms assists researchers in discerning and selecting the appropriate frequency bands amidst a multitude of options. On the other hand, the deployment of AI algorithms facilitates the exploration of additional effective frequency bands, enhancing the depth and breadth of research in this domain. With the development of artificial intelligence and deep learning technologies, emotion recognition by utilizing various frequency band features from different brain regions has emerged as a prominent and contemporary approach (\u003Ca href=\"#ref71\">Pei et al., 2024\u003C/a>; \u003Ca href=\"#ref105\">Xu et al., 2024\u003C/a>). Machine learning based Support Vector Machine (SVM) (\u003Ca href=\"#ref9\">Bagherzadeh et al., 2023\u003C/a>), Na&#x000EF;ve Bayes (NB) (\u003Ca href=\"#ref66\">Oktavia et al., 2019\u003C/a>), and K Nearest Neighbors (KNN) (\u003Ca href=\"#ref83\">Sari et al., 2023\u003C/a>) classifier methods have been applied in this field. Deep learning based classification methods such as Convolutional Neural Networks (CNN) (\u003Ca href=\"#ref107\">Yang et al., 2019\u003C/a>), Recurrent Neural Networks (RNN) (\u003Ca href=\"#ref115\">Zhong et al., 2023\u003C/a>), Long-Short-Term Memory (LSTM) (\u003Ca href=\"#ref25\">Du et al., 2022\u003C/a>), and other classification methods have also been used in EEG recognition studies.\u003C/p> \u003Ca id=\"h4\" name=\"h4\">\u003C/a>\n\u003Ch2>3 Preprocessing and feature extraction of EEG signals\u003C/h2>\n\u003Cp class=\"mb0\">Extracting effective emotional state information from raw EEG signals is a highly challenging task, given that the signal is a multi-frequency non-stationary signal. EEG preprocessing and feature extraction are essential steps in the recognition algorithms for effective analysis and interpretation of the EEG signals. The purpose of preprocessing EEG signals is to eliminate human motion interference and environmental noise that are unrelated to emotion pattern recognition. This is essential for enhancing the accuracy and robustness of the recognition algorithm.\u003C/p>\n\u003Ch3>3.1 EEG preprocessing\u003C/h3>\n\u003Cp class=\"mb15\">Noise removal is a crucial objective of EEG signal preprocessing. EEG signals are often vulnerable to interference from various sources such as environmental electromagnetic signals (~50&#x2013;60&#x2009;Hz), eye movements (~4&#x2009;Hz), electrocardiogram signals (ECG, ~1.2&#x2009;Hz), and so on.\u003C/p>\n\u003Cp class=\"mb0\">The removal of these noises can significantly enhance the robustness of the EEG model. Usually, these disturbing signals can be filtered out with band-pass filters, wavelet packet filtering, or independent component analysis (ICA) methods as in \u003Ca href=\"#tab2\">Table 2\u003C/a>. However, researchers have different opinions regarding the signal filtering methods in preprocessing. Some argue that these methods do not eliminate interfering noise, while others believe that these techniques remove noise at the expense of potentially discarding valuable EEG information. To further improve the denoising performance, the artifact subspace reconstruction (ASR) method can be applied to remove the artificial signals. What&#x2019;s more, the average value of overall electrodes can be applied to subtract from each channel to reduce the system noise (\u003Ca href=\"#ref41\">Katsigiannis and Ramzan, 2018\u003C/a>). Compared to classical machine learning algorithms, deep learning classification methods for emotion recognition are less influenced by the effects of preprocessing techniques.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Table 2\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg\" name=\"table2\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t002.jpg\" alt=\"www.frontiersin.org\" id=\"tab2\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Table 2\u003C/b>. Benefits and drawbacks of EEG preprocessing methods.\n\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb0\">The most popular open source toolbox for EEG preprocessing is EEGLAB running in the MATLAB environment (\u003Ca href=\"#ref60\">Mart&#x000ED;nez-Saez et al., 2024\u003C/a>; \u003Ca href=\"#ref71\">Pei et al., 2024\u003C/a>; \u003Ca href=\"#ref102\">Wu et al., 2024\u003C/a>). This interactive toolbox can be applied to process continuous and event-related EEG signals. Moreover, the artifacts from eye movements can be removed with the run independent component analysis (RunICA) algorithm incorporated in EEGLAB based on the independent component analysis (ICA) method (\u003Ca href=\"#ref102\">Wu et al., 2024\u003C/a>). The expansion of artificial intelligence has led to the integration of EEG signal preprocessing algorithms into a growing array of commercial AI development platforms, including Python, Brainstorm, and Curry.\u003C/p>\n\u003Ch3>3.2 Time domain feature extraction\u003C/h3>\n\u003Cp class=\"mb15\">Emotional changes in the brain can be influenced by musical stimulation, leading to observable effects on EEG signals. These EEG signals exhibit various time-dependent features, which can be analyzed in the time domain. Time-domain features provide intuitive insights and are relatively easy to obtain. Some categories of time-domain features in EEG analysis include event-related potentials (ERPs), statistical features (such as mean, average, standard deviation, skewness, kurtosis, etc.), rise and fall times, and burst suppression (\u003Ca href=\"#ref89\">Stancin et al., 2021\u003C/a>; \u003Ca href=\"#ref49\">Li et al., 2022b\u003C/a>).\u003C/p>\n\u003Cp class=\"mb0\">Time domain features can intuitively capture changes in brain states following music-induced emotions. The active regions corresponding to these emotions can typically be promptly identified through intuitive brain area distribution maps, as in \u003Ca href=\"#fig3\">Figure 3\u003C/a>, offering valuable insights for the improvement of recognition algorithms. Time domain features are generally preferred in emotion recognition research.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Figure 3\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg\" name=\"figure3\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g003.jpg\" alt=\"www.frontiersin.org\" id=\"fig3\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Figure 3\u003C/b>. Topographical maps of EEG signals for different types of music (\u003Ca href=\"#ref104\">Xu J. et al., 2023\u003C/a>).\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">Moreover, event-related potentials (ERPs) are specific patterns of EEG activity that are time-locked to particular sensory, cognitive, or motor events (\u003Ca href=\"#ref61\">Martins et al., 2022\u003C/a>). They reflect the brain&#x2019;s response to stimuli and provide valuable information about cognitive processes, which is very helpful in studying the dynamic processes of emotion change with music stimuli. Rise and fall times refer to the duration it takes for the EEG signal to rise from its baseline to its peak (rise time) or fall back to the baseline (fall time). These measures provide insights into the speed of neural activation or deactivation. Currently, there is a relatively limited body of research on the speed, duration, and recovery time of human emotions stimulated by music. It is important to dedicate attention to these aspects in future studies to gain a deeper understanding of the relevant phenomena with the time domain feature of rise and fall times.\u003C/p>\n\u003Cp class=\"mb0\">By examining these time-domain features of EEG signals, researchers can gain a better understanding of the temporal dynamics of brain activity related to emotional responses to music. The deployment of artificial intelligence algorithms enables the real-time identification of emotions induced by music via EEG signals. Making well-informed choices and applying time-domain features effectively is essential for advancing these studies.\u003C/p>\n\u003Ch3>3.3 Frequency domain feature extraction\u003C/h3>\n\u003Cp class=\"mb15\">As crucial parameters in EEG emotion recognition algorithms, frequency domain features offer more intricate emotion-related information, including the distribution of energy across different frequency bands. For instance, the energy distribution in high-frequency bands (such as &#x03B2;-band and &#x03B3;-band waves) tends to increase during pleasurable and exciting emotional states (\u003Ca href=\"#ref50\">Li et al., 2018\u003C/a>). Analyzing the phase synchronization degree of signals can provide insights into changes in information &#x03B8;-band wave patterns between brain regions during different emotional states. For example, theta synchronization between the frontal and temporal lobes is associated with pleasant emotions (\u003Ca href=\"#ref8\">Ara and Marco Pallar&#x000E9;s, 2020\u003C/a>). Frequency domain features allow for the analysis of interactions between various brain regions. By calculating correlation features between different brain regions at different frequencies, changes in information exchange patterns between brain regions during different emotional states can be observed (\u003Ca href=\"#ref58\">Maffei, 2020\u003C/a>). Based on the inter-correlation maps of &#x03B4;, &#x03B1; and &#x03B3;-band waves stimulated by six different scenarios, the widest topographical distribution is &#x03B4;-band, while the narrowest is &#x03B1;-band (\u003Ca href=\"#ref58\">Maffei, 2020\u003C/a>).\u003C/p>\n\u003Cp class=\"mb0\">Various techniques are commonly employed for extracting frequency domain features as in \u003Ca href=\"#tab3\">Table 3\u003C/a>. These include the following methods: Fourier transform, wavelet transform, independent component analysis, and matrix decomposition (\u003Ca href=\"#ref96\">Torres et al., 2020\u003C/a>; \u003Ca href=\"#ref117\">Zhou et al., 2022\u003C/a>; \u003Ca href=\"#ref51\">Li et al., 2023\u003C/a>; \u003Ca href=\"#ref59\">Mahmoud et al., 2023\u003C/a>). Fourier transform is utilized to convert a time domain signal into a frequency domain signal, providing spectral information such as frequency components and amplitude details (\u003Ca href=\"#ref59\">Mahmoud et al., 2023\u003C/a>). Frequency domain feature extraction techniques based on the Fourier transform encompass power spectral density (PSD), average power spectral density (APSD), and related features. PSD is usually evaluated within a specific frequency band, considered the most commonly applied feature for classical emotion classifiers (\u003Ca href=\"#ref105\">Xu et al., 2024\u003C/a>).\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Table 3\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg\" name=\"table3\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t003.jpg\" alt=\"www.frontiersin.org\" id=\"tab3\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Table 3\u003C/b>. Advantages and disadvantages of frequency domain feature extraction methods.\n\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">Wavelet transform offers a more versatile and multi-scale approach to signal analysis, delivering both frequency and time information (\u003Ca href=\"#ref9\">Bagherzadeh et al., 2023\u003C/a>). Frequency domain feature extraction methods associated with wavelet transform involve wavelet packet decomposition (WPD), wavelet packet energy features, and similar characteristics. Independent component analysis serves as a signal decomposition method grounded in independence assumptions, yielding independent frequency domain components post-decomposition (\u003Ca href=\"#ref85\">Shu et al., 2018\u003C/a>). Frequency domain feature extraction techniques stemming from independent component analysis include frequency band energy distribution, phase synchronization degree, and more. Matrix decomposition is an algebraic signal decomposition method that disentangles the original signal into distinct frequency domain components (\u003Ca href=\"#ref39\">Hossain et al., 2023\u003C/a>). These techniques enable the extraction of diverse frequency domain features such as spectral characteristics, phase synchronization degrees, correlation features, and so forth. In emotion classification applications, a tailored selection and adjustment of methods and feature combinations can be made based on specific requirements.\u003C/p>\n\u003Cp class=\"mb0\">The capabilities of artificial intelligence algorithms in mining large-scale data sets not only enable the automatic extraction of frequency characteristics from EEG signals but also reveal the underlying connections between frequency domain signals and emotions.\u003C/p>\n\u003Ch3>3.4 Time-frequency domain feature extraction\u003C/h3>\n\u003Cp class=\"mb0\">Time-frequency feature extraction methods involve analyzing EEG signal changes in both time and frequency to extract characteristic parameters that capture the dynamic nature of the signal (\u003Ca href=\"#ref9\">Bagherzadeh et al., 2023\u003C/a>). Common techniques of time-frequency domain features include wavelet transform (\u003Ca href=\"#ref43\">Khare and Bajaj, 2021\u003C/a>) and short-time Fourier transform (STFT) (\u003Ca href=\"#ref71\">Pei et al., 2024\u003C/a>). These methods enable the extraction of information across various time scales and frequency ranges, unveiling how signals evolve and frequency as in \u003Ca href=\"#fig4\">Figure 4\u003C/a>, which also has been applied by our group (\u003Ca href=\"#ref48\">Li et al., 2022a\u003C/a>).\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Figure 4\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg\" name=\"figure4\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g004.jpg\" alt=\"www.frontiersin.org\" id=\"fig4\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Figure 4\u003C/b>. Time-frequency plots before and after stimulation were used by the authors&#x2019; research group.\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">By extracting time-frequency features, a more comprehensive description of the signal&#x2019;s dynamic characteristics can be achieved, laying the groundwork for subsequent signal processing and emotion classification analysis.\u003C/p>\n\u003Cp class=\"mb0\">Time-frequency plots typically encompass a vast array of data points, representing a high-dimensional dataset. The application of artificial intelligence algorithms can automatically discern time-frequency patterns associated with various emotions. This capacity for autonomous learning and data mining enhances the efficacy and reliability of time-frequency plots in the identification of emotions induced by music.\u003C/p>\n\u003Ch3>3.5 Other advanced features\u003C/h3>\n\u003Cp class=\"mb15\">The development of new emotion-recognition features has been significantly influenced by researchers&#x2019; profound insights into the brain&#x2019;s response to emotions.\u003C/p>\n\u003Cp class=\"mb0\">Prior physiological and psychological studies have demonstrated that emotions, being intricate mental states, can be discerned by detecting the status of connections between brain regions. In recent years, scholars have advocated for the establishment of a network of brain regions using phase-locked values and the extraction of features from multiple brain functional connectivity networks through the application of the Hilbert transform. These graph features are then fused to facilitate emotion recognition (\u003Ca href=\"#ref47\">Li et al., 2019\u003C/a>). Based on this concept, researchers have introduced a novel feature called asPLV (averaged sub-frequency phase locking value), which is derived from the Morlet transform method. This feature effectively mitigates the impact of the brain&#x2019;s inherent frequency oscillations induced by the cognitive processing of emotional fluctuations, thereby enhancing the accuracy of recognizing mood changes induced by music. The calculation process for asPLV is outlined as in \u003Ca href=\"#tab4\">Table 4\u003C/a>.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Table 4\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg\" name=\"table4\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t004.jpg\" alt=\"www.frontiersin.org\" id=\"tab4\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Table 4\u003C/b>. Typical extraction process flow of asPLV.\n\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">In recent years, scholars have discovered that the spatiotemporal characteristics of EEG play a crucial role in emotion recognition. Many studies have introduced novel spatiotemporal features based on self-attention mechanisms (\u003Ca href=\"#ref116\">Zhou and Lian, 2023\u003C/a>). As our comprehension of the neural mechanisms underlying emotional responses deepens, these new features are critical for enhancing the accuracy of emotion recognition.\u003C/p>\n\u003Cp class=\"mb0\">Other than these commonly applied features already discussed, artificial intelligence algorithms excel in processing multidimensional data, enabling the discovery of innovative feature metrics. These algorithms hold great promise in identifying individual-specific traits, and crafting features that are sensitive to the distinctive attributes of each individual.\u003C/p> \u003Ca id=\"h5\" name=\"h5\">\u003C/a>\n\u003Ch2>4 Emotion data source and modeling\u003C/h2>\n\u003Cp class=\"mb0\">Auto-emotion recognition can be realized by integrating various data sources and emotion models. This is important for the development of music-induced emotion recognition and its application areas. In the realm of music-induced emotion recognition, emotional data sources form the foundation for acquiring emotion related in sights, while models serve as the essential tools for processing and analyzing this valuable information (\u003Ca href=\"#ref79\">Saganowski et al., 2023\u003C/a>; \u003Ca href=\"#ref104\">Xu J. et al., 2023\u003C/a>).\u003C/p>\n\u003Ch3>4.1 Data sources for music-evoked emotion classification\u003C/h3>\n\u003Cp class=\"mb0\">In recent years, in order to promote research on music-induced emotions, a series of databases of music-triggered emotions have been established, with emotion labels provided by psychologists. Although these databases can be used for music-triggered emotion research, they lack a unified criterion. Based on the EEG method of emotion discrimination, researchers also have established emotion databases containing EEG signals. These open source databases are not only important resources for conducting research on music-triggered emotions, but can also be used to evaluate the performance of different EEG algorithms. \u003Ca href=\"#tab5\">Table 5\u003C/a> shows some common open source music emotion databases and their characteristics.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Table 5\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg\" name=\"table5\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t005.jpg\" alt=\"www.frontiersin.org\" id=\"tab5\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Table 5\u003C/b>. Open-source music emotion databases for music-induced emotion classification.\n\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">\u003Cb>The AMG1608 database\u003C/b> is a database containing acoustic features extracted from 1,608 music clips of 30s as well as emotion annotations provided by 665 subjects, consisting of 345 females and 320 males. The database used a dimensional emotion model with validity and arousal (VA) as the coordinates in the emotion annotation, and the subjects annotated the emotional state of each music clip. The dataset contains two subsets of emotion annotations from National Taiwan University and Amazon Turkish Robotics and is characterized by a large amount of data capable of being publicly accessible and can be used for music emotion recognition research.\u003C/p>\n\u003Cp class=\"mb15\">\u003Cb>The CAL500 database\u003C/b> is composed of 500 Western songs&#x2019; clips written by 500 different artists. For the emotion annotation of the music, 174 music-related semantic keywords were used, and at least three subjects annotated keywords for each song. These annotated words were also post-processed algorithmically to constitute a vector of annotated words and weights, ensuring the reliability of the annotation labels. The dataset is able to satisfy the fine granularity and differentiation required in music emotion recognition research.\u003C/p>\n\u003Cp class=\"mb15\">\u003Cb>The DEAM database\u003C/b>, which labels musical emotions in terms of valence and arousal (VA) coordinates, has 1,802 songs licensed under the Creative Commons (CC) license. This music library contains categories such as rock, pop, soul, blues, electronic, classical, hip-hop, international, experimental, ethnic, jazz, country, and pop. The emotion annotations for these songs were made by 21 active teams from all over the world, and these annotations were statistically processed to form a database that can be used for music emotion research.\u003C/p>\n\u003Cp class=\"mb15\">\u003Cb>The emoMusic database\u003C/b> contains 1,000 audio tracks in MP3 format licensed under the Creative Commons (CC) License in eight different genres: blues, electronica, rock, classical, folk, jazz, country, and pop, with 125 tracks in each genre. The emotion labeling of the music was evaluated using valence and arousal (VA) model, where valence indicates positive and negative emotions and arousal indicates emotional intensity. The database collects time-varying (per second) continuous VA rating data, with each song containing at least 10 thematic annotations. The database can be utilized for the conduct of research related to music emotion annotation and other related studies.\u003C/p>\n\u003Cp class=\"mb15\">\u003Cb>The Emotify database\u003C/b> contains 100 pieces of music from each of the four genres of classical, rock, pop, and electronic music randomly selected from a collection of music containing 241 different albums by 140 performers. The database used the Geneva Emotional Music Scale (GEMS), in which subjects labeled the emotions of the music using a Likert scale using a scale of 1&#x2013;5. The database provides case studies and information on the effects of other factors on evoked emotions (gender, mood, music preference).\u003C/p>\n\u003Cp class=\"mb15\">\u003Cb>The DEAP database\u003C/b> is a music emotion recognition database based on an EEG emotion recognition method, which was built together by a consortium of four universities from the UK, the Netherlands, and Sweden, and records EEG and physiological information from 32 subjects who watched a series of forty 1-min music video clips. The database was selected as a semi-automatic stimulus selection method based on emotional labeling is open access to academics and can facilitate research related to emotional stimulation in modern music.\u003C/p>\n\u003Cp class=\"mb15\">\u003Cb>The IADS database\u003C/b> is the International Emotionally Digitized Sound Database, which is divided into two distinct phases. The initial Phase I database, established in 1999, contains a modest collection of data that has seen limited use in contemporary times. In contrast, Phase II is an expansive compilation of 167 digitally captured ambient sounds that are frequently encountered in everyday life, such as the joyful laughter of an infant, the rhythmic sounds of cooking, and the dramatic rumble of thunderstorms, with each sound clip precisely lasting 6&#x2009;s. The collection is meticulously annotated, with each piece of digital audio being evaluated by participants through a self-assessment approach that utilizes the affective dimensions of the Valence-Arousal (VA) model.\u003C/p>\n\u003Cp class=\"mb0\">At present, these databases of music emotions are mainly based on foreign music libraries, the situation is related to the importance of music in the relevant regions, and the establishment of music databases based on Chinese musical compositions is yet to be carried out. Due to the complexity of the signal measurement and classification of EEG in the early stage, there are fewer studies for EEG music-induced emotion recognition. Enabled by artificial intelligence, EEG-based music emotion recognition can help to expand the establishment of databases as soon as possible, and it can help more researchers to apply the established databases. Currently, besides the above common music-induced emotion databases, many researchers also use their own designed libraries to carry out personalized research in the study of music-induced emotions with EEG. As artificial intelligence technology advances, it facilitates the integration of EEG data with other modalities of data, thereby enriching the dimensions of information within the database. The application of data augmentation techniques helps to enhance the generalization capability of models built from the database. With the progression of research in EEG-induced emotion recognition, artificial intelligence can also assist in the automatic consolidation and updating of databases, providing technical support for the construction of more comprehensive, accurate, and holistic datasets.\u003C/p>\n\u003Ch3>4.2 Emotion classification models\u003C/h3>\n\u003Cp class=\"mb0\">To address the challenge of quantifying the emotions elicited by music, researchers have developed a variety of models specifically designed to capture the nuances of music-induced emotions. These models aim to provide a structured approach to understanding and measuring the complex emotional responses that music can evoke. The classical music emotion models for analysis are shown in \u003Ca href=\"#fig5\">Figure 5\u003C/a>, including the Hevner model (\u003Ca href=\"#ref38\">Hevner, 1936\u003C/a>), the Pleasure Arousal Dominance (PAD) model (\u003Ca href=\"#ref76\">Russell, 1980\u003C/a>), and the Thayer model (\u003Ca href=\"#ref95\">Thayer and McNally, 1992\u003C/a>). With the development of artificial intelligence technology, some algorithm-based emotion classification approaches also have been proposed.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Figure 5\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg\" name=\"figure5\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g005.jpg\" alt=\"www.frontiersin.org\" id=\"fig5\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Figure 5\u003C/b>. Schematic diagrams of common emotion recognition models. \u003Cb>(A)\u003C/b> Hevner&#x2019;s model, \u003Cb>(B)\u003C/b> PAD model, \u003Cb>(C)\u003C/b> Thayer&#x2019;s model.\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">In computerized categorization studies of musical emotions, the first psychological Hevner emotion classification model was proposed in 1936 (\u003Ca href=\"#ref38\">Hevner, 1936\u003C/a>). This model classifies music emotion states into eight categories: Solemn, Sad, Dreamy, Quiet, Graceful, Happy, Excited, and Powerful as in \u003Ca href=\"#fig5\">Figure 5A\u003C/a>. Each category can be further subdivided into more detailed and extensive emotion words, with a total of 67. This emotion classification model was set up considering musicology and psychology and is more abundant in the selection of emotion keywords, which is helpful for the research of emotion recognition in musical works. Hevner is a discrete emotion classification model that is often used as an emotion label for songs in music-induced emotion recognition research. However, due to the large number of labeling categories of the model and the relatively low variability of physiological properties of some categories, this model is seldom applied in EEG-based music emotion recognition studies, but it can be considered in relevant studies for featured music.\u003C/p>\n\u003Cp class=\"mb15\">Among the classification models of musical emotions, PAD is a three-dimensional measurement model that was proposed in 1980. As in \u003Ca href=\"#fig5\">Figure 5B\u003C/a>, this model establishes three main dimensions of Pleasure-Displeasure, Arousal-Nonarousal, and Dominance-Submissiveness, which indicate the direction of the emotion, the degree of neurophysiological activation, and the degree of individual feeling, respectively. This categorization provides a continuous quantitative evaluation method, which has been widely used in psychological and emotional brand evaluation (\u003Ca href=\"#ref108\">Yang et al., 2020\u003C/a>) but has not been used much in the actual evaluation of musical emotions. Thayer&#x2019;s model is a two-dimensional model that suggests different emotions are classified based on two-dimensional underlying dimensions, i.e., Energy awakening and Tension awakening. Using stress as the horizontal coordinate and energy as the vertical coordinate, emotions are categorized into four zones: vitality, anxiety, contentment, and depression as in \u003Ca href=\"#fig5\">Figure 5C\u003C/a>. This model is proposed based on a psychological perspective and describes the music mood according to quantitative thinking, which is often used to classify the mood of audiophile music in MP3 and WAV formats (\u003Ca href=\"#ref13\">Brotzer et al., 2019\u003C/a>). Since this model has fewer classification dimensions compared with other mentioned models and is more visible on the emotional response, they are well suitable to be used for EEG recognition of music-induced emotions. Besides the above classical emotion classification models, some researchers have also used probability distribution (\u003Ca href=\"#ref44\">Kim et al., 2022\u003C/a>), neural network method (\u003Ca href=\"#ref106\">Yang, 2021\u003C/a>), linear regression (\u003Ca href=\"#ref35\">Griffiths et al., 2021\u003C/a>), and inverse word pairs (\u003Ca href=\"#ref54\">Liu et al., 2019\u003C/a>) approaches to characterize the emotions of music. The probability distribution method describes the emotions corresponding to the song in the emotion description space, which gives a more comprehensive and intuitive description of the song. The ranking is utilized to order the emotion descriptions of songs according to the degree of relevance of the emotions expressed, which is convenient to provide a quick description method. The antonym pairs can give a relatively objective description of the mood of the music. Several researchers have currently extended discrete and multidimensional models for music emotion description based on these ideas.\u003C/p>\n\u003Cp class=\"mb0\">These new classification models are related to the development of emotion categorization algorithms and have a large potential for application in the field of EEG music-induced emotions. Different emotion models can be used to describe the classification of emotions in different states, meanwhile, there are some intersections between these different models. For different practical applications, people need to choose the appropriate emotion classification models according to the research scenarios and artificial intelligence algorithms.\u003C/p>\n\u003Ch3>4.3 Emotional intensity model\u003C/h3>\n\u003Cp class=\"mb0\">Emotional intensity models were applied to quantitatively delineate the depth of emotions experienced by individuals in specific circumstances, forming the cornerstone for emotion recognition and prediction models. The discourse on quantifying emotional intensity emerged around 1990, advocating for a shift from solely focusing on subjective emotional aspects to incorporating physiological and observable behaviors as metrics of intensity (\u003Ca href=\"#ref76\">Russell, 1980\u003C/a>; \u003Ca href=\"#ref88\">Sonnemans and Frijda, 1994\u003C/a>). In 1994, Frijida introduced a five-dimensional model for scrutinizing subjective emotions, encompassing dimensions such as the frequency and intensity of re-collected and re-experienced emotions, latency and duration of emotions, intensity of actions and propensities, as well as actual behaviors, beliefs, and behavioral changes (\u003Ca href=\"#ref88\">Sonnemans and Frijda, 1994\u003C/a>). In recent years, researchers have explored the use of emotional intensity modeling to study the instantaneous dynamic processes in the brain under external stimuli, as in \u003Ca href=\"#fig6\">Figure 6A\u003C/a> (\u003Ca href=\"#ref31\">Gan et al., 2024\u003C/a>). This innovative approach provides a new approach to the study of the neural mechanisms and processes of music-induced emotions.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Figure 6\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg\" name=\"figure6\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g006.jpg\" alt=\"www.frontiersin.org\" id=\"fig6\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Figure 6\u003C/b>. \u003Cb>(A)\u003C/b> The instantaneous emotional intensity of stimulated emotion dynamic process (\u003Ca href=\"#ref31\">Gan et al., 2024\u003C/a>), \u003Cb>(B)\u003C/b> schematic of a fine-grained emotional division.\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">Despite offering a theoretical framework for objectively describing emotional states, the model&#x2019;s impact was limited due to the scarcity of physiological emotion measures at that time.\u003C/p>\n\u003Cp class=\"mb0\">A prevalent theoretical framework in recent years for elucidating emotional intensity is \u003Ca href=\"#ref76\">Russell, 1980\u003C/a> proposition that emotional experiences can be depicted in a two-dimensional space defined by emotional valence (positive vs. negative emotions) and arousal levels (high vs. low) (\u003Ca href=\"#ref76\">Russell, 1980\u003C/a>). Building upon this framework, researchers have delved into refining each dimension to achieve a nuanced portrayal of emotions illustrated in \u003Ca href=\"#fig6\">Figure 6B\u003C/a>, laying the groundwork for leveraging artificial intelligence in digitally characterizing emotions (\u003Ca href=\"#ref74\">Reisenzein, 1994\u003C/a>; \u003Ca href=\"#ref113\">Zhang et al., 2023\u003C/a>). Physiological emotional intensity indices such as EEG, ECG, and MRI are not only valuable for emotion recognition but also serve as essential tools for studying the dynamic processes and physiological mechanisms underlying music-induced emotional changes (\u003Ca href=\"#ref98\">Ueno and Shimada, 2023\u003C/a>; \u003Ca href=\"#ref102\">Wu et al., 2024\u003C/a>).\u003C/p> \u003Ca id=\"h6\" name=\"h6\">\u003C/a>\n\u003Ch2>5 Artificial intelligence algorithms for EEG emotion recognition\u003C/h2>\n\u003Cp class=\"mb0\">Music-induced EEG-based emotion classification research can be considered an artificial intelligence classification task, where the selection of appropriate classification algorithms is a crucial element in the current research on EEG-based emotion classification. These algorithms are not only the topicality of emotion classification research in EEG but also serve as an important foundation for further research into music-induced emotions (\u003Ca href=\"#ref42\">Khabiri et al., 2023\u003C/a>).\u003C/p>\n\u003Ch3>5.1 Classical machine learning algorithms\u003C/h3>\n\u003Cp class=\"mb0\">Based on the EEG feature signals of music-induced emotions, various classical machine learning classification methods have been commonly used to achieve relatively good classification accuracy. These methods include classical classifiers such as Bayes classifier (BC) (\u003Ca href=\"#ref45\">Koelstra et al., 2012\u003C/a>), linear regression (LR) (\u003Ca href=\"#ref114\">Zheng and Lu, 2015\u003C/a>), support vector machines (SVM) (\u003Ca href=\"#ref9\">Bagherzadeh et al., 2023\u003C/a>), K Nearest Neighbor (KNN) (\u003Ca href=\"#ref42\">Khabiri et al., 2023\u003C/a>), and random forests (RF) (\u003Ca href=\"#ref71\">Pei et al., 2024\u003C/a>), as in \u003Ca href=\"#tab6\">Table 6\u003C/a>.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Table 6\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg\" name=\"table6\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t006.jpg\" alt=\"www.frontiersin.org\" id=\"tab6\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Table 6\u003C/b>. Several classical machine learning methods applied in music-induced emotion classification.\n\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb15\">These algorithms have been extensively employed in the field of emotion classification research and have shown promising results in accurately classifying mu-sic-induced emotions (\u003Ca href=\"#ref22\">Dadebayev et al., 2022\u003C/a>). One of the commonly used supervised classification algorithms for music sentiment is the K Nearest Neighbor (KNN) algorithm. KNN, as a supervised learning algorithm, is highly versatile and easy to understand. It is robust to outliers and has a simple principle. However, the computational results of the KNN algorithm can be influenced by the training set samples as well as the value of K, which represents the number of nearest neighbors considered for classification. It is important to carefully select the appropriate value of K and ensure the representativeness and quality of the training set to achieve accurate classification results in music sentiment analysis. Another commonly used classical classifier for music sentiment analysis is the Support Vector Machine (SVM). When using SVM for classification, the choice of the kernel function has a significant impact on its performance. By mapping the features nonlinearly to a high-dimensional space using the kernel function, SVM improves the robustness of the music emotion recognition algorithm (\u003Ca href=\"#ref15\">Cai et al., 2022\u003C/a>). SVM is particularly effective in handling high-dimensional data, making it suitable for achieving better classification results in music EEG emotion recognition compared to KNN.\u003C/p>\n\u003Cp class=\"mb0\">Classical machine learning algorithms exhibit strong interpretability, high data efficiency, and low computational resource requirements in music emotion recognition research. These characteristics are highly desirable for studying the neural mechanisms of music-induced mood changes. However, in practical applications of music emotions, these models often suffer from poor generalization performance and require improved accuracy.\u003C/p>\n\u003Ch3>5.2 Deep learning algorithms\u003C/h3>\n\u003Cp class=\"mb15\">Although machine learning algorithms have been used for emotion recognition and have shown improvements, there are still challenges such as feature extraction, stability, and accuracy. However, the emergence of deep learning methods in recent years has provided a promising approach for EEG-based music emotion recognition research. Deep learning algorithms, characterized by their strong learning ability, have shown great potential in this field.\u003C/p>\n\u003Cp class=\"mb0\">One notable deep learning algorithm applied in EEG-based music emotion recognition research is Convolutional Neural Networks (CNN). CNN extends the network structure of artificial neural networks by incorporating convolutional layers and pooling layers between the input layer and the fully connected layer. This architecture allows CNN to automatically learn and extract relevant features from the input data, making it suitable for analyzing complex patterns in EEG signals. By leveraging deep learning techniques, researchers can enhance the performance of music emotion recognition systems. Deep learning algorithms can effectively handle the high-dimensional and time-varying nature of EEG signals, leading to improved accuracy and stability in emotion recognition tasks. Moreover, with the ability to capture hierarchical representations, CNN can capture both local and global features in EEG data, enabling a more comprehensive analysis of music-induced emotions (\u003Ca href=\"#ref64\">Moctezuma et al., 2022\u003C/a>; \u003Ca href=\"#ref59\">Mahmoud et al., 2023\u003C/a>). With the development of deep learning algorithms, a variety of deep learning models have been developed and applied to EEG-based music-induced emotion recognition, including recurrent neural networks (RNN) (\u003Ca href=\"#ref23\">Dar et al., 2022\u003C/a>), long and short-term memory networks (LSTM) (\u003Ca href=\"#ref25\">Du et al., 2022\u003C/a>), gated recurrent neural networks (GRNN) (\u003Ca href=\"#ref18\">Weerakody et al., 2021\u003C/a>) and autoencoder (AE) (\u003Ca href=\"#ref55\">Liu et al., 2020\u003C/a>). The properties and applications of these reported deep learning algorithms are summarized in the following \u003Ca href=\"#tab7\">Table 7\u003C/a>.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Table 7\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg\" name=\"table7\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-t007.jpg\" alt=\"www.frontiersin.org\" id=\"tab7\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Table 7\u003C/b>. Properties of typical deep learning methods applied in music-induced emotion classification.\n\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Cp class=\"mb0\">The deep learning algorithms employed in EEG recognition of music-induced emotions demonstrate excellent generalization capabilities and data insensitivity, essential for the practical application of such emotions. While deep learning algorithms typically lack interpretability, recent advancements like GRNN (\u003Ca href=\"#ref18\">Weerakody et al., 2021\u003C/a>) and RNN (\u003Ca href=\"#ref23\">Dar et al., 2022\u003C/a>) can effectively capture the temporal aspects of EEG data, offering a novel approach to investigating the transient characteristics of music-induced emotions.\u003C/p>\n\u003Ch3>5.3 Model optimization and fusion strategies\u003C/h3>\n\u003Cp class=\"mb15\">Previous studies have demonstrated that classical machine learning algorithms as well as deep learning algorithms each possess their unique strengths and weaknesses in EEG-based music-induced emotion recognition research. To address the research and application requirements in related domains, researchers have investigated fusion strategies involving diverse algorithms.\u003C/p>\n\u003Cp class=\"mb15\">To enhance the precision of emotion recognition, researchers have delved into a hybrid deep learning framework combining gated recurrent unit (GRU) and CNN to leverage the strengths of both methodologies. The conventional GRU model is excellent in handling time series data, while the CNN model is adept at capturing spatial features within the data. During the implementation phase, researchers opted to retain all feature information outputted by the GRU and extract spatial information from the temporal features using the CNN model. Ultimately, they achieved a recognition average accuracy of 87.04% (\u003Ca href=\"#ref103\">Xu G. et al., 2023\u003C/a>). Based on the brain&#x2019;s functional network of emotional activity, researchers proposed a multi-feature fusion method combining energy activation, spatial distribution, and brain functional connectivity network features. In the study, the SVM model-based fusion of power activation features of differential entropy (DE), spatial features of common spatial patterns (CSP), five frequency features, and phase synchronization features of EEG phase-locked values (PLV) achieved classification results with an average accuracy around 85% (\u003Ca href=\"#ref68\">Pan et al., 2022\u003C/a>).\u003C/p>\n\u003Cp class=\"mb0\">To realize the fusion between different machine learning algorithms, it can be achieved by combining multiple basic classifiers for better performance, fusing different algorithmic training models for model fusion of multiple ones, and also by joint training of multiple neural network models for fusion of different algorithms, etc. In addition to these fusion approaches mentioned above, some researchers have also considered about the optimization method from a music-induced emotions perspective. A pentameter-based EEG music model was proposed. The model constructs a multi-channel EEG sensor network and records the EEG of individuals in various emotional states to establish a mapping library of EEG and emotions. Subsequently, the music pentameter model is employed to adaptively segment the time-domain EEG signal, transforming the EEG signal. The time-frequency features of the EEG signal, such as amplitude, contour, and signal frequency, are quantitatively represented in a normalized musical space (\u003Ca href=\"#ref52\">Li and Zheng, 2023\u003C/a>). The arithmetic modeling process is described as in \u003Ca href=\"#fig7\">Figure 7\u003C/a>.\u003C/p>\n\u003Cdiv class=\"DottedLine\">\u003C/div> \u003Cdiv class=\"Imageheaders\">Figure 7\u003C/div> \u003Cdiv class=\"FigureDesc\">\u003Ca href=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg\" name=\"figure7\" target=\"_blank\">\n  \u003Cpicture>\n    \u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=480&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg\" media=\"(max-width: 563px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=370&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg\" media=\"(max-width: 1024px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=290&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg\" media=\"(max-width: 1441px)\">\u003Csource type=\"image/webp\" srcset=\"https://images-provider.frontiersin.org/api/ipx/w=410&f=webp/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg\" media=\"\">\u003Csource type=\"image/jpg\" srcset=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg\" media=\"\"> \u003Cimg src=\"https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g007.jpg\" alt=\"www.frontiersin.org\" id=\"fig7\" loading=\"lazy\">\n  \u003C/picture>\n\u003C/a>\n\u003Cp>\u003Cb>Figure 7\u003C/b>. EEG musical staff model process flow chart.\u003C/p>\u003C/div> \u003Cdiv class=\"clear\">\u003C/div> \u003Cdiv class=\"DottedLine\">\u003C/div>\n\u003Ch3>5.4 Algorithm comparison and evaluation\u003C/h3>\n\u003Cp class=\"mb0\">Evaluating these various algorithms used for music-induced emotion EEG is very difficult. There is no clear consensus on an optimal algorithm, and several metrics can be employed to evaluate the selected algorithmic model to satisfy the requirements for various applications. The accuracy of emotion recognition is a fundamental evaluation metric, reflecting the model&#x2019;s performance in correctly predicting samples compared to the total number of samples and overall classifying emotions. Precision and recall, as evaluation metrics for binary classification problems, aid in assessing the model&#x2019;s performance across different categories. With the expansion of applications and the development of algorithms, developing new criteria is also an important part of future research.\u003C/p> \u003Ca id=\"h7\" name=\"h7\">\u003C/a>\n\u003Ch2>6 Application examples and analysis\u003C/h2>\n\u003Cp class=\"mb0\">EEG-based music emotion recognition has emerged as a multidisciplinary research area at the intersection of medicine, psychology, and computer science. This field explores the use of EEG signals to detect and classify the emotional responses evoked by music. The insights gained from EEG-based music emotion recognition have profound implications across various domains.\u003C/p>\n\u003Ch3>6.1 Music therapy\u003C/h3>\n\u003Cp class=\"mb0\">Music therapy is a therapeutic approach that harnesses the power of music and musical activities to promote both physical and mental well-being. The use of music for healing purposes has a long history, dating back to ancient civilizations like Greece, Rome, Egypt, and China. In more recent times, the formal recognition of music as a legitimate form of therapy began with the establishment of the American Music Therapy Association in 1944 (\u003Ca href=\"#ref92\">Taylor, 1981\u003C/a>). This marked a significant milestone in acknowledging music therapy as a valid and effective treatment modality within modern society. To broaden the scope of music therapy, the American Music Therapy Association took a significant step in 2005 by introducing the Research Strategic Priority (RSP) program. The primary objective of this initiative is to delve into the physiological evidence supporting the effectiveness of music therapy in both practical and theoretical contexts. In 2013, a team of researchers from Finland conducted a study to investigate the impact of music on the activity of frontotemporal areas during resting state in individuals with depression. The study utilized EEG-based recognition of music-induced emotions as a methodological approach (\u003Ca href=\"#ref29\">Fachner et al., 2013\u003C/a>). In 2018, a team of Spanish researchers conducted a study to evaluate the emotional response to music in patients with advanced cancer using EEG signals. The study aimed to demonstrate the positive emotional impact of music therapy on these patients (\u003Ca href=\"#ref73\">Ramirez et al., 2018\u003C/a>). In 2020, a team of Canadian researchers conducted a study to explore the potential of music-induced emotions in alleviating psycho-cognitive symptoms of Alzheimer&#x2019;s disease. The study involved combining EEG analysis methods to investigate how music activates the brain system, reduces negative emotions, and increases positive emotions. By analyzing EEG signals, the researchers were able to assess the emotional states induced by music. They found that music had a significant impact on the participants&#x2019; emotional well-being, with the potential to reduce negative emotions and increase positive emotions (\u003Ca href=\"#ref14\">Byrns et al., 2020\u003C/a>).\u003C/p>\n\u003Ch3>6.2 Neuroscience\u003C/h3>\n\u003Cp class=\"mb15\">Brain science is a rapidly growing field of research that offers valuable insights into human thinking, behavior, and consciousness. One area of study within brain science is the investigation of how music stimulates the brain, which has been recognized as a notable example of this research. In 1992, French and German scientists conducted a groundbreaking EEG analysis study to examine the effects of music stimulation on the brain. The study revealed a fascinating phenomenon: different types of music had varying impacts on the intensity of EEG signals across different frequency bands (\u003Ca href=\"#ref90\">Steinberg et al., 1992\u003C/a>). In 2016, a team of Indian researchers conducted a study using EEG to investigate the effects of Hindustani music on brain activity in a relaxed state. The results of the study revealed that Hindustani music had a significant effect on the listeners&#x2019; arousal levels in all activities stimulated by the music. The EEG analysis indicated an increase in brain activity in response to the music, suggesting that it had a stimulating effect on the listeners (\u003Ca href=\"#ref11\">Banerjee et al., 2016\u003C/a>). In 2019, a group of Indian scholars delved into research on the reverse inversion of brain sounds by utilizing Hindustani classical music. They recognized the profound emotional impact of this music and sought to explore the correlation between EEG signals and musical stimuli. By leveraging the real-time recording capability of EEG, researchers from the fields of psychology and neurology conducted studies to analyze the neural mechanisms underlying the stimulation of music, both in positive and negative contexts. These investigations have significantly contributed to the advancement of brain science research (\u003Ca href=\"#ref82\">Sanyal et al., 2019\u003C/a>).\u003C/p>\n\u003Cp class=\"mb0\">In the early stages, EEG, as a direct signal of brain activity, was employed by neuroscientists to conduct exploratory studies on functional brain regions associated with impaired musical ability caused by brain dysfunction. This utilization of EEG monitoring technology has played a pivotal role in advancing our understanding of the brain&#x2019;s mechanisms involved in music processing (\u003Ca href=\"#ref99\">Vuust et al., 2022\u003C/a>). These initial findings laid the technical groundwork for subsequent research on EEG-based music emotion recognition. With a focus on music-induced emotions, researchers have endeavored to further investigate the realm of music-induced emotions using EEG technology (\u003Ca href=\"#ref34\">Gomez-Canon et al., 2021\u003C/a>). From the perspective of music therapy, the utilization of EEG signals offers direct evidence regarding the process of music-induced emotions. By analyzing EEG signals from various brain regions corresponding to different emotions, researchers can obtain more detailed physiological information that aids in the interpretation of the brain mechanisms involved in music therapy. This application of EEG signals provides valuable insights into understanding the effects of music on emotional states and enhances our knowledge of the therapeutic potential of music (\u003Ca href=\"#ref14\">Byrns et al., 2020\u003C/a>; \u003Ca href=\"#ref30\">Fedotchev et al., 2022\u003C/a>).\u003C/p>\n\u003Ch3>6.3 Others\u003C/h3>\n\u003Cp class=\"mb15\">Emotions play a crucial role in human experiences, behaviors, health, and social interactions. Music, a language of the human mind, has the power to vividly and imaginatively express various emotions such as happiness, sadness, and more, and can greatly influence listeners&#x2019; emotional state. In recent years, there has been significant progress in understanding music-induced emotions and their psychological and neurological mechanisms.\u003C/p>\n\u003Cp class=\"mb15\">In clinical medicine, this research can contribute to the development of personalized music therapy interventions for mental health disorders, neurorehabilitation, and stress management. It can also aid in diagnosing and monitoring emotional disorders such as depression and anxiety. In the realm of brain science, EEG-based music emotion recognition provides valuable insights into the neural mechanisms underlying emotional processing and music perception. These findings can enhance our understanding of how the brain responds to music and its impact on emotional well-being. Moreover, in the field of music information, this research can improve music recommendation systems, enhance user experiences, and facilitate music healing approaches. By tailoring music selections based on an individual&#x2019;s emotional responses, music platforms can offer personalized and therapeutic listening experiences. Overall, EEG-based music emotion recognition holds immense potential for diverse applications in fields like clinical medicine, brain science, and music information. It represents a promising avenue for advancing our understanding of the complex relationship between music and emotions and harnessing music&#x2019;s therapeutic benefits.\u003C/p>\n\u003Cp class=\"mb0\">Furthermore, for some long-term music healing processes, the real-time sensitivity of EEG to emotional signals induced by music stimulation can provide evidence for the effectiveness of certain therapeutic methods. This evidence can facilitate the development, correction, and smooth dissemination of related therapeutic techniques. By monitoring changes in EEG signals throughout the music therapy process, researchers can evaluate the effectiveness of different therapeutic methods and fine-tune them accordingly. This approach enhances the precision and efficacy of music therapy, allowing for optimized treatment plans that cater to individual needs (\u003Ca href=\"#ref14\">Byrns et al., 2020\u003C/a>). For music researchers, the individual variability in EEG emotion detection allows for personalized categorization and annotation of musical emotions. This capability is crucial not only for music composition and information retrieval but also for guiding the development of more immersive multimedia environments. By leveraging EEG data to understand how individuals uniquely experience and respond to musical emotions, researchers can enhance the creation of tailored musical experiences and enrich the design of multimedia environments that resonate with diverse emotional responses (\u003Ca href=\"#ref110\">Yu et al., 2022\u003C/a>).\u003C/p> \u003Ca id=\"h8\" name=\"h8\">\u003C/a>\n\u003Ch2>7 Discussion and conclusions\u003C/h2>\n\u003Cp class=\"mb15\">Based on the mentioned model, researchers were able to carry out systematic research on the study of the emotional impacts of the same music on different listeners, the study of the emotional impact of various types of music on the same listener, and the key parameters of music-stimulated emotions. Previous researchers have conducted various studies in terms of music-induced emotion classification models, music-induced datasets, training and classification of emotion models, and so on.\u003C/p>\n\u003Cp class=\"mb15\">As an interdisciplinary challenge, research on EEG-based music-induced emotion recognition has emerged as a valuable approach for real-time and effective assessment of emotional responses to music. This innovative research not only offers new technical tools for studying music-related emotions but also provides a controllable research paradigm applicable to brain science and other fields. In recent years, researchers from various disciplines have made significant progress in addressing this complex issue. By approaching the problem from different perspectives, they have achieved notable results. However, during these investigations, several limitations have also been identified.\u003C/p>\n\u003Cp class=\"mb15\">Compared to other signals commonly used for music emotion recognition, such as audio signals, facial expressions, heart rate, and respiration, EEG signals have distinct advantages. EEG signals belong to physiological signals of the central nervous system, which are typically not under conscious control. Consequently, they can provide information about the current emotional state of an individual that cannot be deliberately concealed. Furthermore, EEG signals offer several benefits when compared to other methods of detecting physiological signals of the central nervous system. EEG is a relatively mature technology that has been extensively studied and validated. It is also portable, non-invasive, and cost-effective, making it practical for use in various research and real-world settings.\u003C/p>\n\u003Cp class=\"mb15\">As EEG monitoring hardware and recognition algorithm software technology continue to evolve, the advantages of personalization, real-time analysis, and the convenience of using EEG to recognize music-induced emotions will be further explored in various application fields. The growing sophistication of EEG technology opens up new possibilities for research and practical implementation of music-based therapies, multimedia environments, and personalized music experiences. As such, the continued development and refinement of EEG-based music emotion recognition has the potential to revolutionize our understanding of the impact of music on human emotions and behavior. Advancements in EEG monitoring hardware and recognition algorithm software technology have opened up new avenues for exploring the potential applications of EEG-based music-induced emotion recognition. With these technological improvements, the advantages of personalization, real-time, and convenience in recognizing music-induced emotions through EEG can be further explored in various fields.\u003C/p>\n\u003Cp class=\"mb15\">At present, the labeling basis of the training set in EEG emotion recognition algorithms largely relies on psychological scales and emotion labels from databases. However, these conventional labeling methods are inherently subjective and discrete. Therefore, there is a pressing need for extensive research to establish a standardized library of emotions based on EEG signals themselves. To address this challenge, musicologists from diverse cultural backgrounds have embarked on initial research into the emotional labeling of music within their respective cultures. As the accuracy of EEG signals for emotion recognition continues to improve, there has been increasing mention of establishing direct EEG signal discrimination for personalized emotion recognition. This advancement holds promise for enhancing our understanding of how individuals from different cultural backgrounds experience and interpret emotions in music, paving the way for more nuanced and culturally sensitive approaches to music emotion recognition (\u003Ca href=\"#ref26\">Du et al., 2023\u003C/a>).\u003C/p>\n\u003Cp class=\"mb15\">Improving the accuracy of music-induced emotion recognition can be a challenging problem that demands long-term research, and the advent of deep learning algorithms in recent years has provided a more effective means of addressing this challenge compared to traditional machine learning approaches. With deep learning algorithms, researchers can leverage large amounts of data to train neural networks that can learn to recognize complex patterns and relationships in music-induced emotions. This approach has shown great promise in improving the accuracy of music emotion recognition, allowing researchers to gain deeper insights into how music affects human emotions and behavior. However, ongoing research and development are still required to further refine and optimize these algorithms for use in practical applications (\u003Ca href=\"#ref70\">Pandey and Seeja, 2022\u003C/a>). As artificial intelligence algorithms continue to undergo continuous optimization and enhancement, new concepts, approaches, and research findings will undoubtedly emerge, offering fresh perspectives and advancing the field of music-induced emotion recognition.\u003C/p>\n\u003Cp class=\"mb0\">The research and development of music-induced emotion recognition based on EEG relies on the continuous expansion of the application field for such technology. Currently, there are some notable examples of music-induced emotion applications in clinical treatment (\u003Ca href=\"#ref14\">Byrns et al., 2020\u003C/a>), neuroscience (\u003Ca href=\"#ref57\">Luo et al., 2023\u003C/a>), and music information retrieval (\u003Ca href=\"#ref51\">Li et al., 2023\u003C/a>). However, there is still a need for further development of related technical products that can be scaled up and made accessible to the general public, allowing them to better understand and benefit from this technology. This requires ongoing efforts to bridge the gap between research and practical implementation, fostering the creation of user-friendly tools and platforms that can effectively harness the potential of music-induced emotion recognition for broader applications and public engagement. In the realm of consumer applications, there is still much to be explored regarding the potential combination of EEG and personalized music to develop emotional regulation technology and products for users.\u003C/p> \u003Ca id=\"h9\" name=\"h9\">\u003C/a>\n\u003Ch2>Author contributions\u003C/h2>\n\u003Cp class=\"mb0\">YS: Conceptualization, Data curation, Funding acquisition, Investigation, Methodology, Writing &#x2013; original draft, Writing &#x2013; review &#x00026; editing. YL: Resources, Supervision, Validation, Writing &#x2013; original draft. YX: Data curation, Resources, Supervision, Validation, Writing &#x2013; review &#x00026; editing. JM: Formal analysis, Visualization, Writing &#x2013; original draft. DL: Conceptualization, Formal analysis, Project administration, Writing &#x2013; original draft, Writing &#x2013; review &#x00026; editing.\u003C/p> \u003Ca id=\"h10\" name=\"h10\">\u003C/a>\n\u003Ch2>Funding\u003C/h2>\n\u003Cp class=\"mb0\">The author(s) declare that financial support was received for the research, authorship, and/or publication of this article. This study was supported by the Zhejiang International Studies University&#x2019;s key project &#x201C;A comparative study of Chinese and English art education from the perspective of nationalization&#x201D; (No. 090500112016), the first-class curriculum construction project from the Department of Education of Zhejiang Province (No. 080830302022), and Zhejiang Natural Science Foundation project (No. LQ21E060006).\u003C/p> \u003Ca id=\"h11\" name=\"h11\">\u003C/a>\n\u003Ch2>Conflict of interest\u003C/h2>\n\u003Cp class=\"mb0\">The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\u003C/p> \u003Ca id=\"h12\" name=\"h12\">\u003C/a>\n\u003Ch2>Publisher&#x2019;s note\u003C/h2>\n\u003Cp class=\"mb0\">All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.\u003C/p> \u003Ca id=\"h13\" name=\"h13\">\u003C/a>\n\u003Ch2>References\u003C/h2>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref1\" id=\"ref1\">\u003C/a>Aftanas, L. I., Reva, N. V., Savotina, L. N., and Makhnev, V. P. (2006). Neurophysiological correlates of induced discrete emotions in humans: an individually oriented analysis. \u003Ci>Neurosci. Behav. Physiol.\u003C/i> 36, 119&#x2013;130. doi: 10.1007/s11055-005-0170-6 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/16380825\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1007/s11055-005-0170-6\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=L.+I.+Aftanas&#x00026;author=N.+V.+Reva&#x00026;author=L.+N.+Savotina&#x00026;author=V.+P.+Makhnev&#x00026;publication_year=2006&#x00026;title=Neurophysiological+correlates+of+induced+discrete+emotions+in+humans:+an+individually+oriented+analysis&#x00026;journal=Neurosci.+Behav.+Physiol.&#x00026;volume=36&#x00026;pages=119-130\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref2\" id=\"ref2\">\u003C/a>Ahmad, J., Ellis, C., Leech, R., Voytek, B., Garces, P., Jones, E., et al. (2022). From mechanisms to markers: novel noninvasive EEG proxy markers of the neural excitation and inhibition system in humans. \u003Ci>Transl. Psychiatry\u003C/i> 12:467. doi: 10.1038/s41398-022-02218-z \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/36344497\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1038/s41398-022-02218-z\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Ahmad&#x00026;author=C.+Ellis&#x00026;author=R.+Leech&#x00026;author=B.+Voytek&#x00026;author=P.+Garces&#x00026;author=E.+Jones&#x00026;publication_year=2022&#x00026;title=From+mechanisms+to+markers:+novel+noninvasive+EEG+proxy+markers+of+the+neural+excitation+and+inhibition+system+in+humans&#x00026;journal=Transl.+Psychiatry&#x00026;volume=12&#x00026;pages=467\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref3\" id=\"ref3\">\u003C/a>Alarcao, S. M., and Fonseca, M. J. (2019). Emotions recognition using EEG signals: a survey. \u003Ci>IEEE Trans. Affect. Comput.\u003C/i> 10, 374&#x2013;393. doi: 10.1109/TAFFC.2017.2714671\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1109/TAFFC.2017.2714671\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=S.+M.+Alarcao&#x00026;author=M.+J.+Fonseca&#x00026;publication_year=2019&#x00026;title=Emotions+recognition+using+EEG+signals:+a+survey&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=10&#x00026;pages=374-393\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref4\" id=\"ref4\">\u003C/a>Aldridge, A., Barnes, E., Bethel, C. L., Carruth, D. W., Kocturova, M., Pleva, M., et al. (2019). &#x201C;Accessible electroencephalograms (EEGs): a comparative review with OpenBCI&#x2019;s ultracortex mark IV headset&#x201D; in \u003Ci>2019 29th international conference radioelektronika (RADIOELEKTRONIKA)\u003C/i> (Pardubice, Czech Republic: IEEE), 1&#x2013;6.\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Aldridge&#x00026;author=E.+Barnes&#x00026;author=C.+L.+Bethel&#x00026;author=D.+W.+Carruth&#x00026;author=M.+Kocturova&#x00026;author=M.+Pleva&#x00026;publication_year=2019&#x00026;title=Accessible+electroencephalograms+(EEGs):+a+comparative+review+with+OpenBCI&#x2019;s+ultracortex+mark+IV+headset&#x00026;journal=2019+29th+international+conference+radioelektronika+(RADIOELEKTRONIKA)&#x00026;pages=1-6\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref5\" id=\"ref5\">\u003C/a>Aljanaki, A., Yang, Y.-H., and Soleymani, M. (2017). Developing a benchmark for emotional analysis of music. \u003Ci>PLoS One\u003C/i> 12:e0173392. doi: 10.1371/journal.pone.0173392 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/28282400\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1371/journal.pone.0173392\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Aljanaki&#x00026;author=Y.-H.+Yang&#x00026;author=M.+Soleymani&#x00026;publication_year=2017&#x00026;title=Developing+a+benchmark+for+emotional+analysis+of+music&#x00026;journal=PLoS+One&#x00026;volume=12&#x00026;pages=e0173392\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref6\" id=\"ref6\">\u003C/a>Apicella, A., Arpaia, P., Frosolone, M., Improta, G., Moccaldi, N., and Pollastro, A. (2022a). EEG-based measurement system for monitoring student engagement in learning 4.0. \u003Ci>Sci. Rep.\u003C/i> 12:5857. doi: 10.1038/s41598-022-09578-y \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/35393470\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1038/s41598-022-09578-y\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Apicella&#x00026;author=P.+Arpaia&#x00026;author=M.+Frosolone&#x00026;author=G.+Improta&#x00026;author=N.+Moccaldi&#x00026;author=A.+Pollastro&#x00026;publication_year=2022a&#x00026;title=EEG-based+measurement+system+for+monitoring+student+engagement+in+learning+4.0&#x00026;journal=Sci.+Rep.&#x00026;volume=12&#x00026;pages=5857\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref7\" id=\"ref7\">\u003C/a>Apicella, A., Arpaia, P., Isgro, F., Mastrati, G., and Moccaldi, N. (2022b). A survey on EEG-based solutions for emotion recognition with a low number of channels. \u003Ci>IEEE Access\u003C/i> 10, 117411&#x2013;117428. doi: 10.1109/ACCESS.2022.3219844\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1109/ACCESS.2022.3219844\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Apicella&#x00026;author=P.+Arpaia&#x00026;author=F.+Isgro&#x00026;author=G.+Mastrati&#x00026;author=N.+Moccaldi&#x00026;publication_year=2022b&#x00026;title=A+survey+on+EEG-based+solutions+for+emotion+recognition+with+a+low+number+of+channels&#x00026;journal=IEEE+Access&#x00026;volume=10&#x00026;pages=117411-117428\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref8\" id=\"ref8\">\u003C/a>Ara, A., and Marco-Pallar&#x000E9;s, J. (2020). Fronto-temporal theta phase-synchronization underlies music-evoked pleasantness. \u003Ci>NeuroImage\u003C/i> 212:116665. doi: 10.1016/j.neuroimage.2020.116665 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/32087373\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1016/j.neuroimage.2020.116665\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Ara&#x00026;author=J.+Marco-Pallar&#x000E9;s&#x00026;publication_year=2020&#x00026;title=Fronto-temporal+theta+phase-synchronization+underlies+music-evoked+pleasantness&#x00026;journal=NeuroImage&#x00026;volume=212&#x00026;pages=116665\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref9\" id=\"ref9\">\u003C/a>Bagherzadeh, S., Maghooli, K., Shalbaf, A., and Maghsoudi, A. (2023). A hybrid EEG-based emotion recognition approach using wavelet convolutional neural networks and support vector machine. \u003Ci>Basic Clin. Neurosci.\u003C/i> 14, 87&#x2013;102. doi: 10.32598/bcn.2021.3133.1 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/37346875\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.32598/bcn.2021.3133.1\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=S.+Bagherzadeh&#x00026;author=K.+Maghooli&#x00026;author=A.+Shalbaf&#x00026;author=A.+Maghsoudi&#x00026;publication_year=2023&#x00026;title=A+hybrid+EEG-based+emotion+recognition+approach+using+wavelet+convolutional+neural+networks+and+support+vector+machine&#x00026;journal=Basic+Clin.+Neurosci.&#x00026;volume=14&#x00026;pages=87-102\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref10\" id=\"ref10\">\u003C/a>Balasubramanian, G., Kanagasabai, A., Mohan, J., and Seshadri, N. P. G. (2018). Music induced emotion using wavelet packet decomposition&#x2014;an EEG study. \u003Ci>Biomed. Signal Process. Control\u003C/i> 42, 115&#x2013;128. doi: 10.1016/j.bspc.2018.01.015\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.bspc.2018.01.015\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=G.+Balasubramanian&#x00026;author=A.+Kanagasabai&#x00026;author=J.+Mohan&#x00026;author=N.+P.+G.+Seshadri&#x00026;publication_year=2018&#x00026;title=Music+induced+emotion+using+wavelet+packet+decomposition&#x2014;an+EEG+study&#x00026;journal=Biomed.+Signal+Process.+Control&#x00026;volume=42&#x00026;pages=115-128\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref11\" id=\"ref11\">\u003C/a>Banerjee, A., Sanyal, S., Patranabis, A., Banerjee, K., Guhathakurta, T., Sengupta, R., et al. (2016). Study on brain dynamics by non linear analysis of music induced EEG signals. \u003Ci>Phys. A Stat. Mech. Appl.\u003C/i> 444, 110&#x2013;120. doi: 10.1016/j.physa.2015.10.030\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.physa.2015.10.030\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Banerjee&#x00026;author=S.+Sanyal&#x00026;author=A.+Patranabis&#x00026;author=K.+Banerjee&#x00026;author=T.+Guhathakurta&#x00026;author=R.+Sengupta&#x00026;publication_year=2016&#x00026;title=Study+on+brain+dynamics+by+non+linear+analysis+of+music+induced+EEG+signals&#x00026;journal=Phys.+A+Stat.+Mech.+Appl.&#x00026;volume=444&#x00026;pages=110-120\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref12\" id=\"ref12\">\u003C/a>Bergee, M. J., and Weingarten, K. M. (2021). Multilevel models of the relationship between music achievement and reading and math achievement. \u003Ci>J. Res. Music. Educ.\u003C/i> 68, 398&#x2013;418. doi: 10.1177/0022429420941432\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1177/0022429420941432\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+J.+Bergee&#x00026;author=K.+M.+Weingarten&#x00026;publication_year=2021&#x00026;title=Multilevel+models+of+the+relationship+between+music+achievement+and+reading+and+math+achievement&#x00026;journal=J.+Res.+Music.+Educ.&#x00026;volume=68&#x00026;pages=398-418\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref13\" id=\"ref13\">\u003C/a>Brotzer, J. M., Mosqueda, E. R., and Gorro, K. (2019). Predicting emotion in music through audio pattern analysis. \u003Ci>IOP Conf. Ser. Mater. Sci. Eng.\u003C/i> 482:12021. doi: 10.1088/1757-899X/482/1/012021\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1088/1757-899X/482/1/012021\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+M.+Brotzer&#x00026;author=E.+R.+Mosqueda&#x00026;author=K.+Gorro&#x00026;publication_year=2019&#x00026;title=Predicting+emotion+in+music+through+audio+pattern+analysis&#x00026;journal=IOP+Conf.+Ser.+Mater.+Sci.+Eng.&#x00026;volume=482&#x00026;pages=12021\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref14\" id=\"ref14\">\u003C/a>Byrns, A., Abdessalem, H. B., Cuesta, M., Bruneau, M.-A., Belleville, S., and Frasson, C. (2020). EEG analysis of the contribution of music therapy and virtual reality to the improvement of cognition in Alzheimer&#x2019;s disease. \u003Ci>J. Biomed. Sci. Eng.\u003C/i> 13, 187&#x2013;201. doi: 10.4236/jbise.2020.138018\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.4236/jbise.2020.138018\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Byrns&#x00026;author=H.+B.+Abdessalem&#x00026;author=M.+Cuesta&#x00026;author=M.-A.+Bruneau&#x00026;author=S.+Belleville&#x00026;author=C.+Frasson&#x00026;publication_year=2020&#x00026;title=EEG+analysis+of+the+contribution+of+music+therapy+and+virtual+reality+to+the+improvement+of+cognition+in+Alzheimer&#x2019;s+disease&#x00026;journal=J.+Biomed.+Sci.+Eng.&#x00026;volume=13&#x00026;pages=187-201\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref15\" id=\"ref15\">\u003C/a>Cai, Q., Cui, G.-C., and Wang, H.-X. (2022). EEG-based emotion recognition using multiple kernel learning. \u003Ci>Mach. Intell. Res.\u003C/i> 19, 472&#x2013;484. doi: 10.1007/s11633-022-1352-1\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1007/s11633-022-1352-1\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=Q.+Cai&#x00026;author=G.-C.+Cui&#x00026;author=H.-X.+Wang&#x00026;publication_year=2022&#x00026;title=EEG-based+emotion+recognition+using+multiple+kernel+learning&#x00026;journal=Mach.+Intell.+Res.&#x00026;volume=19&#x00026;pages=472-484\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref16\" id=\"ref16\">\u003C/a>Carpente, J., Casenhiser, D. M., Kelliher, M., Mulholland, J., Sluder, H. L., Crean, A., et al. (2022). The impact of imitation on engagement in minimally verbal children with autism during improvisational music therapy. \u003Ci>Nord. J. Music. Ther.\u003C/i> 31, 44&#x2013;62. doi: 10.1080/08098131.2021.1924843\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1080/08098131.2021.1924843\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Carpente&#x00026;author=D.+M.+Casenhiser&#x00026;author=M.+Kelliher&#x00026;author=J.+Mulholland&#x00026;author=H.+L.+Sluder&#x00026;author=A.+Crean&#x00026;publication_year=2022&#x00026;title=The+impact+of+imitation+on+engagement+in+minimally+verbal+children+with+autism+during+improvisational+music+therapy&#x00026;journal=Nord.+J.+Music.+Ther.&#x00026;volume=31&#x00026;pages=44-62\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref17\" id=\"ref17\">\u003C/a>Chen, Y.-A., Yang, Y.-H., Wang, J.-C., and Chen, H. (2015). The AMG1608 dataset for music emotion recognition. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), (South Brisbane, Queensland, Australia: IEEE), 693&#x2013;697.\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"http://scholar.google.com/scholar_lookup?author=Y.-A.+Chen&#x00026;author=Y.-H.+Yang&#x00026;author=J.-C.+Wang&#x00026;author=H.+Chen&#x00026;publication_year=2015&#x00026;title=The+AMG1608+dataset+for+music+emotion+recognition&#x00026;pages=693-697\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref19\" id=\"ref19\">\u003C/a>Colin, C., Prince, V., Bensoussan, J.-L., and Picot, M.-C. (2023). Music therapy for health workers to reduce stress, mental workload and anxiety: a systematic review. \u003Ci>J. Public Health\u003C/i> 45, e532&#x2013;e541. doi: 10.1093/pubmed/fdad059 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/37147921\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1093/pubmed/fdad059\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=C.+Colin&#x00026;author=V.+Prince&#x00026;author=J.-L.+Bensoussan&#x00026;author=M.-C.+Picot&#x00026;publication_year=2023&#x00026;title=Music+therapy+for+health+workers+to+reduce+stress+mental+workload+and+anxiety:+a+systematic+review&#x00026;journal=J.+Public+Health&#x00026;volume=45&#x00026;pages=e532-e541\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref20\" id=\"ref20\">\u003C/a>Contreras-Molina, M., Rueda-N&#x000FA;&#x000F1;ez, A., P&#x000E9;rez-Collado, M. L., and Garc&#x000ED;a-Maestro, A. (2021). Effect of music therapy on anxiety and pain in the critical polytraumatised patient. \u003Ci>Enferm. Intensiva\u003C/i> 32, 79&#x2013;87. doi: 10.1016/j.enfie.2020.03.005 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/34099268\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1016/j.enfie.2020.03.005\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Contreras-Molina&#x00026;author=A.+Rueda-N&#x000FA;&#x000F1;ez&#x00026;author=M.+L.+P&#x000E9;rez-Collado&#x00026;author=A.+Garc&#x000ED;a-Maestro&#x00026;publication_year=2021&#x00026;title=Effect+of+music+therapy+on+anxiety+and+pain+in+the+critical+polytraumatised+patient&#x00026;journal=Enferm.+Intensiva&#x00026;volume=32&#x00026;pages=79-87\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref21\" id=\"ref21\">\u003C/a>Cui, X., Wu, Y., Wu, J., You, Z., Xiahou, J., and Ouyang, M. (2022). A review: music-emotion recognition and analysis based on EEG signals. \u003Ci>Front. Neuroinform.\u003C/i> 16:997282. doi: 10.3389/fninf.2022.997282 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/36387584\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3389/fninf.2022.997282\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=X.+Cui&#x00026;author=Y.+Wu&#x00026;author=J.+Wu&#x00026;author=Z.+You&#x00026;author=J.+Xiahou&#x00026;author=M.+Ouyang&#x00026;publication_year=2022&#x00026;title=A+review:+music-emotion+recognition+and+analysis+based+on+EEG+signals&#x00026;journal=Front.+Neuroinform.&#x00026;volume=16&#x00026;pages=997282\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref22\" id=\"ref22\">\u003C/a>Dadebayev, D., Goh, W. W., and Tan, E. X. (2022). EEG-based emotion recognition: review of commercial EEG devices and machine learning techniques. \u003Ci>J. King Saud Univ. Comput. Inf. Sci.\u003C/i> 34, 4385&#x2013;4401. doi: 10.1016/j.jksuci.2021.03.009\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.jksuci.2021.03.009\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=D.+Dadebayev&#x00026;author=W.+W.+Goh&#x00026;author=E.+X.+Tan&#x00026;publication_year=2022&#x00026;title=EEG-based+emotion+recognition:+review+of+commercial+EEG+devices+and+machine+learning+techniques&#x00026;journal=J.+King+Saud+Univ.+Comput.+Inf.+Sci.&#x00026;volume=34&#x00026;pages=4385-4401\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref23\" id=\"ref23\">\u003C/a>Dar, M., Akram, M., Yuvaraj, R., Khawaja, S., and Murugappan, M. (2022). EEG-based emotion charting for Parkinson&#x2019;s disease patients using convolutional recurrent neural networks and cross dataset learning. \u003Ci>Comput. Biol. Med.\u003C/i> 144:105327. doi: 10.1016/j.compbiomed.2022.105327 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/35303579\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1016/j.compbiomed.2022.105327\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Dar&#x00026;author=M.+Akram&#x00026;author=R.+Yuvaraj&#x00026;author=S.+Khawaja&#x00026;author=M.+Murugappan&#x00026;publication_year=2022&#x00026;title=EEG-based+emotion+charting+for+Parkinson&#x2019;s+disease+patients+using+convolutional+recurrent+neural+networks+and+cross+dataset+learning&#x00026;journal=Comput.+Biol.+Med.&#x00026;volume=144&#x00026;pages=105327\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref24\" id=\"ref24\">\u003C/a>Davidson, R. J., and Fox, N. A. (1982). Asymmetrical brain activity discriminates between positive and negative affective stimuli in human infants. \u003Ci>Science\u003C/i> 218, 1235&#x2013;1237. doi: 10.1126/science.7146906 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/7146906\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1126/science.7146906\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=R.+J.+Davidson&#x00026;author=N.+A.+Fox&#x00026;publication_year=1982&#x00026;title=Asymmetrical+brain+activity+discriminates+between+positive+and+negative+affective+stimuli+in+human+infants&#x00026;journal=Science&#x00026;volume=218&#x00026;pages=1235-1237\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref25\" id=\"ref25\">\u003C/a>Du, X., Ma, C., Zhang, G., Li, J., Lai, Y.-K., Zhao, G., et al. (2022). An efficient LSTM network for emotion recognition from multichannel EEG signals. \u003Ci>IEEE Trans. Affect. Comput.\u003C/i> 13, 1528&#x2013;1540. doi: 10.1109/TAFFC.2020.3013711\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1109/TAFFC.2020.3013711\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=X.+Du&#x00026;author=C.+Ma&#x00026;author=G.+Zhang&#x00026;author=J.+Li&#x00026;author=Y.-K.+Lai&#x00026;author=G.+Zhao&#x00026;publication_year=2022&#x00026;title=An+efficient+LSTM+network+for+emotion+recognition+from+multichannel+EEG+signals&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=13&#x00026;pages=1528-1540\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref26\" id=\"ref26\">\u003C/a>Du, R., Zhu, S., Ni, H., Mao, T., Li, J., and Wei, R. (2023). Valence-arousal classification of emotion evoked by Chinese ancient-style music using 1D-CNN-BiLSTM model on EEG signals for college students. \u003Ci>Multimed. Tools Appl.\u003C/i> 82, 15439&#x2013;15456. doi: 10.1007/s11042-022-14011-7 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/36213341\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1007/s11042-022-14011-7\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=R.+Du&#x00026;author=S.+Zhu&#x00026;author=H.+Ni&#x00026;author=T.+Mao&#x00026;author=J.+Li&#x00026;author=R.+Wei&#x00026;publication_year=2023&#x00026;title=Valence-arousal+classification+of+emotion+evoked+by+Chinese+ancient-style+music+using+1D-CNN-BiLSTM+model+on+EEG+signals+for+college+students&#x00026;journal=Multimed.+Tools+Appl.&#x00026;volume=82&#x00026;pages=15439-15456\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref27\" id=\"ref27\">\u003C/a>Egger, M., Ley, M., and Hanke, S. (2019). Emotion recognition from physiological signal analysis: a review. \u003Ci>Electron. Notes Theor. Comput. Sci.\u003C/i> 343, 35&#x2013;55. doi: 10.1016/j.entcs.2019.04.009\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.entcs.2019.04.009\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Egger&#x00026;author=M.+Ley&#x00026;author=S.+Hanke&#x00026;publication_year=2019&#x00026;title=Emotion+recognition+from+physiological+signal+analysis:+a+review&#x00026;journal=Electron.+Notes+Theor.+Comput.+Sci.&#x00026;volume=343&#x00026;pages=35-55\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref28\" id=\"ref28\">\u003C/a>Er, M. B., &#x000C7;i&#x011F;, H., and Aydilek, &#x0130;. B. (2021). A new approach to recognition of human emotions using brain signals and music stimuli. \u003Ci>Appl. Acoust.\u003C/i> 175:107840. doi: 10.1016/j.apacoust.2020.107840\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.apacoust.2020.107840\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+B.+Er&#x00026;author=H.+&#x000C7;i&#x011F;&#x00026;author=&#x0130;.+B.+Aydilek&#x00026;publication_year=2021&#x00026;title=A+new+approach+to+recognition+of+human+emotions+using+brain+signals+and+music+stimuli&#x00026;journal=Appl.+Acoust.&#x00026;volume=175&#x00026;pages=107840\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref29\" id=\"ref29\">\u003C/a>Fachner, J., Gold, C., and Erkkil&#x000E4;, J. (2013). Music therapy modulates fronto-temporal activity in rest-EEG in depressed clients. \u003Ci>Brain Topogr.\u003C/i> 26, 338&#x2013;354. doi: 10.1007/s10548-012-0254-x \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/22983820\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1007/s10548-012-0254-x\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Fachner&#x00026;author=C.+Gold&#x00026;author=J.+Erkkil&#x000E4;&#x00026;publication_year=2013&#x00026;title=Music+therapy+modulates+fronto-temporal+activity+in+rest-EEG+in+depressed+clients&#x00026;journal=Brain+Topogr.&#x00026;volume=26&#x00026;pages=338-354\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref30\" id=\"ref30\">\u003C/a>Fedotchev, A., Parin, S., Polevaya, S., and Zemlianaia, A. (2022). EEG-based musical neurointerfaces in the correction of stress-induced states. \u003Ci>Brain-Comput. Interfaces\u003C/i> 9, 1&#x2013;6. doi: 10.1080/2326263X.2021.1964874\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1080/2326263X.2021.1964874\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Fedotchev&#x00026;author=S.+Parin&#x00026;author=S.+Polevaya&#x00026;author=A.+Zemlianaia&#x00026;publication_year=2022&#x00026;title=EEG-based+musical+neurointerfaces+in+the+correction+of+stress-induced+states&#x00026;journal=Brain-Comput.+Interfaces&#x00026;volume=9&#x00026;pages=1-6\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref31\" id=\"ref31\">\u003C/a>Gan, K., Li, R., Zhang, J., Sun, Z., and Yin, Z. (2024). Instantaneous estimation of momentary affective responses using neurophysiological signals and a spatiotemporal emotional intensity regression network. \u003Ci>Neural Netw.\u003C/i> 172:106080. doi: 10.1016/j.neunet.2023.12.034 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/38160622\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1016/j.neunet.2023.12.034\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=K.+Gan&#x00026;author=R.+Li&#x00026;author=J.+Zhang&#x00026;author=Z.+Sun&#x00026;author=Z.+Yin&#x00026;publication_year=2024&#x00026;title=Instantaneous+estimation+of+momentary+affective+responses+using+neurophysiological+signals+and+a+spatiotemporal+emotional+intensity+regression+network&#x00026;journal=Neural+Netw.&#x00026;volume=172&#x00026;pages=106080\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref32\" id=\"ref32\">\u003C/a>Geipel, J., Koenig, J., Hillecke, T. K., and Resch, F. (2022). Short-term music therapy treatment for adolescents with depression &#x2013; a pilot study. \u003Ci>Art. Psychother.\u003C/i> 77:101874. doi: 10.1016/j.aip.2021.101874\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.aip.2021.101874\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Geipel&#x00026;author=J.+Koenig&#x00026;author=T.+K.+Hillecke&#x00026;author=F.+Resch&#x00026;publication_year=2022&#x00026;title=Short-term+music+therapy+treatment+for+adolescents+with+depression+&#x2013;+a+pilot+study&#x00026;journal=Art.+Psychother.&#x00026;volume=77&#x00026;pages=101874\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref33\" id=\"ref33\">\u003C/a>Geretsegger, M., Fusar-Poli, L., Elefant, C., M&#x000F6;ssler, K. A., Vitale, G., and Gold, C. (2022). Music therapy for autistic people. \u003Ci>Cochrane Database Syst. Rev.\u003C/i> 2022:CD004381. doi: 10.1002/14651858.CD004381.pub4 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/35532041\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1002/14651858.CD004381.pub4\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Geretsegger&#x00026;author=L.+Fusar-Poli&#x00026;author=C.+Elefant&#x00026;author=K.+A.+M&#x000F6;ssler&#x00026;author=G.+Vitale&#x00026;author=C.+Gold&#x00026;publication_year=2022&#x00026;title=Music+therapy+for+autistic+people&#x00026;journal=Cochrane+Database+Syst.+Rev.&#x00026;volume=2022&#x00026;pages=CD004381\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref34\" id=\"ref34\">\u003C/a>Gomez-Canon, J. S., Cano, E., Eerola, T., Herrera, P., Hu, X., Yang, Y.-H., et al. (2021). Music emotion recognition: toward new, robust standards in personalized and context-sensitive applications. \u003Ci>IEEE Signal Process. Mag.\u003C/i> 38, 106&#x2013;114. doi: 10.1109/MSP.2021.3106232\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1109/MSP.2021.3106232\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+S.+Gomez-Canon&#x00026;author=E.+Cano&#x00026;author=T.+Eerola&#x00026;author=P.+Herrera&#x00026;author=X.+Hu&#x00026;author=Y.-H.+Yang&#x00026;publication_year=2021&#x00026;title=Music+emotion+recognition:+toward+new+robust+standards+in+personalized+and+context-sensitive+applications&#x00026;journal=IEEE+Signal+Process.+Mag.&#x00026;volume=38&#x00026;pages=106-114\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref35\" id=\"ref35\">\u003C/a>Griffiths, D., Cunningham, S., Weinel, J., and Picking, R. (2021). A multi-genre model for music emotion recognition using linear regressors. \u003Ci>J. New Music Res.\u003C/i> 50, 355&#x2013;372. doi: 10.1080/09298215.2021.1977336\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1080/09298215.2021.1977336\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=D.+Griffiths&#x00026;author=S.+Cunningham&#x00026;author=J.+Weinel&#x00026;author=R.+Picking&#x00026;publication_year=2021&#x00026;title=A+multi-genre+model+for+music+emotion+recognition+using+linear+regressors&#x00026;journal=J.+New+Music+Res.&#x00026;volume=50&#x00026;pages=355-372\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref36\" id=\"ref36\">\u003C/a>Guo, S., Lu, J., Wang, Y., Li, Y., Huang, B., Zhang, Y., et al. (2020). Sad music modulates pain perception: an EEG study. \u003Ci>J. Pain Res.\u003C/i> 13, 2003&#x2013;2012. doi: 10.2147/JPR.S264188 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/32848448\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.2147/JPR.S264188\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=S.+Guo&#x00026;author=J.+Lu&#x00026;author=Y.+Wang&#x00026;author=Y.+Li&#x00026;author=B.+Huang&#x00026;author=Y.+Zhang&#x00026;publication_year=2020&#x00026;title=Sad+music+modulates+pain+perception:+an+EEG+study&#x00026;journal=J.+Pain+Res.&#x00026;volume=13&#x00026;pages=2003-2012\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref37\" id=\"ref37\">\u003C/a>Hartmann, M., Mavrolampados, A., Toiviainen, P., Saarikallio, S., Foubert, K., Brabant, O., et al. (2023). Musical interaction in music therapy for depression treatment. \u003Ci>Psychol. Music\u003C/i> 51, 33&#x2013;50. doi: 10.1177/03057356221084368\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1177/03057356221084368\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Hartmann&#x00026;author=A.+Mavrolampados&#x00026;author=P.+Toiviainen&#x00026;author=S.+Saarikallio&#x00026;author=K.+Foubert&#x00026;author=O.+Brabant&#x00026;publication_year=2023&#x00026;title=Musical+interaction+in+music+therapy+for+depression+treatment&#x00026;journal=Psychol.+Music&#x00026;volume=51&#x00026;pages=33-50\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref38\" id=\"ref38\">\u003C/a>Hevner, K. (1936). Experimental studies of the elements of expression in music. \u003Ci>Am. J. Psychol.\u003C/i> 48:246. doi: 10.2307/1415746\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.2307/1415746\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=K.+Hevner&#x00026;publication_year=1936&#x00026;title=Experimental+studies+of+the+elements+of+expression+in+music&#x00026;journal=Am.+J.+Psychol.&#x00026;volume=48&#x00026;pages=246\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref39\" id=\"ref39\">\u003C/a>Hossain, S., Rahman, M., Chakrabarty, A., Rashid, M., Kuwana, A., and Kobayashi, H. (2023). Emotional state classification from MUSIC-based features of multichannel EEG signals. \u003Ci>Bioengineering\u003C/i> 10:99. doi: 10.3390/bioengineering10010099 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/36671671\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3390/bioengineering10010099\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=S.+Hossain&#x00026;author=M.+Rahman&#x00026;author=A.+Chakrabarty&#x00026;author=M.+Rashid&#x00026;author=A.+Kuwana&#x00026;author=H.+Kobayashi&#x00026;publication_year=2023&#x00026;title=Emotional+state+classification+from+MUSIC-based+features+of+multichannel+EEG+signals&#x00026;journal=Bioengineering&#x00026;volume=10&#x00026;pages=99\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref40\" id=\"ref40\">\u003C/a>Humphreys, J. T. (1998). Musical aptitude testing: from James McKeen Cattell to Carl Emil seashore. \u003Ci>Res. Stud. Music Educ.\u003C/i> 10, 42&#x2013;53. doi: 10.1177/1321103X9801000104\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1177/1321103X9801000104\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+T.+Humphreys&#x00026;publication_year=1998&#x00026;title=Musical+aptitude+testing:+from+James+McKeen+Cattell+to+Carl+Emil+seashore&#x00026;journal=Res.+Stud.+Music+Educ.&#x00026;volume=10&#x00026;pages=42-53\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref41\" id=\"ref41\">\u003C/a>Katsigiannis, S., and Ramzan, N. (2018). DREAMER: a database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices. \u003Ci>IEEE J. Biomed. Health Inform.\u003C/i> 22, 98&#x2013;107. doi: 10.1109/JBHI.2017.2688239 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/28368836\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1109/JBHI.2017.2688239\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=S.+Katsigiannis&#x00026;author=N.+Ramzan&#x00026;publication_year=2018&#x00026;title=DREAMER:+a+database+for+emotion+recognition+through+EEG+and+ECG+signals+from+wireless+low-cost+off-the-shelf+devices&#x00026;journal=IEEE+J.+Biomed.+Health+Inform.&#x00026;volume=22&#x00026;pages=98-107\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref42\" id=\"ref42\">\u003C/a>Khabiri, H., Naseh Talebi, M., Kamran, M. F., Akbari, S., Zarrin, F., and Mohandesi, F. (2023). Music-induced emotion recognition based on feature reduction using PCA from EEG signals. \u003Ci>Front. Biomed. Technol.\u003C/i> 11, 59&#x2013;68. doi: 10.18502/fbt.v11i1.14512\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.18502/fbt.v11i1.14512\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=H.+Khabiri&#x00026;author=M.+Naseh+Talebi&#x00026;author=M.+F.+Kamran&#x00026;author=S.+Akbari&#x00026;author=F.+Zarrin&#x00026;author=F.+Mohandesi&#x00026;publication_year=2023&#x00026;title=Music-induced+emotion+recognition+based+on+feature+reduction+using+PCA+from+EEG+signals&#x00026;journal=Front.+Biomed.+Technol.&#x00026;volume=11&#x00026;pages=59-68\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref43\" id=\"ref43\">\u003C/a>Khare, S. K., and Bajaj, V. (2021). Time&#x2013;frequency representation and convolutional neural network-based emotion recognition. \u003Ci>IEEE Trans. Neural Networks Learn. Syst.\u003C/i> 32, 2901&#x2013;2909. doi: 10.1109/TNNLS.2020.3008938 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/32735536\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1109/TNNLS.2020.3008938\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=S.+K.+Khare&#x00026;author=V.+Bajaj&#x00026;publication_year=2021&#x00026;title=Time&#x2013;frequency+representation+and+convolutional+neural+network-based+emotion+recognition&#x00026;journal=IEEE+Trans.+Neural+Networks+Learn.+Syst.&#x00026;volume=32&#x00026;pages=2901-2909\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref44\" id=\"ref44\">\u003C/a>Kim, H., Zhang, D., Kim, L., and Im, C.-H. (2022). Classification of individual&#x2019;s discrete emotions reflected in facial microexpressions using electroencephalogram and facial electromyogram. \u003Ci>Expert Syst. Appl.\u003C/i> 188:116101. doi: 10.1016/j.eswa.2021.116101\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.eswa.2021.116101\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=H.+Kim&#x00026;author=D.+Zhang&#x00026;author=L.+Kim&#x00026;author=C.-H.+Im&#x00026;publication_year=2022&#x00026;title=Classification+of+individual&#x2019;s+discrete+emotions+reflected+in+facial+microexpressions+using+electroencephalogram+and+facial+electromyogram&#x00026;journal=Expert+Syst.+Appl.&#x00026;volume=188&#x00026;pages=116101\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref45\" id=\"ref45\">\u003C/a>Koelstra, S., Muhl, C., Soleymani, M., Lee, J.-S., Yazdani, A., Ebrahimi, T., et al. (2012). DEAP: a database for emotion analysis using physiological signals. \u003Ci>IEEE Trans. Affect. Comput.\u003C/i> 3, 18&#x2013;31. doi: 10.1109/T-AFFC.2011.15\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1109/T-AFFC.2011.15\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=S.+Koelstra&#x00026;author=C.+Muhl&#x00026;author=M.+Soleymani&#x00026;author=J.-S.+Lee&#x00026;author=A.+Yazdani&#x00026;author=T.+Ebrahimi&#x00026;publication_year=2012&#x00026;title=DEAP:+a+database+for+emotion+analysis+using+physiological+signals&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=3&#x00026;pages=18-31\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref46\" id=\"ref46\">\u003C/a>Lapomarda, G., Valer, S., Job, R., and Grecucci, A. (2022). Built to last: theta and delta changes in resting-state EEG activity after regulating emotions. \u003Ci>Brain Behav.\u003C/i> 12:e2597. doi: 10.1002/brb3.2597 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/35560984\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1002/brb3.2597\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=G.+Lapomarda&#x00026;author=S.+Valer&#x00026;author=R.+Job&#x00026;author=A.+Grecucci&#x00026;publication_year=2022&#x00026;title=Built+to+last:+theta+and+delta+changes+in+resting-state+EEG+activity+after+regulating+emotions&#x00026;journal=Brain+Behav.&#x00026;volume=12&#x00026;pages=e2597\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref47\" id=\"ref47\">\u003C/a>Li, P., Liu, H., Si, Y., Li, C., Li, F., Zhu, X., et al. (2019). EEG based emotion recognition by combining functional connectivity network and local activations. \u003Ci>IEEE Trans. Biomed. Eng.\u003C/i> 66, 2869&#x2013;2881. doi: 10.1109/TBME.2019.2897651 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/30735981\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1109/TBME.2019.2897651\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=P.+Li&#x00026;author=H.+Liu&#x00026;author=Y.+Si&#x00026;author=C.+Li&#x00026;author=F.+Li&#x00026;author=X.+Zhu&#x00026;publication_year=2019&#x00026;title=EEG+based+emotion+recognition+by+combining+functional+connectivity+network+and+local+activations&#x00026;journal=IEEE+Trans.+Biomed.+Eng.&#x00026;volume=66&#x00026;pages=2869-2881\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref48\" id=\"ref48\">\u003C/a>Li, D., Ruan, Y., Zheng, F., Lijuan, S., and Lin, Q. (2022a). Effect of Taiji post-standing on the brain analyzed with EEG signals. \u003Ci>J. Taiji Sci.\u003C/i> 1, 2&#x2013;7. doi: 10.57612/2022.jts.01.01\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.57612/2022.jts.01.01\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=D.+Li&#x00026;author=Y.+Ruan&#x00026;author=F.+Zheng&#x00026;author=S.+Lijuan&#x00026;author=Q.+Lin&#x00026;publication_year=2022a&#x00026;title=Effect+of+Taiji+post-standing+on+the+brain+analyzed+with+EEG+signals&#x00026;journal=J.+Taiji+Sci.&#x00026;volume=1&#x00026;pages=2-7\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref49\" id=\"ref49\">\u003C/a>Li, D., Ruan, Y., Zheng, F., Su, Y., and Lin, Q. (2022b). Fast sleep stage classification using cascaded support vector machines with single-channel EEG signals. \u003Ci>Sensors\u003C/i> 22:9914. doi: 10.3390/s22249914 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/36560286\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3390/s22249914\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=D.+Li&#x00026;author=Y.+Ruan&#x00026;author=F.+Zheng&#x00026;author=Y.+Su&#x00026;author=Q.+Lin&#x00026;publication_year=2022b&#x00026;title=Fast+sleep+stage+classification+using+cascaded+support+vector+machines+with+single-channel+EEG+signals&#x00026;journal=Sensors&#x00026;volume=22&#x00026;pages=9914\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref50\" id=\"ref50\">\u003C/a>Li, M., Xu, H., Liu, X., and Lu, S. (2018). Emotion recognition from multichannel EEG signals using K-nearest neighbor classification. \u003Ci>Technol. Health Care\u003C/i> 26, 509&#x2013;519. doi: 10.3233/THC-174836 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/29758974\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3233/THC-174836\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Li&#x00026;author=H.+Xu&#x00026;author=X.+Liu&#x00026;author=S.+Lu&#x00026;publication_year=2018&#x00026;title=Emotion+recognition+from+multichannel+EEG+signals+using+K-nearest+neighbor+classification&#x00026;journal=Technol.+Health+Care&#x00026;volume=26&#x00026;pages=509-519\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref51\" id=\"ref51\">\u003C/a>Li, X., Zhang, Y., Tiwari, P., Song, D., Hu, B., Yang, M., et al. (2023). EEG based emotion recognition: a tutorial and review. \u003Ci>ACM Comput. Surv.\u003C/i> 55, 1&#x2013;57. doi: 10.1145/3524499\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1145/3524499\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=X.+Li&#x00026;author=Y.+Zhang&#x00026;author=P.+Tiwari&#x00026;author=D.+Song&#x00026;author=B.+Hu&#x00026;author=M.+Yang&#x00026;publication_year=2023&#x00026;title=EEG+based+emotion+recognition:+a+tutorial+and+review&#x00026;journal=ACM+Comput.+Surv.&#x00026;volume=55&#x00026;pages=1-57\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref52\" id=\"ref52\">\u003C/a>Li, Y., and Zheng, W. (2023). EEG processing in emotion recognition: inspired from a musical staff. \u003Ci>Multimed. Tools Appl.\u003C/i> 82, 4161&#x2013;4180. doi: 10.1007/s11042-022-13405-x\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1007/s11042-022-13405-x\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=Y.+Li&#x00026;author=W.+Zheng&#x00026;publication_year=2023&#x00026;title=EEG+processing+in+emotion+recognition:+inspired+from+a+musical+staff&#x00026;journal=Multimed.+Tools+Appl.&#x00026;volume=82&#x00026;pages=4161-4180\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref53\" id=\"ref53\">\u003C/a>Liang, J., Tian, X., and Yang, W. (2021). Application of music therapy in general surgical treatment. \u003Ci>Biomed. Res. Int.\u003C/i> 2021, 1&#x2013;4. doi: 10.1155/2021/6169183 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/34621896\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1155/2021/6169183\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Liang&#x00026;author=X.+Tian&#x00026;author=W.+Yang&#x00026;publication_year=2021&#x00026;title=Application+of+music+therapy+in+general+surgical+treatment&#x00026;journal=Biomed.+Res.+Int.&#x00026;volume=2021&#x00026;pages=1-4\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref54\" id=\"ref54\">\u003C/a>Liu, H., Fang, Y., and Huang, Q. (2019). Music emotion recognition using a variant of recurrent neural network., in Proceedings of the 2018 international conference on mathematics, modeling, simulation and statistics application (MMSSA 2018), (Shanghai, China: Atlantis Press).\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"http://scholar.google.com/scholar_lookup?author=H.+Liu&#x00026;author=Y.+Fang&#x00026;author=Q.+Huang&#x00026;publication_year=2019&#x00026;title=Music+emotion+recognition+using+a+variant+of+recurrent+neural+network&#x00026;\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref55\" id=\"ref55\">\u003C/a>Liu, J., Wu, G., Luo, Y., Qiu, S., Yang, S., Li, W., et al. (2020). EEG-based emotion classification using a deep neural network and sparse autoencoder. \u003Ci>Front. Syst. Neurosci.\u003C/i> 14:43. doi: 10.3389/fnsys.2020.00043 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/32982703\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3389/fnsys.2020.00043\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Liu&#x00026;author=G.+Wu&#x00026;author=Y.+Luo&#x00026;author=S.+Qiu&#x00026;author=S.+Yang&#x00026;author=W.+Li&#x00026;publication_year=2020&#x00026;title=EEG-based+emotion+classification+using+a+deep+neural+network+and+sparse+autoencoder&#x00026;journal=Front.+Syst.+Neurosci.&#x00026;volume=14&#x00026;pages=43\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref56\" id=\"ref56\">\u003C/a>Lu, G., Jia, R., Liang, D., Yu, J., Wu, Z., and Chen, C. (2021). Effects of music therapy on anxiety: a meta-analysis of randomized controlled trials. \u003Ci>Psychiatry Res.\u003C/i> 304:114137. doi: 10.1016/j.psychres.2021.114137\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.psychres.2021.114137\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=G.+Lu&#x00026;author=R.+Jia&#x00026;author=D.+Liang&#x00026;author=J.+Yu&#x00026;author=Z.+Wu&#x00026;author=C.+Chen&#x00026;publication_year=2021&#x00026;title=Effects+of+music+therapy+on+anxiety:+a+meta-analysis+of+randomized+controlled+trials&#x00026;journal=Psychiatry+Res.&#x00026;volume=304&#x00026;pages=114137\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref57\" id=\"ref57\">\u003C/a>Luo, E., Pan, W., and Fan, X. (2023). Music, language, and autism: neurological insights for enhanced learning. \u003Ci>Int. J. Innov. Res. Med. Sci.\u003C/i> 8, 398&#x2013;408. doi: 10.23958/ijirms/vol08-i09/1743\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.23958/ijirms/vol08-i09/1743\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=E.+Luo&#x00026;author=W.+Pan&#x00026;author=X.+Fan&#x00026;publication_year=2023&#x00026;title=Music+language+and+autism:+neurological+insights+for+enhanced+learning&#x00026;journal=Int.+J.+Innov.+Res.+Med.+Sci.&#x00026;volume=8&#x00026;pages=398-408\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref58\" id=\"ref58\">\u003C/a>Maffei, A. (2020). Spectrally resolved EEG intersubject correlation reveals distinct cortical oscillatory patterns during free-viewing of affective scenes. \u003Ci>Psychophysiology\u003C/i> 57:e13652. doi: 10.1111/psyp.13652 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/33460185\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1111/psyp.13652\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Maffei&#x00026;publication_year=2020&#x00026;title=Spectrally+resolved+EEG+intersubject+correlation+reveals+distinct+cortical+oscillatory+patterns+during+free-viewing+of+affective+scenes&#x00026;journal=Psychophysiology&#x00026;volume=57&#x00026;pages=e13652\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref59\" id=\"ref59\">\u003C/a>Mahmoud, A., Amin, K., Al Rahhal, M., Elkilani, W., Mekhalfi, M., and Ibrahim, M. (2023). A CNN approach for emotion recognition via EEG. \u003Ci>Symmetry\u003C/i> 15:1822. doi: 10.3390/sym15101822\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.3390/sym15101822\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Mahmoud&#x00026;author=K.+Amin&#x00026;author=M.+Al+Rahhal&#x00026;author=W.+Elkilani&#x00026;author=M.+Mekhalfi&#x00026;author=M.+Ibrahim&#x00026;publication_year=2023&#x00026;title=A+CNN+approach+for+emotion+recognition+via+EEG&#x00026;journal=Symmetry&#x00026;volume=15&#x00026;pages=1822\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref60\" id=\"ref60\">\u003C/a>Mart&#x000ED;nez-Saez, M., Ros, L., L&#x000F3;pez-Cano, M., Nieto, M., Navarro, B., and Latorre, J. (2024). Effect of popular songs from the reminiscence bump as autobiographical memory cues in aging: a preliminary study using EEG. \u003Ci>Front. Neurosci.\u003C/i> 17:1300751. doi: 10.3389/fnins.2023.1300751 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/38264494\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3389/fnins.2023.1300751\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Mart&#x000ED;nez-Saez&#x00026;author=L.+Ros&#x00026;author=M.+L&#x000F3;pez-Cano&#x00026;author=M.+Nieto&#x00026;author=B.+Navarro&#x00026;author=J.+Latorre&#x00026;publication_year=2024&#x00026;title=Effect+of+popular+songs+from+the+reminiscence+bump+as+autobiographical+memory+cues+in+aging:+a+preliminary+study+using+EEG&#x00026;journal=Front.+Neurosci.&#x00026;volume=17&#x00026;pages=1300751\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref61\" id=\"ref61\">\u003C/a>Martins, I., Lima, C., and Pinheiro, A. (2022). Enhanced salience of musical sounds in singers and instrumentalists. \u003Ci>Cogn. Affect. Behav. Ne.\u003C/i> 22, 1044&#x2013;1062. doi: 10.3758/s13415-022-01007-x \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/35501427\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3758/s13415-022-01007-x\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=I.+Martins&#x00026;author=C.+Lima&#x00026;author=A.+Pinheiro&#x00026;publication_year=2022&#x00026;title=Enhanced+salience+of+musical+sounds+in+singers+and+instrumentalists&#x00026;journal=Cogn.+Affect.+Behav.+Ne.&#x00026;volume=22&#x00026;pages=1044-1062\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref62\" id=\"ref62\">\u003C/a>Metfessel, M. (1950). Carl emil seashore, 1866-1949. \u003Ci>Science\u003C/i> 111, 713&#x2013;717. doi: 10.1126/science.111.2896.713 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/15431064\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1126/science.111.2896.713\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Metfessel&#x00026;publication_year=1950&#x00026;title=Carl+emil+seashore+1866-1949&#x00026;journal=Science&#x00026;volume=111&#x00026;pages=713-717\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref63\" id=\"ref63\">\u003C/a>Micallef Grimaud, A., and Eerola, T. (2022). An interactive approach to emotional expression through musical cues. \u003Ci>Music. Sci.\u003C/i> 5:205920432110617. doi: 10.1177/20592043211061745\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1177/20592043211061745\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Micallef+Grimaud&#x00026;author=T.+Eerola&#x00026;publication_year=2022&#x00026;title=An+interactive+approach+to+emotional+expression+through+musical+cues&#x00026;journal=Music.+Sci.&#x00026;volume=5&#x00026;pages=205920432110617\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref64\" id=\"ref64\">\u003C/a>Moctezuma, L., Abe, T., and Molinas, M. (2022). Two-dimensional CNN-based distinction of human emotions from EEG channels selected by multi-objective evolutionary algorithm. \u003Ci>Sci. Rep.\u003C/i> 12:3523. doi: 10.1038/s41598-022-07517-5 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/35241745\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1038/s41598-022-07517-5\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=L.+Moctezuma&#x00026;author=T.+Abe&#x00026;author=M.+Molinas&#x00026;publication_year=2022&#x00026;title=Two-dimensional+CNN-based+distinction+of+human+emotions+from+EEG+channels+selected+by+multi-objective+evolutionary+algorithm&#x00026;journal=Sci.+Rep.&#x00026;volume=12&#x00026;pages=3523\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref65\" id=\"ref65\">\u003C/a>Nawaz, R., Ng, J. T., Nisar, H., and Voon, Y. V. (2019). Can background music help to relieve stress? An EEG analysis., in 2019 IEEE international conference on signal and image processing applications (ICSIPA), Kuala Lumpur, Malaysia: IEEE, 68&#x2013;72.\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"http://scholar.google.com/scholar_lookup?author=R.+Nawaz&#x00026;author=J.+T.+Ng&#x00026;author=H.+Nisar&#x00026;author=Y.+V.+Voon&#x00026;publication_year=2019&#x00026;title=Can+background+music+help+to+relieve+stress?+An+EEG+analysis&#x00026;pages=68-72\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref66\" id=\"ref66\">\u003C/a>Oktavia, N. Y., Wibawa, A. D., Pane, E. S., and Purnomo, M. H. (2019). &#x201C;Human emotion classification based on EEG signals using na&#x000EF;ve bayes method&#x201D; in \u003Ci>2019 international seminar on application for technology of information and communication (iSemantic)\u003C/i> (Semarang, Indonesia: IEEE), 319&#x2013;324.\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"http://scholar.google.com/scholar_lookup?author=N.+Y.+Oktavia&#x00026;author=A.+D.+Wibawa&#x00026;author=E.+S.+Pane&#x00026;author=M.+H.+Purnomo&#x00026;publication_year=2019&#x00026;title=Human+emotion+classification+based+on+EEG+signals+using+na&#x000EF;ve+bayes+method&#x00026;journal=2019+international+seminar+on+application+for+technology+of+information+and+communication+(iSemantic)&#x00026;pages=319-324\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref67\" id=\"ref67\">\u003C/a>Palazzi, A., Meschini, R., and Piccinini, C. A. (2021). NICU music therapy effects on maternal mental health and preterm infant&#x2019;s emotional arousal. \u003Ci>Infant Ment. Health J.\u003C/i> 42, 672&#x2013;689. doi: 10.1002/imhj.21938 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/34378804\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1002/imhj.21938\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Palazzi&#x00026;author=R.+Meschini&#x00026;author=C.+A.+Piccinini&#x00026;publication_year=2021&#x00026;title=NICU+music+therapy+effects+on+maternal+mental+health+and+preterm+infant&#x2019;s+emotional+arousal&#x00026;journal=Infant+Ment.+Health+J.&#x00026;volume=42&#x00026;pages=672-689\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref68\" id=\"ref68\">\u003C/a>Pan, J., Yang, F., Qiu, L., and Huang, H. (2022). Fusion of EEG-based activation, spatial, and connection patterns for fear emotion recognition. \u003Ci>Comput. Intell. Neurosci.\u003C/i> 2022, 1&#x2013;11. doi: 10.1155/2022/3854513 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/35463262\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1155/2022/3854513\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Pan&#x00026;author=F.+Yang&#x00026;author=L.+Qiu&#x00026;author=H.+Huang&#x00026;publication_year=2022&#x00026;title=Fusion+of+EEG-based+activation+spatial+and+connection+patterns+for+fear+emotion+recognition&#x00026;journal=Comput.+Intell.+Neurosci.&#x00026;volume=2022&#x00026;pages=1-11\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref69\" id=\"ref69\">\u003C/a>Panda, R., Malheiro, R., and Paiva, R. P. (2023). Audio features for music emotion recognition: a survey. \u003Ci>IEEE Trans. Affect. Comput.\u003C/i> 14, 68&#x2013;88. doi: 10.1109/TAFFC.2020.3032373\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1109/TAFFC.2020.3032373\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=R.+Panda&#x00026;author=R.+Malheiro&#x00026;author=R.+P.+Paiva&#x00026;publication_year=2023&#x00026;title=Audio+features+for+music+emotion+recognition:+a+survey&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=14&#x00026;pages=68-88\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref70\" id=\"ref70\">\u003C/a>Pandey, P., and Seeja, K. R. (2022). Subject independent emotion recognition from EEG using VMD and deep learning. \u003Ci>J. King Saud Univ. Comput. Inf. Sci.\u003C/i> 34, 1730&#x2013;1738. doi: 10.1016/j.jksuci.2019.11.003\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.jksuci.2019.11.003\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=P.+Pandey&#x00026;author=K.+R.+Seeja&#x00026;publication_year=2022&#x00026;title=Subject+independent+emotion+recognition+from+EEG+using+VMD+and+deep+learning&#x00026;journal=J.+King+Saud+Univ.+Comput.+Inf.+Sci.&#x00026;volume=34&#x00026;pages=1730-1738\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref71\" id=\"ref71\">\u003C/a>Pei, G., Shang, Q., Hua, S., Li, T., and Jin, J. (2024). EEG-based affective computing in virtual reality with a balancing of the computational efficiency and recognition accuracy. \u003Ci>Comput. Hum. Behav.\u003C/i> 152:108085. doi: 10.1016/j.chb.2023.108085\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.chb.2023.108085\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=G.+Pei&#x00026;author=Q.+Shang&#x00026;author=S.+Hua&#x00026;author=T.+Li&#x00026;author=J.+Jin&#x00026;publication_year=2024&#x00026;title=EEG-based+affective+computing+in+virtual+reality+with+a+balancing+of+the+computational+efficiency+and+recognition+accuracy&#x00026;journal=Comput.+Hum.+Behav.&#x00026;volume=152&#x00026;pages=108085\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref72\" id=\"ref72\">\u003C/a>Perlovsky, L. (2012). Cognitive function, origin, and evolution of musical emotions. \u003Ci>Music. Sci.\u003C/i> 16, 185&#x2013;199. doi: 10.1177/1029864912448327\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1177/1029864912448327\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=L.+Perlovsky&#x00026;publication_year=2012&#x00026;title=Cognitive+function+origin+and+evolution+of+musical+emotions&#x00026;journal=Music.+Sci.&#x00026;volume=16&#x00026;pages=185-199\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref73\" id=\"ref73\">\u003C/a>Ramirez, R., Planas, J., Escude, N., Mercade, J., and Farriols, C. (2018). EEG-based analysis of the emotional effect of music therapy on palliative care cancer patients. \u003Ci>Front. Psychol.\u003C/i> 9:254. doi: 10.3389/fpsyg.2018.00254\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.3389/fpsyg.2018.00254\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=R.+Ramirez&#x00026;author=J.+Planas&#x00026;author=N.+Escude&#x00026;author=J.+Mercade&#x00026;author=C.+Farriols&#x00026;publication_year=2018&#x00026;title=EEG-based+analysis+of+the+emotional+effect+of+music+therapy+on+palliative+care+cancer+patients&#x00026;journal=Front.+Psychol.&#x00026;volume=9&#x00026;pages=254\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref74\" id=\"ref74\">\u003C/a>Reisenzein, R. (1994). Pleasure-arousal theory and the intensity of emotions. \u003Ci>J. Pers. Soc. Psychol.\u003C/i> 67, 525&#x2013;539. doi: 10.1037/0022-3514.67.3.525\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1037/0022-3514.67.3.525\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=R.+Reisenzein&#x00026;publication_year=1994&#x00026;title=Pleasure-arousal+theory+and+the+intensity+of+emotions&#x00026;journal=J.+Pers.+Soc.+Psychol.&#x00026;volume=67&#x00026;pages=525-539\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref75\" id=\"ref75\">\u003C/a>Rolls, E. T. (2015). Limbic systems for emotion and for memory, but no single limbic system. \u003Ci>Cortex\u003C/i> 62, 119&#x2013;157. doi: 10.1016/j.cortex.2013.12.005\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.cortex.2013.12.005\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=E.+T.+Rolls&#x00026;publication_year=2015&#x00026;title=Limbic+systems+for+emotion+and+for+memory+but+no+single+limbic+system&#x00026;journal=Cortex&#x00026;volume=62&#x00026;pages=119-157\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref76\" id=\"ref76\">\u003C/a>Russell, J. A. (1980). A circumplex model of affect. \u003Ci>J. Pers. Soc. Psychol.\u003C/i> 39, 1161&#x2013;1178. doi: 10.1037/h0077714\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1037/h0077714\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+A.+Russell&#x00026;publication_year=1980&#x00026;title=A+circumplex+model+of+affect&#x00026;journal=J.+Pers.+Soc.+Psychol.&#x00026;volume=39&#x00026;pages=1161-1178\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref77\" id=\"ref77\">\u003C/a>Ruth, N., and Schramm, H. (2021). Effects of prosocial lyrics and musical production elements on emotions, thoughts and behavior. \u003Ci>Psychol. Music\u003C/i> 49, 759&#x2013;776. doi: 10.1177/0305735620902534\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1177/0305735620902534\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=N.+Ruth&#x00026;author=H.+Schramm&#x00026;publication_year=2021&#x00026;title=Effects+of+prosocial+lyrics+and+musical+production+elements+on+emotions+thoughts+and+behavior&#x00026;journal=Psychol.+Music&#x00026;volume=49&#x00026;pages=759-776\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref78\" id=\"ref78\">\u003C/a>Ryczkowska, A. (2022). Positive mood induction through music: the significance of listener age and musical timbre. \u003Ci>Psychol. Music\u003C/i> 50, 1961&#x2013;1975. doi: 10.1177/03057356221081164\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1177/03057356221081164\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Ryczkowska&#x00026;publication_year=2022&#x00026;title=Positive+mood+induction+through+music:+the+significance+of+listener+age+and+musical+timbre&#x00026;journal=Psychol.+Music&#x00026;volume=50&#x00026;pages=1961-1975\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref79\" id=\"ref79\">\u003C/a>Saganowski, S., Perz, B., Polak, A. G., and Kazienko, P. (2023). Emotion recognition for everyday life using physiological signals from wearables: a systematic literature review. \u003Ci>IEEE Trans. Affect. Comput.\u003C/i> 14, 1876&#x2013;1897. doi: 10.1109/TAFFC.2022.3176135\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1109/TAFFC.2022.3176135\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=S.+Saganowski&#x00026;author=B.+Perz&#x00026;author=A.+G.+Polak&#x00026;author=P.+Kazienko&#x00026;publication_year=2023&#x00026;title=Emotion+recognition+for+everyday+life+using+physiological+signals+from+wearables:+a+systematic+literature+review&#x00026;journal=IEEE+Trans.+Affect.+Comput.&#x00026;volume=14&#x00026;pages=1876-1897\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref80\" id=\"ref80\">\u003C/a>Salakka, I., Pitk&#x000E4;niemi, A., Pentik&#x000E4;inen, E., Mikkonen, K., Saari, P., Toiviainen, P., et al. (2021). What makes music memorable? Relationships between acoustic musical features and music-evoked emotions and memories in older adults. \u003Ci>PLoS One\u003C/i> 16:e0251692. doi: 10.1371/journal.pone.0251692\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1371/journal.pone.0251692\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=I.+Salakka&#x00026;author=A.+Pitk&#x000E4;niemi&#x00026;author=E.+Pentik&#x000E4;inen&#x00026;author=K.+Mikkonen&#x00026;author=P.+Saari&#x00026;author=P.+Toiviainen&#x00026;publication_year=2021&#x00026;title=What+makes+music+memorable?+Relationships+between+acoustic+musical+features+and+music-evoked+emotions+and+memories+in+older+adults&#x00026;journal=PLoS+One&#x00026;volume=16&#x00026;pages=e0251692\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref81\" id=\"ref81\">\u003C/a>Sammler, D., Grigutsch, M., Fritz, T., and Koelsch, S. (2007). Music and emotion: electrophysiological correlates of the processing of pleasant and unpleasant music. \u003Ci>Psychophysiology\u003C/i> 44, 293&#x2013;304. doi: 10.1111/j.1469-8986.2007.00497.x\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1111/j.1469-8986.2007.00497.x\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=D.+Sammler&#x00026;author=M.+Grigutsch&#x00026;author=T.+Fritz&#x00026;author=S.+Koelsch&#x00026;publication_year=2007&#x00026;title=Music+and+emotion:+electrophysiological+correlates+of+the+processing+of+pleasant+and+unpleasant+music&#x00026;journal=Psychophysiology&#x00026;volume=44&#x00026;pages=293-304\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref82\" id=\"ref82\">\u003C/a>Sanyal, S., Nag, S., Banerjee, A., Sengupta, R., and Ghosh, D. (2019). Music of brain and music on brain: a novel EEG sonification approach. \u003Ci>Cogn. Neurodyn.\u003C/i> 13, 13&#x2013;31. doi: 10.1007/s11571-018-9502-4 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/30728868\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1007/s11571-018-9502-4\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=S.+Sanyal&#x00026;author=S.+Nag&#x00026;author=A.+Banerjee&#x00026;author=R.+Sengupta&#x00026;author=D.+Ghosh&#x00026;publication_year=2019&#x00026;title=Music+of+brain+and+music+on+brain:+a+novel+EEG+sonification+approach&#x00026;journal=Cogn.+Neurodyn.&#x00026;volume=13&#x00026;pages=13-31\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref83\" id=\"ref83\">\u003C/a>Sari, D. A. L., Kusumaningrum, T. D., and Kusumoputro, B. (2023). Non-linear EEG based emotional classification using k-nearest neighbor and weighted k-nearest neighbor with variation of features selection methods. \u003Ci>AIP Conf. Proc.\u003C/i> 2654:020004. doi: 10.1063/5.0116377\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1063/5.0116377\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=D.+A.+L.+Sari&#x00026;author=T.+D.+Kusumaningrum&#x00026;author=B.+Kusumoputro&#x00026;publication_year=2023&#x00026;title=Non-linear+EEG+based+emotional+classification+using+k-nearest+neighbor+and+weighted+k-nearest+neighbor+with+variation+of+features+selection+methods&#x00026;journal=AIP+Conf.+Proc.&#x00026;volume=2654&#x00026;pages=020004\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref84\" id=\"ref84\">\u003C/a>Schmidt, L. A., and Trainor, L. J. (2001). Frontal brain electrical activity (EEG) distinguishes valence and intensity of musical emotions. \u003Ci>Cogn. Emot.\u003C/i> 15, 487&#x2013;500. doi: 10.1080/02699930126048\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1080/02699930126048\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=L.+A.+Schmidt&#x00026;author=L.+J.+Trainor&#x00026;publication_year=2001&#x00026;title=Frontal+brain+electrical+activity+(EEG)+distinguishes+valence+and+intensity+of+musical+emotions&#x00026;journal=Cogn.+Emot.&#x00026;volume=15&#x00026;pages=487-500\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref85\" id=\"ref85\">\u003C/a>Shu, L., Xie, J., Yang, M., Li, Z., Li, Z., Liao, D., et al. (2018). A review of emotion recognition using physiological signals. \u003Ci>Sensors\u003C/i> 18:2074. doi: 10.3390/s18072074 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/29958457\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3390/s18072074\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=L.+Shu&#x00026;author=J.+Xie&#x00026;author=M.+Yang&#x00026;author=Z.+Li&#x00026;author=Z.+Li&#x00026;author=D.+Liao&#x00026;publication_year=2018&#x00026;title=A+review+of+emotion+recognition+using+physiological+signals&#x00026;journal=Sensors&#x00026;volume=18&#x00026;pages=2074\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref86\" id=\"ref86\">\u003C/a>Silverman, D. (1963). The rationale and history of the 10-20 system of the international federation. \u003Ci>Am. J. EEG Technol.\u003C/i> 3, 17&#x2013;22. doi: 10.1080/00029238.1963.11080602\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1080/00029238.1963.11080602\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=D.+Silverman&#x00026;publication_year=1963&#x00026;title=The+rationale+and+history+of+the+10-20+system+of+the+international+federation&#x00026;journal=Am.+J.+EEG+Technol.&#x00026;volume=3&#x00026;pages=17-22\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref87\" id=\"ref87\">\u003C/a>Soleymani, M., Caro, M. N., Schmidt, E. M., Sha, C.-Y., and Yang, Y.-H. (2013). 1000 songs for emotional analysis of music., in Proceedings of the 2nd ACM international workshop on crowdsourcing for multimedia, (Barcelona, Spain: ACM), 1&#x2013;6.\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Soleymani&#x00026;author=M.+N.+Caro&#x00026;author=E.+M.+Schmidt&#x00026;author=C.-Y.+Sha&#x00026;author=Y.-H.+Yang&#x00026;publication_year=2013&#x00026;title=1000+songs+for+emotional+analysis+of+music&#x00026;pages=1-6\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref88\" id=\"ref88\">\u003C/a>Sonnemans, J., and Frijda, N. H. (1994). The structure of subjective emotional intensity. \u003Ci>Cogn. Emot.\u003C/i> 8, 329&#x2013;350. doi: 10.1080/02699939408408945\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1080/02699939408408945\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Sonnemans&#x00026;author=N.+H.+Frijda&#x00026;publication_year=1994&#x00026;title=The+structure+of+subjective+emotional+intensity&#x00026;journal=Cogn.+Emot.&#x00026;volume=8&#x00026;pages=329-350\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref89\" id=\"ref89\">\u003C/a>Stancin, I., Cifrek, M., and Jovic, A. (2021). A review of EEG signal features and their application in driver drowsiness detection systems. \u003Ci>Sensors\u003C/i> 21:3786. doi: 10.3390/s21113786 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/34070732\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3390/s21113786\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=I.+Stancin&#x00026;author=M.+Cifrek&#x00026;author=A.+Jovic&#x00026;publication_year=2021&#x00026;title=A+review+of+EEG+signal+features+and+their+application+in+driver+drowsiness+detection+systems&#x00026;journal=Sensors&#x00026;volume=21&#x00026;pages=3786\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref90\" id=\"ref90\">\u003C/a>Steinberg, R., G&#x000FC;nther, W., Stiltz, I., and Rondot, P. (1992). EEG-mapping during music stimulation. \u003Ci>Psychomusicol. J. Res. Music Cogn.\u003C/i> 11, 157&#x2013;170. doi: 10.1037/h0094123\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1037/h0094123\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=R.+Steinberg&#x00026;author=W.+G&#x000FC;nther&#x00026;author=I.+Stiltz&#x00026;author=P.+Rondot&#x00026;publication_year=1992&#x00026;title=EEG-mapping+during+music+stimulation&#x00026;journal=Psychomusicol.+J.+Res.+Music+Cogn.&#x00026;volume=11&#x00026;pages=157-170\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref91\" id=\"ref91\">\u003C/a>Tang, Z., Xia, D., Li, X., Wang, X., Ying, J., and Yang, H. (2023). Evaluation of the effect of music on idea generation using electrocardiography and electroencephalography signals. \u003Ci>Int. J. Technol. Des. Educ.\u003C/i> 33, 1607&#x2013;1625. doi: 10.1007/s10798-022-09782-x\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1007/s10798-022-09782-x\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=Z.+Tang&#x00026;author=D.+Xia&#x00026;author=X.+Li&#x00026;author=X.+Wang&#x00026;author=J.+Ying&#x00026;author=H.+Yang&#x00026;publication_year=2023&#x00026;title=Evaluation+of+the+effect+of+music+on+idea+generation+using+electrocardiography+and+electroencephalography+signals&#x00026;journal=Int.+J.+Technol.+Des.+Educ.&#x00026;volume=33&#x00026;pages=1607-1625\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref92\" id=\"ref92\">\u003C/a>Taylor, D. B. (1981). Music in general hospital treatment from 1900 to 1950. \u003Ci>J. Music. Ther.\u003C/i> 18, 62&#x2013;73. doi: 10.1093/jmt/18.2.62 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/10298285\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1093/jmt/18.2.62\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=D.+B.+Taylor&#x00026;publication_year=1981&#x00026;title=Music+in+general+hospital+treatment+from+1900+to+1950&#x00026;journal=J.+Music.+Ther.&#x00026;volume=18&#x00026;pages=62-73\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref93\" id=\"ref93\">\u003C/a>Tcherkassof, A., and Dupr&#x000E9;, D. (2021). The emotion&#x2013;facial expression link: evidence from human and automatic expression recognition. \u003Ci>Psychol. Res.\u003C/i> 85, 2954&#x2013;2969. doi: 10.1007/s00426-020-01448-4 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/33236175\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1007/s00426-020-01448-4\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=A.+Tcherkassof&#x00026;author=D.+Dupr&#x000E9;&#x00026;publication_year=2021&#x00026;title=The+emotion&#x2013;facial+expression+link:+evidence+from+human+and+automatic+expression+recognition&#x00026;journal=Psychol.+Res.&#x00026;volume=85&#x00026;pages=2954-2969\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref94\" id=\"ref94\">\u003C/a>Thaut, M. H., Francisco, G., and Hoemberg, V. (2021). Editorial: the clinical neuroscience of music: evidence based approaches and neurologic music therapy. \u003Ci>Front. Neurosci.\u003C/i> 15:740329. doi: 10.3389/fnins.2021.740329 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/34630025\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3389/fnins.2021.740329\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+H.+Thaut&#x00026;author=G.+Francisco&#x00026;author=V.+Hoemberg&#x00026;publication_year=2021&#x00026;title=Editorial:+the+clinical+neuroscience+of+music:+evidence+based+approaches+and+neurologic+music+therapy&#x00026;journal=Front.+Neurosci.&#x00026;volume=15&#x00026;pages=740329\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref95\" id=\"ref95\">\u003C/a>Thayer, R. E., and McNally, R. J. (1992). The biopsychology of mood and arousal. \u003Ci>Cogn. Behav. Neurol.\u003C/i> 5:65.\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"http://scholar.google.com/scholar_lookup?author=R.+E.+Thayer&#x00026;author=R.+J.+McNally&#x00026;publication_year=1992&#x00026;title=The+biopsychology+of+mood+and+arousal&#x00026;journal=Cogn.+Behav.+Neurol.&#x00026;volume=5&#x00026;pages=65\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref96\" id=\"ref96\">\u003C/a>Torres, E. P., Torres, E. A., Hern&#x000E1;ndez-&#x000C1;lvarez, M., and Yoo, S. G. (2020). EEG-based BCI emotion recognition: a survey. \u003Ci>Sensors\u003C/i> 20:5083. doi: 10.3390/s20185083 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/32906731\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3390/s20185083\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=E.+P.+Torres&#x00026;author=E.+A.+Torres&#x00026;author=M.+Hern&#x000E1;ndez-&#x000C1;lvarez&#x00026;author=S.+G.+Yoo&#x00026;publication_year=2020&#x00026;title=EEG-based+BCI+emotion+recognition:+a+survey&#x00026;journal=Sensors&#x00026;volume=20&#x00026;pages=5083\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref97\" id=\"ref97\">\u003C/a>Turnbull, D., Barrington, L., Torres, D., and Lanckriet, G. (2008). Semantic annotation and retrieval of music and sound effects. \u003Ci>IEEE Trans. Audio Speech Lang. Process.\u003C/i> 16, 467&#x2013;476. doi: 10.1109/TASL.2007.913750\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1109/TASL.2007.913750\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=D.+Turnbull&#x00026;author=L.+Barrington&#x00026;author=D.+Torres&#x00026;author=G.+Lanckriet&#x00026;publication_year=2008&#x00026;title=Semantic+annotation+and+retrieval+of+music+and+sound+effects&#x00026;journal=IEEE+Trans.+Audio+Speech+Lang.+Process.&#x00026;volume=16&#x00026;pages=467-476\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref98\" id=\"ref98\">\u003C/a>Ueno, F., and Shimada, S. (2023). Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music. \u003Ci>Front. Hum. Neurosci.\u003C/i> 17:1225377. doi: 10.3389/fnhum.2023.1225377\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.3389/fnhum.2023.1225377\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=F.+Ueno&#x00026;author=S.+Shimada&#x00026;publication_year=2023&#x00026;title=Inter-subject+correlations+of+EEG+reflect+subjective+arousal+and+acoustic+features+of+music&#x00026;journal=Front.+Hum.+Neurosci.&#x00026;volume=17&#x00026;pages=1225377\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref99\" id=\"ref99\">\u003C/a>Vuust, P., Heggli, O. A., Friston, K. J., and Kringelbach, M. L. (2022). Music in the brain. \u003Ci>Nat. Rev. Neurosci.\u003C/i> 23, 287&#x2013;305. doi: 10.1038/s41583-022-00578-5\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1038/s41583-022-00578-5\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=P.+Vuust&#x00026;author=O.+A.+Heggli&#x00026;author=K.+J.+Friston&#x00026;author=M.+L.+Kringelbach&#x00026;publication_year=2022&#x00026;title=Music+in+the+brain&#x00026;journal=Nat.+Rev.+Neurosci.&#x00026;volume=23&#x00026;pages=287-305\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref100\" id=\"ref100\">\u003C/a>Wang, X., Wei, Y., and Yang, D. (2022). Cross-cultural analysis of the correlation between musical elements and emotion. \u003Ci>Cognit. Comput. Syst.\u003C/i> 4, 116&#x2013;129. doi: 10.1049/ccs2.12032\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1049/ccs2.12032\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=X.+Wang&#x00026;author=Y.+Wei&#x00026;author=D.+Yang&#x00026;publication_year=2022&#x00026;title=Cross-cultural+analysis+of+the+correlation+between+musical+elements+and+emotion&#x00026;journal=Cognit.+Comput.+Syst.&#x00026;volume=4&#x00026;pages=116-129\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref18\" id=\"ref18\">\u003C/a>Weerakody, P. B., Wong, K. W., Wang, G., and Ela, W. (2021). A review of irregular time series data handling with gated recurrent neural networks. \u003Ci>Neurocomputing\u003C/i> 441, 161&#x2013;178. doi: 10.1016/j.neucom.2021.02.046\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.neucom.2021.02.046\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=P.+B.+Weerakody&#x00026;author=K.+W.+Wong&#x00026;author=G.+Wang&#x00026;author=W.+Ela&#x00026;publication_year=2021&#x00026;title=A+review+of+irregular+time+series+data+handling+with+gated+recurrent+neural+networks&#x00026;journal=Neurocomputing&#x00026;volume=441&#x00026;pages=161-178\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref101\" id=\"ref101\">\u003C/a>Witkower, Z., Hill, A. K., Koster, J., and Tracy, J. L. (2021). Beyond face value: evidence for the universality of bodily expressions of emotion. \u003Ci>Affect. Sci.\u003C/i> 2, 221&#x2013;229. doi: 10.1007/s42761-021-00052-y \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/36059900\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1007/s42761-021-00052-y\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=Z.+Witkower&#x00026;author=A.+K.+Hill&#x00026;author=J.+Koster&#x00026;author=J.+L.+Tracy&#x00026;publication_year=2021&#x00026;title=Beyond+face+value:+evidence+for+the+universality+of+bodily+expressions+of+emotion&#x00026;journal=Affect.+Sci.&#x00026;volume=2&#x00026;pages=221-229\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref102\" id=\"ref102\">\u003C/a>Wu, Q., Sun, L., Ding, N., and Yang, Y. (2024). Musical tension is affected by metrical structure dynamically and hierarchically. \u003Ci>Cogn. Neurodyn.\u003C/i> 18, 1955&#x2013;1976. doi: 10.1007/s11571-023-10058-w \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/39104669\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1007/s11571-023-10058-w\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=Q.+Wu&#x00026;author=L.+Sun&#x00026;author=N.+Ding&#x00026;author=Y.+Yang&#x00026;publication_year=2024&#x00026;title=Musical+tension+is+affected+by+metrical+structure+dynamically+and+hierarchically&#x00026;journal=Cogn.+Neurodyn.&#x00026;volume=18&#x00026;pages=1955-1976\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref103\" id=\"ref103\">\u003C/a>Xu, G., Guo, W., and Wang, Y. (2023). Subject-independent EEG emotion recognition with hybrid spatio-temporal GRU-conv architecture. \u003Ci>Med. Biol. Eng. Comput.\u003C/i> 61, 61&#x2013;73. doi: 10.1007/s11517-022-02686-x \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/36322243\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1007/s11517-022-02686-x\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=G.+Xu&#x00026;author=W.+Guo&#x00026;author=Y.+Wang&#x00026;publication_year=2023&#x00026;title=Subject-independent+EEG+emotion+recognition+with+hybrid+spatio-temporal+GRU-conv+architecture&#x00026;journal=Med.+Biol.+Eng.+Comput.&#x00026;volume=61&#x00026;pages=61-73\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref104\" id=\"ref104\">\u003C/a>Xu, J., Hu, L., Qiao, R., Hu, Y., and Tian, Y. (2023). Music-emotion EEG coupling effects based on representational similarity. \u003Ci>J. Neurosci. Methods\u003C/i> 398:109959. doi: 10.1016/j.jneumeth.2023.109959 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/37661055\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.1016/j.jneumeth.2023.109959\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Xu&#x00026;author=L.+Hu&#x00026;author=R.+Qiao&#x00026;author=Y.+Hu&#x00026;author=Y.+Tian&#x00026;publication_year=2023&#x00026;title=Music-emotion+EEG+coupling+effects+based+on+representational+similarity&#x00026;journal=J.+Neurosci.+Methods&#x00026;volume=398&#x00026;pages=109959\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref105\" id=\"ref105\">\u003C/a>Xu, J., Qian, W., Hu, L., Liao, G., and Tian, Y. (2024). EEG decoding for musical emotion with functional connectivity features. \u003Ci>Biomed. Signal Process. Control\u003C/i> 89:105744. doi: 10.1016/j.bspc.2023.105744\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.bspc.2023.105744\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Xu&#x00026;author=W.+Qian&#x00026;author=L.+Hu&#x00026;author=G.+Liao&#x00026;author=Y.+Tian&#x00026;publication_year=2024&#x00026;title=EEG+decoding+for+musical+emotion+with+functional+connectivity+features&#x00026;journal=Biomed.+Signal+Process.+Control&#x00026;volume=89&#x00026;pages=105744\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref106\" id=\"ref106\">\u003C/a>Yang, J. (2021). A novel music emotion recognition model using neural network technology. \u003Ci>Front. Psychol.\u003C/i> 12:760060. doi: 10.3389/fpsyg.2021.760060 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/34650499\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3389/fpsyg.2021.760060\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=J.+Yang&#x00026;publication_year=2021&#x00026;title=A+novel+music+emotion+recognition+model+using+neural+network+technology&#x00026;journal=Front.+Psychol.&#x00026;volume=12&#x00026;pages=760060\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref107\" id=\"ref107\">\u003C/a>Yang, H., Han, J., and Min, K. (2019). A multi-column CNN model for emotion recognition from EEG signals. \u003Ci>Sensors\u003C/i> 19:4736. doi: 10.3390/s19214736 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/31683608\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3390/s19214736\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=H.+Yang&#x00026;author=J.+Han&#x00026;author=K.+Min&#x00026;publication_year=2019&#x00026;title=A+multi-column+CNN+model+for+emotion+recognition+from+EEG+signals&#x00026;journal=Sensors&#x00026;volume=19&#x00026;pages=4736\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref108\" id=\"ref108\">\u003C/a>Yang, K., Kim, H. M., and Zimmerman, J. (2020). Emotional branding on fashion brand websites: harnessing the pleasure-arousal-dominance (P-a-D) model. \u003Ci>J. Fash. Mark. Manag.\u003C/i> 24, 555&#x2013;570. doi: 10.1108/JFMM-03-2019-0055\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1108/JFMM-03-2019-0055\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=K.+Yang&#x00026;author=H.+M.+Kim&#x00026;author=J.+Zimmerman&#x00026;publication_year=2020&#x00026;title=Emotional+branding+on+fashion+brand+websites:+harnessing+the+pleasure-arousal-dominance+(P-a-D)+model&#x00026;journal=J.+Fash.+Mark.+Manag.&#x00026;volume=24&#x00026;pages=555-570\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref109\" id=\"ref109\">\u003C/a>Yang, W., Makita, K., Nakao, T., Kanayama, N., Machizawa, M. G., Sasaoka, T., et al. (2018). Affective auditory stimulus database: an expanded version of the international affective digitized sounds (IADS-E). \u003Ci>Behav. Res. Methods\u003C/i> 50, 1415&#x2013;1429. doi: 10.3758/s13428-018-1027-6 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/29520632\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3758/s13428-018-1027-6\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=W.+Yang&#x00026;author=K.+Makita&#x00026;author=T.+Nakao&#x00026;author=N.+Kanayama&#x00026;author=M.+G.+Machizawa&#x00026;author=T.+Sasaoka&#x00026;publication_year=2018&#x00026;title=Affective+auditory+stimulus+database:+an+expanded+version+of+the+international+affective+digitized+sounds+(IADS-E)&#x00026;journal=Behav.+Res.+Methods&#x00026;volume=50&#x00026;pages=1415-1429\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref110\" id=\"ref110\">\u003C/a>Yu, M., Xiao, S., Hua, M., Wang, H., Chen, X., Tian, F., et al. (2022). EEG-based emotion recognition in an immersive virtual reality environment: from local activity to brain network features. \u003Ci>Biomed. Signal Process. Control\u003C/i> 72:103349. doi: 10.1016/j.bspc.2021.103349\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.bspc.2021.103349\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Yu&#x00026;author=S.+Xiao&#x00026;author=M.+Hua&#x00026;author=H.+Wang&#x00026;author=X.+Chen&#x00026;author=F.+Tian&#x00026;publication_year=2022&#x00026;title=EEG-based+emotion+recognition+in+an+immersive+virtual+reality+environment:+from+local+activity+to+brain+network+features&#x00026;journal=Biomed.+Signal+Process.+Control&#x00026;volume=72&#x00026;pages=103349\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref111\" id=\"ref111\">\u003C/a>Zentner, M., Grandjean, D., and Scherer, K. R. (2008). Emotions evoked by the sound of music: characterization, classification, and measurement. \u003Ci>Emotion\u003C/i> 8, 494&#x2013;521. doi: 10.1037/1528-3542.8.4.494\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1037/1528-3542.8.4.494\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Zentner&#x00026;author=D.+Grandjean&#x00026;author=K.+R.+Scherer&#x00026;publication_year=2008&#x00026;title=Emotions+evoked+by+the+sound+of+music:+characterization+classification+and+measurement&#x00026;journal=Emotion&#x00026;volume=8&#x00026;pages=494-521\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref112\" id=\"ref112\">\u003C/a>Zhang, M., Ding, Y., Zhang, J., Jiang, X., Xu, N., Zhang, L., et al. (2022). Effect of group impromptu music therapy on emotional regulation and depressive symptoms of college students: a randomized controlled study. \u003Ci>Front. Psychol.\u003C/i> 13:851526. doi: 10.3389/fpsyg.2022.851526 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/35432107\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3389/fpsyg.2022.851526\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Zhang&#x00026;author=Y.+Ding&#x00026;author=J.+Zhang&#x00026;author=X.+Jiang&#x00026;author=N.+Xu&#x00026;author=L.+Zhang&#x00026;publication_year=2022&#x00026;title=Effect+of+group+impromptu+music+therapy+on+emotional+regulation+and+depressive+symptoms+of+college+students:+a+randomized+controlled+study&#x00026;journal=Front.+Psychol.&#x00026;volume=13&#x00026;pages=851526\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref113\" id=\"ref113\">\u003C/a>Zhang, L., Xia, B., Wang, Y., Zhang, W., and Han, Y. (2023). A fine-grained approach for EEG-based emotion recognition using clustering and hybrid deep neural networks. \u003Ci>Electronics\u003C/i> 12:4717. doi: 10.3390/electronics12234717\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.3390/electronics12234717\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=L.+Zhang&#x00026;author=B.+Xia&#x00026;author=Y.+Wang&#x00026;author=W.+Zhang&#x00026;author=Y.+Han&#x00026;publication_year=2023&#x00026;title=A+fine-grained+approach+for+EEG-based+emotion+recognition+using+clustering+and+hybrid+deep+neural+networks&#x00026;journal=Electronics&#x00026;volume=12&#x00026;pages=4717\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref114\" id=\"ref114\">\u003C/a>Zheng, W.-L., and Lu, B.-L. (2015). Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks. \u003Ci>IEEE Trans. Auton. Ment. Dev.\u003C/i> 7, 162&#x2013;175. doi: 10.1109/TAMD.2015.2431497\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1109/TAMD.2015.2431497\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=W.-L.+Zheng&#x00026;author=B.-L.+Lu&#x00026;publication_year=2015&#x00026;title=Investigating+critical+frequency+bands+and+channels+for+EEG-based+emotion+recognition+with+deep+neural+networks&#x00026;journal=IEEE+Trans.+Auton.+Ment.+Dev.&#x00026;volume=7&#x00026;pages=162-175\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\" style=\"margin-bottom:0.5em;\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref115\" id=\"ref115\">\u003C/a>Zhong, M., Yang, Q., Liu, Y., Zhen, B., Zhao, F., and Xie, B. (2023). EEG emotion recognition based on TQWT-features and hybrid convolutional recurrent neural network. \u003Ci>Biomed. Signal Process. Control\u003C/i> 79:104211. doi: 10.1016/j.bspc.2022.104211\u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://doi.org/10.1016/j.bspc.2022.104211\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=M.+Zhong&#x00026;author=Q.+Yang&#x00026;author=Y.+Liu&#x00026;author=B.+Zhen&#x00026;author=F.+Zhao&#x00026;author=B.+Xie&#x00026;publication_year=2023&#x00026;title=EEG+emotion+recognition+based+on+TQWT-features+and+hybrid+convolutional+recurrent+neural+network&#x00026;journal=Biomed.+Signal+Process.+Control&#x00026;volume=79&#x00026;pages=104211\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref116\" id=\"ref116\">\u003C/a>Zhou, Y., and Lian, J. (2023). Identification of emotions evoked by music via spatial-temporal transformer in multi-channel EEG signals. \u003Ci>Front. Neurosci.\u003C/i> 17:1188696. doi: 10.3389/fnins.2023.1188696 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/37483354\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3389/fnins.2023.1188696\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=Y.+Zhou&#x00026;author=J.+Lian&#x00026;publication_year=2023&#x00026;title=Identification+of+emotions+evoked+by+music+via+spatial-temporal+transformer+in+multi-channel+EEG+signals&#x00026;journal=Front.+Neurosci.&#x00026;volume=17&#x00026;pages=1188696\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003Cdiv class=\"References\">\n\u003Cp class=\"ReferencesCopy1\">\u003Ca name=\"ref117\" id=\"ref117\">\u003C/a>Zhou, T. H., Liang, W., Liu, H., Wang, L., Ryu, K. H., and Nam, K. W. (2022). EEG emotion recognition applied to the effect analysis of music on emotion changes in psychological healthcare. \u003Ci>Int. J. Environ. Res. Public Health\u003C/i> 20:378. doi: 10.3390/ijerph20010378 \u003C/p>\n\u003Cp class=\"ReferencesCopy2\">\u003Ca href=\"https://pubmed.ncbi.nlm.nih.gov/36612700\" target=\"_blank\">PubMed Abstract\u003C/a> | \u003Ca href=\"https://doi.org/10.3390/ijerph20010378\" target=\"_blank\">Crossref Full Text\u003C/a> | \u003Ca href=\"http://scholar.google.com/scholar_lookup?author=T.+H.+Zhou&#x00026;author=W.+Liang&#x00026;author=H.+Liu&#x00026;author=L.+Wang&#x00026;author=K.+H.+Ryu&#x00026;author=K.+W.+Nam&#x00026;publication_year=2022&#x00026;title=EEG+emotion+recognition+applied+to+the+effect+analysis+of+music+on+emotion+changes+in+psychological+healthcare&#x00026;journal=Int.+J.+Environ.+Res.+Public+Health&#x00026;volume=20&#x00026;pages=378\" target=\"_blank\">Google Scholar\u003C/a>\u003C/p>\u003C/div>\n\u003C/div> \u003Cdiv class=\"thinLineM20\">\u003C/div> \u003Cdiv class=\"AbstractSummary\">\n\u003Cp>\u003Cspan>Keywords:\u003C/span> music-induced, emotion recognition, artificial intelligence, personalization, applications\u003C/p>\n\u003Cp>\u003Cspan>Citation:\u003C/span> Su Y, Liu Y, Xiao Y, Ma J and Li D (2024) A review of artificial intelligence methods enabled music-evoked EEG emotion recognition and their applications. \u003Ci>Front. Neurosci\u003C/i>. 18:1400444. doi: 10.3389/fnins.2024.1400444\u003C/p>\n\u003Cp class=\"timestamps\">\u003Cspan>Received:\u003C/span> 17 April 2024; \u003Cspan>Accepted:\u003C/span> 14 August 2024;\u003Cbr> \u003Cspan>Published:\u003C/span> 04 September 2024.\u003C/p> \u003Cdiv>\n\u003Cp>Edited by:\u003C/p> \u003Ca href=\"https://loop.frontiersin.org/people/29627/overview\">Dan Zhang\u003C/a>, Tsinghua University, China\u003C/div> \u003Cdiv>\n\u003Cp>Reviewed by:\u003C/p> \u003Ca href=\"https://loop.frontiersin.org/people/1583946/overview\">Jun Jiang\u003C/a>, Shanghai Normal University, China\u003Cbr> \u003Ca href=\"https://loop.frontiersin.org/people/2086612/overview\">Wen Li\u003C/a>, Universiti Sains Malaysia, Malaysia\u003C/div>\n\u003Cp>\u003Cspan>Copyright\u003C/span> &#x000A9; 2024 Su, Liu, Xiao, Ma and Li. This is an open-access article distributed under the terms of the \u003Ca rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\" target=\"_blank\">Creative Commons Attribution License (CC BY)\u003C/a>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.\u003C/p>\n\u003Cp>\u003Cspan>&#x0002A;Correspondence:\u003C/span> Dezhao Li, \u003Ca id=\"encmail\">ZGxpYWVAY29ubmVjdC51c3QuaGs=\u003C/a>\u003C/p> \u003Cdiv class=\"clear\">\u003C/div> \u003C/div>","\u003Cul class=\"flyoutJournal\"> \u003Cli>\u003Ca href=\"#h1\">Abstract\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h2\">1 Introduction\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h3\">2 EEG signal and emotions\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h4\">3 Preprocessing and feature extraction of EEG signals\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h5\">4 Emotion data source and modeling\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h6\">5 Artificial intelligence algorithms for EEG emotion recognition\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h7\">6 Application examples and analysis\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h8\">7 Discussion and conclusions\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h9\">Author contributions\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h10\">Funding\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h11\">Conflict of interest\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h12\">Publisher&#x2019;s note\u003C/a>\u003C/li> \u003Cli>\u003Ca href=\"#h13\">References\u003C/a>\u003C/li> \u003C/ul>",[608,614,620],{"name":609,"fileServerPackageEntryId":19,"fileServerId":610,"fileServerVersionNumber":115,"type":611},"EPUB.epub","1400444/epub",{"code":612,"name":613},"EPUB","epub",{"name":615,"fileServerPackageEntryId":19,"fileServerId":616,"fileServerVersionNumber":115,"type":617},"Publishers-proof.pdf","1400444/publishers-proof",{"code":618,"name":619},"PDF","pdf",{"name":621,"fileServerPackageEntryId":622,"fileServerId":623,"fileServerVersionNumber":115,"type":624},"fnins-18-1400444.xml","fnins-18-1400444/fnins-18-1400444.xml","1400444/xml",{"code":625,"name":626},"NLM_XML","xml","fnins-18-1400444-HTML","v3",{"title":630,"link":631,"meta":635,"script":741},"Frontiers | A review of artificial intelligence methods enabled music-evoked EEG emotion recognition and their applications",[632],{"rel":633,"href":634},"canonical","https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/full",[636,639,642,644,647,651,654,658,661,664,667,669,671,673,675,677,680,683,685,688,690,692,695,698,701,704,707,711,715,718,721,724,727,730,733,736,738],{"hid":637,"property":637,"name":637,"content":638},"description","Music is an archaic form of emotional expression and arousal that can induce strong emotional experiences in listeners, which has important research and prac...",{"hid":640,"property":640,"name":641,"content":630},"og:title","title",{"hid":643,"property":643,"name":637,"content":638},"og:description",{"hid":645,"name":645,"content":646},"keywords","artificial intelligence,emotion recognition,music-induced,personalization,applications",{"hid":648,"property":648,"name":649,"content":650},"og:site_name","site_name","Frontiers",{"hid":652,"property":652,"name":363,"content":653},"og:image","https://images-provider.frontiersin.org/api/ipx/w=1200&f=png/https://www.frontiersin.org/files/Articles/1400444/fnins-18-1400444-HTML/image_m/fnins-18-1400444-g001.jpg",{"hid":655,"property":655,"name":656,"content":657},"og:type","type","article",{"hid":659,"property":659,"name":660,"content":634},"og:url","url",{"hid":662,"name":662,"content":663},"twitter:card","summary_large_image",{"hid":665,"name":665,"content":666},"citation_volume","18",{"hid":668,"name":668,"content":116},"citation_journal_title",{"hid":670,"name":670,"content":650},"citation_publisher",{"hid":672,"name":672,"content":590},"citation_journal_abbrev",{"hid":674,"name":674,"content":591},"citation_issn",{"hid":676,"name":676,"content":512},"citation_doi",{"hid":678,"name":678,"content":679},"citation_firstpage","1400444",{"hid":681,"name":681,"content":682},"citation_language","English",{"hid":684,"name":684,"content":513},"citation_title",{"hid":686,"name":686,"content":687},"citation_keywords","artificial intelligence; emotion recognition; music-induced; personalization; applications",{"hid":689,"name":689,"content":518},"citation_abstract",{"hid":691,"name":691,"content":521},"citation_article_type",{"hid":693,"name":693,"content":694},"citation_pdf_url","https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/pdf",{"hid":696,"name":696,"content":697},"citation_xml_url","https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1400444/xml",{"hid":699,"name":699,"content":700},"citation_fulltext_world_readable","yes",{"hid":702,"name":702,"content":703},"citation_online_date","2024/08/14",{"hid":705,"name":705,"content":706},"citation_publication_date","2024/09/04",{"hid":708,"name":709,"content":710},"citation_author_0","citation_author","Su, Yan ",{"hid":712,"name":713,"content":714},"citation_author_institution_0","citation_author_institution","School of Art, Zhejiang International Studies University, China",{"hid":716,"name":709,"content":717},"citation_author_1","Liu, Yong ",{"hid":719,"name":713,"content":720},"citation_author_institution_1","School of Education, Hangzhou Normal University, China",{"hid":722,"name":709,"content":723},"citation_author_2","Xiao, Yan ",{"hid":725,"name":713,"content":726},"citation_author_institution_2","School of Arts and Media, Beijing Normal University, China",{"hid":728,"name":709,"content":729},"citation_author_3","Ma, Jiaqi ",{"hid":731,"name":713,"content":732},"citation_author_institution_3","College of Science, Zhejiang University of Technology, China",{"hid":734,"name":709,"content":735},"citation_author_4","Li, Dezhao ",{"hid":737,"name":713,"content":732},"citation_author_institution_4",{"hid":739,"name":739,"content":740},"dc.identifier","doi:10.3389/fnins.2024.1400444",[742,745,747,749,751],{"src":743,"body":13,"type":744,"async":13},"https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6","text/javascript",{"src":746,"body":13,"type":744,"async":13},"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML",{"src":748,"body":13,"type":744,"async":13},"https://d1bxh8uas1mnw7.cloudfront.net/assets/altmetric_badges-f0bc9b243ff5677d05460c1eb71834ca998946d764eb3bc244ab4b18ba50d21e.js",{"src":750,"body":13,"type":744,"async":13},"https://api.altmetric.com/v1/doi/10.3389/fnins.2024.1400444?callback=_altmetric.embed_callback&domain=www.frontiersin.org&key=3c130976ca2b8f2e88f8377633751ba1&cache_until=14-15",{"src":752,"body":13,"type":744,"async":13},"https://crossmark-cdn.crossref.org/widget/v2.0/widget.js",{"articleHubSlug":19,"articleHubPage":754,"articleHubArticlesList":755,"canJournalHasArticleHub":355,"articleDoiList":756},{},[],[],{"title":19,"image":-1,"breadcrumbs":758,"linksCollection":759,"metricsCollection":761},[],{"total":367,"items":760},[],{"total":367,"items":762},[]]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{isDevMode:false,appName:"article-pages-2024",baseUrl:"https://www.frontiersin.org",domain:"frontiersin.org",loopUrl:"https://loop.frontiersin.org",ssMainDomain:"frontiersin.org",contentfulUrl:"https://graphql.contentful.com/content/v1/spaces",spaceId:1,spaceName:"Frontiers",liveSpaceId:1,tenantLogoUrl:"",frontiersGraphUrl:"https://apollo-federation-gateway.frontiersin.org",registrationApiUrl:"https://onboarding-ui.frontiersin.org",emailDigestApiUrl:"https://api-email-digest.frontiersin.org",journalFilesApiUrl:"https://files-journal-api.frontiersin.org",searchApiUrl:"https://search-api.frontiersin.org",productionForumApiUrl:"https://production-forum-api-v2.frontiersin.org",gtmServerUrl:"https://tag-manager.frontiersin.org",gtmId:"GTM-M322FV2",gtmAuth:"owVbWxfaJr21yQv1fe1cAQ",gtmPreview:"env-1",figsharePluginUrl:"https://widgets.figshare.com/static/figshare.js",figshareApiUrl:"https://api.figshare.com/v2/collections/search",figshareTimeout:3000,crossmarkPublishedDate:"2015/08/24",googleRecaptchaSiteKey:"6LdG3i0UAAAAAOC4qUh35ubHgJotEHp_STXHgr_v",googleRecaptchaKeyName:"FrontiersRecaptchaV2",faviconSize16:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_16-tenantFavicon-Frontiers.png",faviconSize32:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_32-tenantFavicon-Frontiers.png",faviconSize180:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_180-tenantFavicon-Frontiers.png",faviconSize192:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_192-tenantFavicon-Frontiers.png",faviconSize512:"https://static2.frontiersin.org/static-resources/tenants/frontiers/favicon_512-tenantFavicon-Frontiers.png",socialMediaImg:"https://brand.frontiersin.org/m/1c8bcb536c789e11/Guidelines-Frontiers_Logo_1200x628_1-91to1.png",linkedArticleCopyText:"'{\"articleTypeCopyText\":[{\"articleTypeId\":0,\"originalArticleCopyText\":\"Part of this article's content has been mentioned in:\",\"linkedArticleCopyText\":\"This article mentions parts of:\"},{\"articleTypeId\":122,\"originalArticleCopyText\":\"Parts of this article's content have been modified or rectified in:\",\"linkedArticleCopyText\":\"This article is an erratum on:\"},{\"articleTypeId\":129,\"originalArticleCopyText\":\"Parts of this article's content have been modified or rectified in:\",\"linkedArticleCopyText\":\"This article is an addendum to:\"},{\"articleTypeId\":128,\"originalArticleCopyText\":\"A correction has been applied to this article in:\",\"linkedArticleCopyText\":\"This article is a correction to:\"},{\"articleTypeId\":134,\"originalArticleCopyText\":\"A retraction of this article was approved in:\",\"linkedArticleCopyText\":\"This article is a retraction of:\"},{\"articleTypeId\":29,\"originalArticleCopyText\":\"A commentary has been posted on this article:\",\"linkedArticleCopyText\":\"This article is a commentary on:\"},{\"articleTypeId\":30,\"originalArticleCopyText\":\"A commentary has been posted on this article:\",\"linkedArticleCopyText\":\"This article is a commentary on:\"}],\"articleIdCopyText\":[]}'\n",acceptedArticleExcludedJournalIds:"'[2766,979]'\n",articleTypeConfigurableLabel:"\u003C\u003Carticle-type:uppercase>> article",settingsFeaturesSwitchers:"'{\"displayTitlePillLabels\":true,\"displayRelatedArticlesBox\":true,\"showEditors\":true,\"showReviewers\":true,\"showLoopImpactLink\":true,\"enableFigshare\":false,\"useXmlImages\":true}'\n",journalsWithArticleHub:"'{\"strings\":[\"science\"]}'\n",terminologySettings:"'{\"terms\":[{\"sequenceNumber\":1,\"key\":\"frontiers\",\"tenantTerm\":\"Frontiers\",\"frontiersDefaultTerm\":\"Frontiers\",\"category\":\"Customer\"},{\"sequenceNumber\":2,\"key\":\"submission_system\",\"tenantTerm\":\"submission system\",\"frontiersDefaultTerm\":\"submission system\",\"category\":\"Product\"},{\"sequenceNumber\":3,\"key\":\"public_pages\",\"tenantTerm\":\"public pages\",\"frontiersDefaultTerm\":\"public pages\",\"category\":\"Product\"},{\"sequenceNumber\":4,\"key\":\"my_frontiers\",\"tenantTerm\":\"my frontiers\",\"frontiersDefaultTerm\":\"my frontiers\",\"category\":\"Product\"},{\"sequenceNumber\":5,\"key\":\"digital_editorial_office\",\"tenantTerm\":\"digital editorial office\",\"frontiersDefaultTerm\":\"digital editorial office\",\"category\":\"Product\"},{\"sequenceNumber\":6,\"key\":\"deo\",\"tenantTerm\":\"DEO\",\"frontiersDefaultTerm\":\"DEO\",\"category\":\"Product\"},{\"sequenceNumber\":7,\"key\":\"digital_editorial_office_for_chiefs\",\"tenantTerm\":\"digital editorial office for chiefs\",\"frontiersDefaultTerm\":\"digital editorial office for chiefs\",\"category\":\"Product\"},{\"sequenceNumber\":8,\"key\":\"digital_editorial_office_for_eof\",\"tenantTerm\":\"digital editorial office for eof\",\"frontiersDefaultTerm\":\"digital editorial office for eof\",\"category\":\"Product\"},{\"sequenceNumber\":9,\"key\":\"editorial_office\",\"tenantTerm\":\"editorial office\",\"frontiersDefaultTerm\":\"editorial office\",\"category\":\"Product\"},{\"sequenceNumber\":10,\"key\":\"eof\",\"tenantTerm\":\"EOF\",\"frontiersDefaultTerm\":\"EOF\",\"category\":\"Product\"},{\"sequenceNumber\":11,\"key\":\"research_topic_management\",\"tenantTerm\":\"research topic management\",\"frontiersDefaultTerm\":\"research topic management\",\"category\":\"Product\"},{\"sequenceNumber\":12,\"key\":\"review_forum\",\"tenantTerm\":\"review forum\",\"frontiersDefaultTerm\":\"review forum\",\"category\":\"Product\"},{\"sequenceNumber\":13,\"key\":\"accounting_office\",\"tenantTerm\":\"accounting office\",\"frontiersDefaultTerm\":\"accounting office\",\"category\":\"Product\"},{\"sequenceNumber\":14,\"key\":\"aof\",\"tenantTerm\":\"AOF\",\"frontiersDefaultTerm\":\"AOF\",\"category\":\"Product\"},{\"sequenceNumber\":15,\"key\":\"publishing_office\",\"tenantTerm\":\"publishing office\",\"frontiersDefaultTerm\":\"publishing office\",\"category\":\"Product\"},{\"sequenceNumber\":16,\"key\":\"production_office\",\"tenantTerm\":\"production office forum\",\"frontiersDefaultTerm\":\"production office forum\",\"category\":\"Product\"},{\"sequenceNumber\":17,\"key\":\"pof\",\"tenantTerm\":\"POF\",\"frontiersDefaultTerm\":\"POF\",\"category\":\"Product\"},{\"sequenceNumber\":18,\"key\":\"book_office_forum\",\"tenantTerm\":\"book office forum\",\"frontiersDefaultTerm\":\"book office forum\",\"category\":\"Product\"},{\"sequenceNumber\":19,\"key\":\"bof\",\"tenantTerm\":\"BOF\",\"frontiersDefaultTerm\":\"BOF\",\"category\":\"Product\"},{\"sequenceNumber\":20,\"key\":\"aira\",\"tenantTerm\":\"AIRA\",\"frontiersDefaultTerm\":\"AIRA\",\"category\":\"Product\"},{\"sequenceNumber\":21,\"key\":\"editorial_board_management\",\"tenantTerm\":\"editorial board management\",\"frontiersDefaultTerm\":\"editorial board management\",\"category\":\"Product\"},{\"sequenceNumber\":22,\"key\":\"ebm\",\"tenantTerm\":\"EBM\",\"frontiersDefaultTerm\":\"EBM\",\"category\":\"Product\"},{\"sequenceNumber\":23,\"key\":\"domain\",\"tenantTerm\":\"domain\",\"frontiersDefaultTerm\":\"domain\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":24,\"key\":\"journal\",\"tenantTerm\":\"journal\",\"frontiersDefaultTerm\":\"journal\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":25,\"key\":\"section\",\"tenantTerm\":\"section\",\"frontiersDefaultTerm\":\"section\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":26,\"key\":\"domains\",\"tenantTerm\":\"domains\",\"frontiersDefaultTerm\":\"domains\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":27,\"key\":\"specialty_section\",\"tenantTerm\":\"specialty section\",\"frontiersDefaultTerm\":\"specialty section\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":28,\"key\":\"specialty_journal\",\"tenantTerm\":\"specialty journal\",\"frontiersDefaultTerm\":\"specialty journal\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":29,\"key\":\"journals\",\"tenantTerm\":\"journals\",\"frontiersDefaultTerm\":\"journals\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":30,\"key\":\"sections\",\"tenantTerm\":\"sections\",\"frontiersDefaultTerm\":\"sections\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":31,\"key\":\"specialty_sections\",\"tenantTerm\":\"specialty sections\",\"frontiersDefaultTerm\":\"specialty sections\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":32,\"key\":\"specialty_journals\",\"tenantTerm\":\"specialty journals\",\"frontiersDefaultTerm\":\"specialty journals\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":33,\"key\":\"manuscript\",\"tenantTerm\":\"manuscript\",\"frontiersDefaultTerm\":\"manuscript\",\"category\":\"Core\"},{\"sequenceNumber\":34,\"key\":\"manuscripts\",\"tenantTerm\":\"manuscripts\",\"frontiersDefaultTerm\":\"manuscripts\",\"category\":\"Core\"},{\"sequenceNumber\":35,\"key\":\"article\",\"tenantTerm\":\"article\",\"frontiersDefaultTerm\":\"article\",\"category\":\"Core\"},{\"sequenceNumber\":36,\"key\":\"articles\",\"tenantTerm\":\"articles\",\"frontiersDefaultTerm\":\"articles\",\"category\":\"Core\"},{\"sequenceNumber\":37,\"key\":\"article_type\",\"tenantTerm\":\"article type\",\"frontiersDefaultTerm\":\"article type\",\"category\":\"Core\"},{\"sequenceNumber\":38,\"key\":\"article_types\",\"tenantTerm\":\"article types\",\"frontiersDefaultTerm\":\"article types\",\"category\":\"Core\"},{\"sequenceNumber\":39,\"key\":\"author\",\"tenantTerm\":\"author\",\"frontiersDefaultTerm\":\"author\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":40,\"key\":\"authors\",\"tenantTerm\":\"authors\",\"frontiersDefaultTerm\":\"authors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":41,\"key\":\"authoring\",\"tenantTerm\":\"authoring\",\"frontiersDefaultTerm\":\"authoring\",\"category\":\"Core\"},{\"sequenceNumber\":42,\"key\":\"authored\",\"tenantTerm\":\"authored\",\"frontiersDefaultTerm\":\"authored\",\"category\":\"Core\"},{\"sequenceNumber\":43,\"key\":\"accept\",\"tenantTerm\":\"accept\",\"frontiersDefaultTerm\":\"accept\",\"category\":\"Process\"},{\"sequenceNumber\":44,\"key\":\"accepted\",\"tenantTerm\":\"accepted\",\"frontiersDefaultTerm\":\"accepted\",\"category\":\"Process\"},{\"sequenceNumber\":45,\"key\":\"assistant_field_chief_editor\",\"tenantTerm\":\"Assistant Field Chief Editor\",\"frontiersDefaultTerm\":\"Assistant Field Chief Editor\",\"description\":\"An editorial role on a Field Journal that a Registered User may hold. This gives them rights to different functionality and parts of the platform\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":46,\"key\":\"assistant_specialty_chief_editor\",\"tenantTerm\":\"Assistant Specialty Chief Editor\",\"frontiersDefaultTerm\":\"Assistant Specialty Chief Editor\",\"description\":\"An editorial role on a specialty that a Registered User may hold. This gives them rights to different functionality and parts of the platform\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":47,\"key\":\"assistant_specialty_chief_editors\",\"tenantTerm\":\"Assistant Specialty Chief Editors\",\"frontiersDefaultTerm\":\"Assistant Specialty Chief Editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":48,\"key\":\"associate_editor\",\"tenantTerm\":\"Associate Editor\",\"frontiersDefaultTerm\":\"Associate Editor\",\"description\":\"An editorial role on a specialty that a Registered User may hold. This gives them rights to different functionality and parts of the platform\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":49,\"key\":\"specialty_chief_editor\",\"tenantTerm\":\"Specialty Chief Editor\",\"frontiersDefaultTerm\":\"Specialty Chief Editor\",\"description\":\"An editorial role on a specialty that a Registered User may hold. This gives them rights to different functionality and parts of the platform\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":50,\"key\":\"specialty_chief_editors\",\"tenantTerm\":\"Specialty Chief Editors\",\"frontiersDefaultTerm\":\"Specialty Chief Editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":51,\"key\":\"chief_editor\",\"tenantTerm\":\"Chief Editor\",\"frontiersDefaultTerm\":\"Chief Editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":52,\"key\":\"chief_editors\",\"tenantTerm\":\"Chief Editors\",\"frontiersDefaultTerm\":\"Chief Editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":53,\"key\":\"call_for_participation\",\"tenantTerm\":\"call for participation\",\"frontiersDefaultTerm\":\"call for participation\",\"category\":\"Process\"},{\"sequenceNumber\":54,\"key\":\"citation\",\"tenantTerm\":\"citation\",\"frontiersDefaultTerm\":\"citation\",\"category\":\"Misc.\"},{\"sequenceNumber\":55,\"key\":\"citations\",\"tenantTerm\":\"citations\",\"frontiersDefaultTerm\":\"citations\",\"category\":\"Misc.\"},{\"sequenceNumber\":56,\"key\":\"contributor\",\"tenantTerm\":\"contributor\",\"frontiersDefaultTerm\":\"contributor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":57,\"key\":\"contributors\",\"tenantTerm\":\"contributors\",\"frontiersDefaultTerm\":\"contributors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":58,\"key\":\"corresponding_author\",\"tenantTerm\":\"corresponding author\",\"frontiersDefaultTerm\":\"corresponding author\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":59,\"key\":\"corresponding_authors\",\"tenantTerm\":\"corresponding authors\",\"frontiersDefaultTerm\":\"corresponding authors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":60,\"key\":\"decline\",\"tenantTerm\":\"decline\",\"frontiersDefaultTerm\":\"decline\",\"category\":\"Process\"},{\"sequenceNumber\":61,\"key\":\"declined\",\"tenantTerm\":\"declined\",\"frontiersDefaultTerm\":\"declined\",\"category\":\"Process\"},{\"sequenceNumber\":62,\"key\":\"reject\",\"tenantTerm\":\"reject\",\"frontiersDefaultTerm\":\"reject\",\"category\":\"Process\"},{\"sequenceNumber\":63,\"key\":\"rejected\",\"tenantTerm\":\"rejected\",\"frontiersDefaultTerm\":\"rejected\",\"category\":\"Process\"},{\"sequenceNumber\":64,\"key\":\"publish\",\"tenantTerm\":\"publish\",\"frontiersDefaultTerm\":\"publish\",\"category\":\"Core\"},{\"sequenceNumber\":65,\"key\":\"published\",\"tenantTerm\":\"published\",\"frontiersDefaultTerm\":\"published\",\"category\":\"Core\"},{\"sequenceNumber\":66,\"key\":\"publication\",\"tenantTerm\":\"publication\",\"frontiersDefaultTerm\":\"publication\",\"category\":\"Core\"},{\"sequenceNumber\":67,\"key\":\"peer_review\",\"tenantTerm\":\"peer review\",\"frontiersDefaultTerm\":\"peer review\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":68,\"key\":\"peer_reviewed\",\"tenantTerm\":\"peer reviewed\",\"frontiersDefaultTerm\":\"peer reviewed\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":69,\"key\":\"initial_validation\",\"tenantTerm\":\"initial validation\",\"frontiersDefaultTerm\":\"initial validation\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":70,\"key\":\"editorial_assignment\",\"tenantTerm\":\"editorial assignment\",\"frontiersDefaultTerm\":\"editorial assignment\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":71,\"key\":\"independent_review\",\"tenantTerm\":\"independent review\",\"frontiersDefaultTerm\":\"independent review\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":72,\"key\":\"interactive_review\",\"tenantTerm\":\"interactive review\",\"frontiersDefaultTerm\":\"interactive review\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":73,\"key\":\"review\",\"tenantTerm\":\"review\",\"frontiersDefaultTerm\":\"review\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":74,\"key\":\"reviewing\",\"tenantTerm\":\"reviewing\",\"frontiersDefaultTerm\":\"reviewing\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":75,\"key\":\"reviewer\",\"tenantTerm\":\"reviewer\",\"frontiersDefaultTerm\":\"reviewer\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":76,\"key\":\"reviewers\",\"tenantTerm\":\"reviewers\",\"frontiersDefaultTerm\":\"reviewers\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":77,\"key\":\"review_finalized\",\"tenantTerm\":\"review finalized\",\"frontiersDefaultTerm\":\"review finalized\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":78,\"key\":\"final_decision\",\"tenantTerm\":\"final decision\",\"frontiersDefaultTerm\":\"final decision\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":79,\"key\":\"final_validation\",\"tenantTerm\":\"final validation\",\"frontiersDefaultTerm\":\"final validation\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":80,\"key\":\"ae_accept_manuscript\",\"tenantTerm\":\"recommend to accept manuscript\",\"frontiersDefaultTerm\":\"accept manuscript\",\"category\":\"Process\"},{\"sequenceNumber\":81,\"key\":\"fee\",\"tenantTerm\":\"fee\",\"frontiersDefaultTerm\":\"fee\",\"category\":\"Accounting\"},{\"sequenceNumber\":82,\"key\":\"fees\",\"tenantTerm\":\"fees\",\"frontiersDefaultTerm\":\"fees\",\"category\":\"Accounting\"},{\"sequenceNumber\":83,\"key\":\"guest_associate_editor\",\"tenantTerm\":\"Guest Associate Editor\",\"frontiersDefaultTerm\":\"Guest Associate Editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":84,\"key\":\"guest_associate_editors\",\"tenantTerm\":\"Guest Associate Editors\",\"frontiersDefaultTerm\":\"Guest Associate Editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":85,\"key\":\"in_review\",\"tenantTerm\":\"in review\",\"frontiersDefaultTerm\":\"in review\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":86,\"key\":\"institutional_member\",\"tenantTerm\":\"institutional partner\",\"frontiersDefaultTerm\":\"institutional partner\",\"category\":\"Accounting\"},{\"sequenceNumber\":87,\"key\":\"institutional_membership\",\"tenantTerm\":\"institutional partnership\",\"frontiersDefaultTerm\":\"institutional partnership\",\"category\":\"Accounting\"},{\"sequenceNumber\":88,\"key\":\"article_processing_charge\",\"tenantTerm\":\"article processing charge\",\"frontiersDefaultTerm\":\"article processing charge\",\"category\":\"Accounting\"},{\"sequenceNumber\":89,\"key\":\"article_processing_charges\",\"tenantTerm\":\"article processing charges\",\"frontiersDefaultTerm\":\"article processing charges\",\"category\":\"Accounting\"},{\"sequenceNumber\":90,\"key\":\"apcs\",\"tenantTerm\":\"APCs\",\"frontiersDefaultTerm\":\"APCs\",\"category\":\"Accounting\"},{\"sequenceNumber\":91,\"key\":\"apc\",\"tenantTerm\":\"APC\",\"frontiersDefaultTerm\":\"APC\",\"category\":\"Accounting\"},{\"sequenceNumber\":92,\"key\":\"received\",\"tenantTerm\":\"received\",\"frontiersDefaultTerm\":\"received\",\"description\":\"Date manuscript was received on.\",\"category\":\"Core\"},{\"sequenceNumber\":93,\"key\":\"transferred\",\"tenantTerm\":\"transferred\",\"frontiersDefaultTerm\":\"transferred\",\"category\":\"Core\"},{\"sequenceNumber\":94,\"key\":\"transfer\",\"tenantTerm\":\"transfer\",\"frontiersDefaultTerm\":\"transfer\",\"category\":\"Core\"},{\"sequenceNumber\":95,\"key\":\"research_topic\",\"tenantTerm\":\"research topic\",\"frontiersDefaultTerm\":\"research topic\",\"category\":\"Core\"},{\"sequenceNumber\":96,\"key\":\"research_topics\",\"tenantTerm\":\"research topics\",\"frontiersDefaultTerm\":\"research topics\",\"category\":\"Core\"},{\"sequenceNumber\":97,\"key\":\"topic_editor\",\"tenantTerm\":\"Topic Editor\",\"frontiersDefaultTerm\":\"Topic Editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":98,\"key\":\"review_editor\",\"tenantTerm\":\"Review Editor\",\"frontiersDefaultTerm\":\"Review Editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":99,\"key\":\"title\",\"tenantTerm\":\"title\",\"frontiersDefaultTerm\":\"title\",\"category\":\"Manuscript Metadata\"},{\"sequenceNumber\":100,\"key\":\"running_title\",\"tenantTerm\":\"running title\",\"frontiersDefaultTerm\":\"running title\",\"category\":\"Manuscript Metadata\"},{\"sequenceNumber\":101,\"key\":\"submit\",\"tenantTerm\":\"submit\",\"frontiersDefaultTerm\":\"submit\",\"category\":\"Process\"},{\"sequenceNumber\":102,\"key\":\"submitted\",\"tenantTerm\":\"submitted\",\"frontiersDefaultTerm\":\"submitted\",\"category\":\"Process\"},{\"sequenceNumber\":103,\"key\":\"submitting\",\"tenantTerm\":\"submitting\",\"frontiersDefaultTerm\":\"submitting\",\"category\":\"Process\"},{\"sequenceNumber\":104,\"key\":\"t_e\",\"tenantTerm\":\"TE\",\"frontiersDefaultTerm\":\"TE\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":105,\"key\":\"topic\",\"tenantTerm\":\"topic\",\"frontiersDefaultTerm\":\"topic\",\"category\":\"Process\"},{\"sequenceNumber\":106,\"key\":\"topic_summary\",\"tenantTerm\":\"topic summary\",\"frontiersDefaultTerm\":\"topic summary\",\"category\":\"Process\"},{\"sequenceNumber\":107,\"key\":\"figure\",\"tenantTerm\":\"figure\",\"frontiersDefaultTerm\":\"figure\",\"category\":\"Manuscript Metadata\"},{\"sequenceNumber\":108,\"key\":\"figures\",\"tenantTerm\":\"figures\",\"frontiersDefaultTerm\":\"figures\",\"category\":\"Manuscript Metadata\"},{\"sequenceNumber\":109,\"key\":\"editorial_file\",\"tenantTerm\":\"editorial file\",\"frontiersDefaultTerm\":\"editorial file\",\"category\":\"Core\"},{\"sequenceNumber\":110,\"key\":\"editorial_files\",\"tenantTerm\":\"editorial files\",\"frontiersDefaultTerm\":\"editorial files\",\"category\":\"Core\"},{\"sequenceNumber\":111,\"key\":\"e_book\",\"tenantTerm\":\"e-book\",\"frontiersDefaultTerm\":\"e-book\",\"category\":\"Core\"},{\"sequenceNumber\":112,\"key\":\"organization\",\"tenantTerm\":\"organization\",\"frontiersDefaultTerm\":\"organization\",\"category\":\"Core\"},{\"sequenceNumber\":113,\"key\":\"institution\",\"tenantTerm\":\"institution\",\"frontiersDefaultTerm\":\"institution\",\"category\":\"Core\"},{\"sequenceNumber\":114,\"key\":\"reference\",\"tenantTerm\":\"reference\",\"frontiersDefaultTerm\":\"reference\",\"category\":\"Manuscript Metadata\"},{\"sequenceNumber\":115,\"key\":\"references\",\"tenantTerm\":\"references\",\"frontiersDefaultTerm\":\"references\",\"category\":\"Manuscript Metadata\"},{\"sequenceNumber\":116,\"key\":\"sce\",\"tenantTerm\":\"SCE\",\"frontiersDefaultTerm\":\"SCE\",\"description\":\"Abbreviation for Specialty Chief Editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":117,\"key\":\"submission\",\"tenantTerm\":\"submission\",\"frontiersDefaultTerm\":\"submission\",\"category\":\"Process\"},{\"sequenceNumber\":118,\"key\":\"submissions\",\"tenantTerm\":\"submissions\",\"frontiersDefaultTerm\":\"submissions\",\"category\":\"Process\"},{\"sequenceNumber\":119,\"key\":\"editing\",\"tenantTerm\":\"editing\",\"frontiersDefaultTerm\":\"editing\",\"category\":\"Process\"},{\"sequenceNumber\":120,\"key\":\"in_preparation\",\"tenantTerm\":\"in preparation\",\"frontiersDefaultTerm\":\"in preparation\",\"category\":\"Process\"},{\"sequenceNumber\":121,\"key\":\"country_region\",\"tenantTerm\":\"country/region\",\"frontiersDefaultTerm\":\"country/region\",\"description\":\"Because of political issues, some of the country listings are actually classified as `regions` and we need to include this. However other clients may not want to do this.\",\"category\":\"Manuscript Metadata\"},{\"sequenceNumber\":122,\"key\":\"countries_regions\",\"tenantTerm\":\"countries/regions\",\"frontiersDefaultTerm\":\"countries/regions\",\"description\":\"Because of political issues, some of the country listings are actually classified as `regions` and we need to include this. However other clients may not want to do this.\",\"category\":\"Manuscript Metadata\"},{\"sequenceNumber\":123,\"key\":\"specialty\",\"tenantTerm\":\"specialty\",\"frontiersDefaultTerm\":\"specialty\",\"category\":\"Core\"},{\"sequenceNumber\":124,\"key\":\"specialties\",\"tenantTerm\":\"specialties\",\"frontiersDefaultTerm\":\"specialties\",\"category\":\"Core\"},{\"sequenceNumber\":125,\"key\":\"associate_editors\",\"tenantTerm\":\"Associate Editors\",\"frontiersDefaultTerm\":\"Associate Editors\",\"description\":\"An editorial role on a specialty that a Registered User may hold. This gives them rights to different functionality and parts of the platform\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":126,\"key\":\"reviewed\",\"tenantTerm\":\"reviewed\",\"frontiersDefaultTerm\":\"reviewed\",\"category\":\"Peer Review Process\"},{\"sequenceNumber\":127,\"key\":\"institutional_members\",\"tenantTerm\":\"institutional partners\",\"frontiersDefaultTerm\":\"institutional partners\",\"category\":\"Accounting\"},{\"sequenceNumber\":128,\"key\":\"institutional_memberships\",\"tenantTerm\":\"institutional partnerships\",\"frontiersDefaultTerm\":\"institutional partnerships\",\"category\":\"Accounting\"},{\"sequenceNumber\":129,\"key\":\"assistant_field_chief_editors\",\"tenantTerm\":\"Assistant Field Chief Editors\",\"frontiersDefaultTerm\":\"Assistant Field Chief Editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":130,\"key\":\"publications\",\"tenantTerm\":\"publications\",\"frontiersDefaultTerm\":\"publications\",\"category\":\"Process\"},{\"sequenceNumber\":131,\"key\":\"ae_accepted\",\"tenantTerm\":\"recommended acceptance\",\"frontiersDefaultTerm\":\"accepted\",\"category\":\"Process\"},{\"sequenceNumber\":132,\"key\":\"field_journal\",\"tenantTerm\":\"field journal\",\"frontiersDefaultTerm\":\"field journal\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":133,\"key\":\"field_journals\",\"tenantTerm\":\"field journals\",\"frontiersDefaultTerm\":\"field journals\",\"category\":\"Taxonomy\"},{\"sequenceNumber\":134,\"key\":\"program_manager\",\"tenantTerm\":\"program manager\",\"frontiersDefaultTerm\":\"program manager\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":135,\"key\":\"journal_manager\",\"tenantTerm\":\"journal manager\",\"frontiersDefaultTerm\":\"journal manager\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":136,\"key\":\"journal_specialist\",\"tenantTerm\":\"journal specialist\",\"frontiersDefaultTerm\":\"journal specialist\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":137,\"key\":\"program_managers\",\"tenantTerm\":\"program managers\",\"frontiersDefaultTerm\":\"program managers\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":138,\"key\":\"journal_managers\",\"tenantTerm\":\"journal managers\",\"frontiersDefaultTerm\":\"journal managers\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":139,\"key\":\"journal_specialists\",\"tenantTerm\":\"journal specialists\",\"frontiersDefaultTerm\":\"journal specialists\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":140,\"key\":\"cover_letter\",\"tenantTerm\":\"manuscript contribution to the field\",\"frontiersDefaultTerm\":\"manuscript contribution to the field\",\"category\":\"Process\"},{\"sequenceNumber\":141,\"key\":\"ae_accepted_manuscript\",\"tenantTerm\":\"recommended to accept manuscript\",\"frontiersDefaultTerm\":\"accepted manuscript\",\"category\":\"Process\"},{\"sequenceNumber\":142,\"key\":\"recommend_for_rejection\",\"tenantTerm\":\"recommend for rejection\",\"frontiersDefaultTerm\":\"recommend for rejection\",\"category\":\"Process\"},{\"sequenceNumber\":143,\"key\":\"recommended_for_rejection\",\"tenantTerm\":\"recommended for rejection\",\"frontiersDefaultTerm\":\"recommended for rejection\",\"category\":\"Process\"},{\"sequenceNumber\":144,\"key\":\"ae\",\"tenantTerm\":\"AE\",\"frontiersDefaultTerm\":\"AE\",\"description\":\"Associate Editor - board member\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":145,\"key\":\"re\",\"tenantTerm\":\"RE\",\"frontiersDefaultTerm\":\"RE\",\"description\":\"Review Editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":146,\"key\":\"rev\",\"tenantTerm\":\"REV\",\"frontiersDefaultTerm\":\"REV\",\"description\":\"Reviewer\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":147,\"key\":\"aut\",\"tenantTerm\":\"AUT\",\"frontiersDefaultTerm\":\"AUT\",\"description\":\"Author\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":148,\"key\":\"coraut\",\"tenantTerm\":\"CORAUT\",\"frontiersDefaultTerm\":\"CORAUT\",\"description\":\"Corresponding author\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":149,\"key\":\"saut\",\"tenantTerm\":\"SAUT\",\"frontiersDefaultTerm\":\"SAUT\",\"description\":\"Submitting author\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":150,\"key\":\"coaut\",\"tenantTerm\":\"COAUT\",\"frontiersDefaultTerm\":\"COAUT\",\"description\":\"co-author\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":151,\"key\":\"tsof\",\"tenantTerm\":\"TSOF\",\"frontiersDefaultTerm\":\"TSOF\",\"description\":\"Typesetter\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":152,\"key\":\"typesetting_office\",\"tenantTerm\":\"typesetting office\",\"frontiersDefaultTerm\":\"typesetting office\",\"category\":\"Product\"},{\"sequenceNumber\":153,\"key\":\"config\",\"tenantTerm\":\"CONFIG\",\"frontiersDefaultTerm\":\"CONFIG\",\"description\":\"Configuration office role\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":154,\"key\":\"jm\",\"tenantTerm\":\"JM\",\"frontiersDefaultTerm\":\"JM\",\"description\":\"Journal Manager\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":155,\"key\":\"rte\",\"tenantTerm\":\"RTE\",\"frontiersDefaultTerm\":\"RTE\",\"description\":\"Research topic editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":156,\"key\":\"organizations\",\"tenantTerm\":\"organizations\",\"frontiersDefaultTerm\":\"organizations\",\"category\":\"Core\"},{\"sequenceNumber\":157,\"key\":\"publishing\",\"tenantTerm\":\"publishing\",\"frontiersDefaultTerm\":\"publishing\",\"category\":\"Core\"},{\"sequenceNumber\":158,\"key\":\"acceptance\",\"tenantTerm\":\"acceptance\",\"frontiersDefaultTerm\":\"acceptance\",\"category\":\"Process\"},{\"sequenceNumber\":159,\"key\":\"preferred_associate_editor\",\"tenantTerm\":\"preferred associate editor\",\"frontiersDefaultTerm\":\"preferred associate editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":160,\"key\":\"topic_editors\",\"tenantTerm\":\"Topic Editors\",\"frontiersDefaultTerm\":\"Topic Editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":161,\"key\":\"institutions\",\"tenantTerm\":\"institutions\",\"frontiersDefaultTerm\":\"institutions\",\"category\":\"Core\"},{\"sequenceNumber\":162,\"key\":\"author(s)\",\"tenantTerm\":\"author(s)\",\"frontiersDefaultTerm\":\"author(s)\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":163,\"key\":\"figure(s)\",\"tenantTerm\":\"figure(s)\",\"frontiersDefaultTerm\":\"figure(s)\",\"category\":\"Manuscript Metadata\"},{\"sequenceNumber\":164,\"key\":\"co-authors\",\"tenantTerm\":\"co-authors\",\"frontiersDefaultTerm\":\"co-authors\",\"description\":\"co-authors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":165,\"key\":\"editorial_board_members\",\"tenantTerm\":\"editorial board members\",\"frontiersDefaultTerm\":\"editorial board members\",\"description\":\"editorial board members\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":166,\"key\":\"editorial_board\",\"tenantTerm\":\"editorial board\",\"frontiersDefaultTerm\":\"editorial board\",\"description\":\"editorial board\",\"category\":\"Product\"},{\"sequenceNumber\":167,\"key\":\"co-authorship\",\"tenantTerm\":\"co-authorship\",\"frontiersDefaultTerm\":\"co-authorship\",\"description\":\"co-authorship\",\"category\":\"Misc.\"},{\"sequenceNumber\":168,\"key\":\"role_id_1\",\"tenantTerm\":\"registration office\",\"frontiersDefaultTerm\":\"registration office\",\"category\":\"User Role\"},{\"sequenceNumber\":169,\"key\":\"role_id_2\",\"tenantTerm\":\"editorial office\",\"frontiersDefaultTerm\":\"editorial office\",\"category\":\"User Role\"},{\"sequenceNumber\":170,\"key\":\"role_id_7\",\"tenantTerm\":\"field chief editor\",\"frontiersDefaultTerm\":\"field chief editor\",\"category\":\"User Role\"},{\"sequenceNumber\":171,\"key\":\"role_id_8\",\"tenantTerm\":\"assistant field chief editor\",\"frontiersDefaultTerm\":\"assistant field chief editor\",\"category\":\"User Role\"},{\"sequenceNumber\":172,\"key\":\"role_id_9\",\"tenantTerm\":\"specialty chief editor\",\"frontiersDefaultTerm\":\"specialty chief editor\",\"category\":\"User Role\"},{\"sequenceNumber\":173,\"key\":\"role_id_10\",\"tenantTerm\":\"assistant specialty chief editor\",\"frontiersDefaultTerm\":\"assistant specialty chief editor\",\"category\":\"User Role\"},{\"sequenceNumber\":174,\"key\":\"role_id_11\",\"tenantTerm\":\"associate editor\",\"frontiersDefaultTerm\":\"associate editor\",\"category\":\"User Role\"},{\"sequenceNumber\":175,\"key\":\"role_id_12\",\"tenantTerm\":\"guest associate editor\",\"frontiersDefaultTerm\":\"guest associate editor\",\"category\":\"User Role\"},{\"sequenceNumber\":176,\"key\":\"role_id_13\",\"tenantTerm\":\"review editor\",\"frontiersDefaultTerm\":\"review editor\",\"category\":\"User Role\"},{\"sequenceNumber\":177,\"key\":\"role_id_14\",\"tenantTerm\":\"reviewer\",\"frontiersDefaultTerm\":\"reviewer\",\"category\":\"User Role\"},{\"sequenceNumber\":178,\"key\":\"role_id_15\",\"tenantTerm\":\"author\",\"frontiersDefaultTerm\":\"author\",\"category\":\"User Role\"},{\"sequenceNumber\":179,\"key\":\"role_id_16\",\"tenantTerm\":\"corresponding author\",\"frontiersDefaultTerm\":\"corresponding author\",\"category\":\"User Role\"},{\"sequenceNumber\":180,\"key\":\"role_id_17\",\"tenantTerm\":\"submitting author\",\"frontiersDefaultTerm\":\"submitting author\",\"category\":\"User Role\"},{\"sequenceNumber\":181,\"key\":\"role_id_18\",\"tenantTerm\":\"co-author\",\"frontiersDefaultTerm\":\"co-author\",\"category\":\"User Role\"},{\"sequenceNumber\":182,\"key\":\"role_id_20\",\"tenantTerm\":\"production office\",\"frontiersDefaultTerm\":\"production office\",\"category\":\"User Role\"},{\"sequenceNumber\":183,\"key\":\"role_id_22\",\"tenantTerm\":\"typesetting office (typesetter)\",\"frontiersDefaultTerm\":\"typesetting office (typesetter)\",\"category\":\"User Role\"},{\"sequenceNumber\":184,\"key\":\"role_id_24\",\"tenantTerm\":\"registered user\",\"frontiersDefaultTerm\":\"registered user\",\"category\":\"User Role\"},{\"sequenceNumber\":185,\"key\":\"role_id_35\",\"tenantTerm\":\"job office\",\"frontiersDefaultTerm\":\"job office\",\"category\":\"User Role\"},{\"sequenceNumber\":186,\"key\":\"role_id_41\",\"tenantTerm\":\"special event administrator\",\"frontiersDefaultTerm\":\"special event administrator\",\"category\":\"User Role\"},{\"sequenceNumber\":187,\"key\":\"role_id_42\",\"tenantTerm\":\"special event reviewer\",\"frontiersDefaultTerm\":\"special event reviewer\",\"category\":\"User Role\"},{\"sequenceNumber\":188,\"key\":\"role_id_43\",\"tenantTerm\":\"submit abstract\",\"frontiersDefaultTerm\":\"submit abstract\",\"category\":\"User Role\"},{\"sequenceNumber\":189,\"key\":\"role_id_52\",\"tenantTerm\":\"events office\",\"frontiersDefaultTerm\":\"events office\",\"category\":\"User Role\"},{\"sequenceNumber\":190,\"key\":\"role_id_53\",\"tenantTerm\":\"event administrator\",\"frontiersDefaultTerm\":\"event administrator\",\"category\":\"User Role\"},{\"sequenceNumber\":191,\"key\":\"role_id_89\",\"tenantTerm\":\"content management office\",\"frontiersDefaultTerm\":\"content management office\",\"category\":\"User Role\"},{\"sequenceNumber\":192,\"key\":\"role_id_98\",\"tenantTerm\":\"accounting office\",\"frontiersDefaultTerm\":\"accounting office\",\"category\":\"User Role\"},{\"sequenceNumber\":193,\"key\":\"role_id_99\",\"tenantTerm\":\"projects\",\"frontiersDefaultTerm\":\"projects\",\"category\":\"User Role\"},{\"sequenceNumber\":194,\"key\":\"role_id_103\",\"tenantTerm\":\"configuration office\",\"frontiersDefaultTerm\":\"configuration office\",\"category\":\"User Role\"},{\"sequenceNumber\":195,\"key\":\"role_id_104\",\"tenantTerm\":\"beta user\",\"frontiersDefaultTerm\":\"beta user\",\"category\":\"User Role\"},{\"sequenceNumber\":196,\"key\":\"role_id_106\",\"tenantTerm\":\"wfconf\",\"frontiersDefaultTerm\":\"wfconf\",\"category\":\"User Role\"},{\"sequenceNumber\":197,\"key\":\"role_id_107\",\"tenantTerm\":\"rt management beta user\",\"frontiersDefaultTerm\":\"rt management beta user\",\"category\":\"User Role\"},{\"sequenceNumber\":198,\"key\":\"role_id_108\",\"tenantTerm\":\"deo beta user\",\"frontiersDefaultTerm\":\"deo beta user\",\"category\":\"User Role\"},{\"sequenceNumber\":199,\"key\":\"role_id_109\",\"tenantTerm\":\"search beta user\",\"frontiersDefaultTerm\":\"search beta user\",\"category\":\"User Role\"},{\"sequenceNumber\":200,\"key\":\"role_id_110\",\"tenantTerm\":\"journal manager\",\"frontiersDefaultTerm\":\"journal manager\",\"category\":\"User Role\"},{\"sequenceNumber\":201,\"key\":\"role_id_111\",\"tenantTerm\":\"myfrontiers beta user\",\"frontiersDefaultTerm\":\"myfrontiers beta user\",\"category\":\"User Role\"},{\"sequenceNumber\":202,\"key\":\"role_id_21\",\"tenantTerm\":\"copy editor\",\"frontiersDefaultTerm\":\"copy editor\",\"category\":\"User Role\"},{\"sequenceNumber\":203,\"key\":\"role_id_1_abr\",\"tenantTerm\":\"ROF\",\"frontiersDefaultTerm\":\"ROF\",\"category\":\"User Role\"},{\"sequenceNumber\":204,\"key\":\"role_id_2_abr\",\"tenantTerm\":\"EOF\",\"frontiersDefaultTerm\":\"EOF\",\"category\":\"User Role\"},{\"sequenceNumber\":205,\"key\":\"role_id_7_abr\",\"tenantTerm\":\"FCE\",\"frontiersDefaultTerm\":\"FCE\",\"category\":\"User Role\"},{\"sequenceNumber\":206,\"key\":\"role_id_8_abr\",\"tenantTerm\":\"AFCE\",\"frontiersDefaultTerm\":\"AFCE\",\"category\":\"User Role\"},{\"sequenceNumber\":207,\"key\":\"role_id_9_abr\",\"tenantTerm\":\"SCE\",\"frontiersDefaultTerm\":\"SCE\",\"category\":\"User Role\"},{\"sequenceNumber\":208,\"key\":\"role_id_10_abr\",\"tenantTerm\":\"ASCE\",\"frontiersDefaultTerm\":\"ASCE\",\"category\":\"User Role\"},{\"sequenceNumber\":209,\"key\":\"role_id_11_abr\",\"tenantTerm\":\"AE\",\"frontiersDefaultTerm\":\"AE\",\"category\":\"User Role\"},{\"sequenceNumber\":210,\"key\":\"role_id_12_abr\",\"tenantTerm\":\"GAE\",\"frontiersDefaultTerm\":\"GAE\",\"category\":\"User Role\"},{\"sequenceNumber\":211,\"key\":\"role_id_13_abr\",\"tenantTerm\":\"RE\",\"frontiersDefaultTerm\":\"RE\",\"category\":\"User Role\"},{\"sequenceNumber\":212,\"key\":\"role_id_14_abr\",\"tenantTerm\":\"REV\",\"frontiersDefaultTerm\":\"REV\",\"category\":\"User Role\"},{\"sequenceNumber\":213,\"key\":\"role_id_15_abr\",\"tenantTerm\":\"AUT\",\"frontiersDefaultTerm\":\"AUT\",\"category\":\"User Role\"},{\"sequenceNumber\":214,\"key\":\"role_id_16_abr\",\"tenantTerm\":\"CORAUT\",\"frontiersDefaultTerm\":\"CORAUT\",\"category\":\"User Role\"},{\"sequenceNumber\":215,\"key\":\"role_id_17_abr\",\"tenantTerm\":\"SAUT\",\"frontiersDefaultTerm\":\"SAUT\",\"category\":\"User Role\"},{\"sequenceNumber\":216,\"key\":\"role_id_18_abr\",\"tenantTerm\":\"COAUT\",\"frontiersDefaultTerm\":\"COAUT\",\"category\":\"User Role\"},{\"sequenceNumber\":217,\"key\":\"role_id_20_abr\",\"tenantTerm\":\"POF\",\"frontiersDefaultTerm\":\"POF\",\"category\":\"User Role\"},{\"sequenceNumber\":218,\"key\":\"role_id_22_abr\",\"tenantTerm\":\"TSOF\",\"frontiersDefaultTerm\":\"TSOF\",\"category\":\"User Role\"},{\"sequenceNumber\":219,\"key\":\"role_id_24_abr\",\"tenantTerm\":\"RU\",\"frontiersDefaultTerm\":\"RU\",\"category\":\"User Role\"},{\"sequenceNumber\":220,\"key\":\"role_id_35_abr\",\"tenantTerm\":\"JOF\",\"frontiersDefaultTerm\":\"JOF\",\"category\":\"User Role\"},{\"sequenceNumber\":221,\"key\":\"role_id_41_abr\",\"tenantTerm\":\"SE-ADM\",\"frontiersDefaultTerm\":\"SE-ADM\",\"category\":\"User Role\"},{\"sequenceNumber\":222,\"key\":\"role_id_42_abr\",\"tenantTerm\":\"SE-REV\",\"frontiersDefaultTerm\":\"SE-REV\",\"category\":\"User Role\"},{\"sequenceNumber\":223,\"key\":\"role_id_43_abr\",\"tenantTerm\":\"SE-AUT\",\"frontiersDefaultTerm\":\"SE-AUT\",\"category\":\"User Role\"},{\"sequenceNumber\":224,\"key\":\"role_id_52_abr\",\"tenantTerm\":\"EVOF\",\"frontiersDefaultTerm\":\"EVOF\",\"category\":\"User Role\"},{\"sequenceNumber\":225,\"key\":\"role_id_53_abr\",\"tenantTerm\":\"EV-ADM\",\"frontiersDefaultTerm\":\"EV-ADM\",\"category\":\"User Role\"},{\"sequenceNumber\":226,\"key\":\"role_id_89_abr\",\"tenantTerm\":\"COMOF\",\"frontiersDefaultTerm\":\"COMOF\",\"category\":\"User Role\"},{\"sequenceNumber\":227,\"key\":\"role_id_98_abr\",\"tenantTerm\":\"AOF\",\"frontiersDefaultTerm\":\"AOF\",\"category\":\"User Role\"},{\"sequenceNumber\":228,\"key\":\"role_id_99_abr\",\"tenantTerm\":\"Projects\",\"frontiersDefaultTerm\":\"Projects\",\"category\":\"User Role\"},{\"sequenceNumber\":229,\"key\":\"role_id_103_abr\",\"tenantTerm\":\"CONFIG\",\"frontiersDefaultTerm\":\"CONFIG\",\"category\":\"User Role\"},{\"sequenceNumber\":230,\"key\":\"role_id_104_abr\",\"tenantTerm\":\"BETA\",\"frontiersDefaultTerm\":\"BETA\",\"category\":\"User Role\"},{\"sequenceNumber\":231,\"key\":\"role_id_106_abr\",\"tenantTerm\":\"WFCONF\",\"frontiersDefaultTerm\":\"WFCONF\",\"category\":\"User Role\"},{\"sequenceNumber\":232,\"key\":\"role_id_107_abr\",\"tenantTerm\":\"RTBETA\",\"frontiersDefaultTerm\":\"RTBETA\",\"category\":\"User Role\"},{\"sequenceNumber\":233,\"key\":\"role_id_108_abr\",\"tenantTerm\":\"DEOBETA\",\"frontiersDefaultTerm\":\"DEOBETA\",\"category\":\"User Role\"},{\"sequenceNumber\":234,\"key\":\"role_id_109_abr\",\"tenantTerm\":\"SEARCHBETA\",\"frontiersDefaultTerm\":\"SEARCHBETA\",\"category\":\"User Role\"},{\"sequenceNumber\":235,\"key\":\"role_id_110_abr\",\"tenantTerm\":\"JM\",\"frontiersDefaultTerm\":\"JM\",\"category\":\"User Role\"},{\"sequenceNumber\":236,\"key\":\"role_id_111_abr\",\"tenantTerm\":\"MFBETA\",\"frontiersDefaultTerm\":\"MFBETA\",\"category\":\"User Role\"},{\"sequenceNumber\":237,\"key\":\"role_id_21_abr\",\"tenantTerm\":\"COPED\",\"frontiersDefaultTerm\":\"COPED\",\"category\":\"User Role\"},{\"sequenceNumber\":238,\"key\":\"reviewer_editorial_board\",\"tenantTerm\":\"editorial board\",\"frontiersDefaultTerm\":\"editorial board\",\"description\":\"This is the label for the review editorial board\",\"category\":\"Label\"},{\"sequenceNumber\":239,\"key\":\"field_chief_editor\",\"tenantTerm\":\"Field Chief Editor\",\"frontiersDefaultTerm\":\"Field Chief Editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":240,\"key\":\"field_chief_editors\",\"tenantTerm\":\"Field Chief Editors\",\"frontiersDefaultTerm\":\"Field Chief Editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":241,\"key\":\"editor\",\"tenantTerm\":\"editor\",\"frontiersDefaultTerm\":\"editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":242,\"key\":\"editors\",\"tenantTerm\":\"editors\",\"frontiersDefaultTerm\":\"editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":243,\"key\":\"board\",\"tenantTerm\":\"board\",\"frontiersDefaultTerm\":\"board\",\"category\":\"Label\"},{\"sequenceNumber\":244,\"key\":\"boards\",\"tenantTerm\":\"boards\",\"frontiersDefaultTerm\":\"boards\",\"category\":\"Label\"},{\"sequenceNumber\":245,\"key\":\"article_collection\",\"tenantTerm\":\"article collection\",\"frontiersDefaultTerm\":\"article collection\",\"category\":\"Label\"},{\"sequenceNumber\":246,\"key\":\"article_collections\",\"tenantTerm\":\"article collections\",\"frontiersDefaultTerm\":\"article collections\",\"category\":\"Label\"},{\"sequenceNumber\":247,\"key\":\"handling_editor\",\"tenantTerm\":\"handling editor\",\"frontiersDefaultTerm\":\"associate editor\",\"description\":\"This terminology key is for the person assigned to edit a manuscript. It is a label for the temporary handling editor assignment.\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":248,\"key\":\"handling_editors\",\"tenantTerm\":\"handling editors\",\"frontiersDefaultTerm\":\"associate editors\",\"description\":\"This terminology key is for the person assigned to edit a manuscript. It is a label for the temporary handling editor assignment.\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":249,\"key\":\"ae_accept\",\"tenantTerm\":\"recommend acceptance\",\"frontiersDefaultTerm\":\"accept\",\"category\":\"Process\"},{\"sequenceNumber\":250,\"key\":\"rtm\",\"tenantTerm\":\"RTM\",\"frontiersDefaultTerm\":\"RTM\",\"category\":\"Product\"},{\"sequenceNumber\":251,\"key\":\"frontiers_media_sa\",\"tenantTerm\":\"Frontiers Media S.A\",\"frontiersDefaultTerm\":\"Frontiers Media S.A\",\"category\":\"Customer\"},{\"sequenceNumber\":252,\"key\":\"review_editors\",\"tenantTerm\":\"Review Editors\",\"frontiersDefaultTerm\":\"Review Editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":253,\"key\":\"journal_card_chief_editor\",\"tenantTerm\":\"Chief Editor\",\"frontiersDefaultTerm\":\"Chief Editor\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":254,\"key\":\"journal_card_chief_editors\",\"tenantTerm\":\"Chief Editors\",\"frontiersDefaultTerm\":\"Chief Editors\",\"category\":\"Label (Role)\"},{\"sequenceNumber\":255,\"key\":\"call_for_papers\",\"tenantTerm\":\"Call for papers\",\"frontiersDefaultTerm\":\"Call for papers\",\"category\":\"Label\"},{\"sequenceNumber\":256,\"key\":\"calls_for_papers\",\"tenantTerm\":\"Calls for papers\",\"frontiersDefaultTerm\":\"Calls for papers\",\"category\":\"Label\"},{\"sequenceNumber\":257,\"key\":\"supervising_editor\",\"tenantTerm\":\"Supervising Editor\",\"frontiersDefaultTerm\":\"Supervising Editor\",\"description\":\"A Chief or Assistant Chief editor who is assigned to a manuscript to supervise.\",\"category\":\"Role\",\"externalKey\":\"supervising_editor\"},{\"sequenceNumber\":258,\"key\":\"supervising_editors\",\"tenantTerm\":\"Supervising Editors\",\"frontiersDefaultTerm\":\"Supervising Editors\",\"description\":\"A Chief or Assistant Chief editor who is assigned to a manuscript to supervise.\",\"category\":\"Role\",\"externalKey\":\"supervising_editors\"},{\"sequenceNumber\":259,\"key\":\"reviewer_endorse\",\"tenantTerm\":\"endorse\",\"frontiersDefaultTerm\":\"endorse\",\"category\":\"Label\"},{\"sequenceNumber\":260,\"key\":\"reviewer_endorsed\",\"tenantTerm\":\"endorsed\",\"frontiersDefaultTerm\":\"endorsed\",\"category\":\"Label\"},{\"sequenceNumber\":261,\"key\":\"reviewer_endorse_publication\",\"tenantTerm\":\"endorse publication\",\"frontiersDefaultTerm\":\"endorse publication\",\"category\":\"Label\"},{\"sequenceNumber\":262,\"key\":\"reviewer_endorsed_publication\",\"tenantTerm\":\"endorsed publication\",\"frontiersDefaultTerm\":\"endorsed publication\",\"category\":\"Label\"},{\"sequenceNumber\":263,\"key\":\"editor_role\",\"tenantTerm\":\"editor role\",\"frontiersDefaultTerm\":\"Editor Role\",\"category\":\"Label\"},{\"sequenceNumber\":264,\"key\":\"editor_roles\",\"tenantTerm\":\"editor roles\",\"frontiersDefaultTerm\":\"Editor Roles\",\"category\":\"Label\"},{\"sequenceNumber\":265,\"key\":\"editorial_role\",\"tenantTerm\":\"editorial role\",\"frontiersDefaultTerm\":\"Editorial Role\",\"category\":\"Label\"},{\"sequenceNumber\":266,\"key\":\"editorial_roles\",\"tenantTerm\":\"editorial roles\",\"frontiersDefaultTerm\":\"Editorial Roles\",\"category\":\"Label\"},{\"sequenceNumber\":267,\"key\":\"call_for_paper\",\"tenantTerm\":\"Call for paper\",\"frontiersDefaultTerm\":\"Call for paper\",\"category\":\"Label\"},{\"sequenceNumber\":268,\"key\":\"research_topic_abstract\",\"tenantTerm\":\"manuscript summary\",\"frontiersDefaultTerm\":\"manuscript summary\",\"category\":\"Process\"},{\"sequenceNumber\":269,\"key\":\"research_topic_abstracts\",\"tenantTerm\":\"manuscript summaries\",\"frontiersDefaultTerm\":\"manuscript summaries\",\"category\":\"Process\"},{\"sequenceNumber\":270,\"key\":\"submissions_team_manager\",\"tenantTerm\":\"Journal Manager\",\"frontiersDefaultTerm\":\"Content Manager\",\"category\":\"Process\"},{\"sequenceNumber\":271,\"key\":\"submissions_team\",\"tenantTerm\":\"Journal Team\",\"frontiersDefaultTerm\":\"Content Team\",\"category\":\"Process\"},{\"sequenceNumber\":272,\"key\":\"topic_coordinator\",\"tenantTerm\":\"topic coordinator\",\"frontiersDefaultTerm\":\"topic coordinator\",\"category\":\"Process\"},{\"sequenceNumber\":273,\"key\":\"topic_coordinators\",\"tenantTerm\":\"topic coordinators\",\"frontiersDefaultTerm\":\"topic coordinators\",\"category\":\"Process\"},{\"sequenceNumber\":274,\"key\":\"frontiers_journal_team\",\"tenantTerm\":\"frontiers journal team\",\"frontiersDefaultTerm\":\"frontiers journal team\",\"category\":\"Label (Role)\"}]}'\n",impactGtmId:"GTM-NJF2ML9B",impactGtmAuth:"fYo-7NoDm9w37QgFMs03SA",impactGtmPreview:"env-1",impactGoogleMapsApiKey:"AIzaSyCTEhX2EU8PWfi86GW9FI00FTcyKk6Ifr8",qualtricsV4ToV3:"'{\"enabled\":true,\"interceptId\":\"SI_er0E2ze69Oz8Yn4\",\"projectId\":\"ZN_cLOy77JHkpc8Gai\",\"brandId\":\"frontiersin\"}'\n",qualtricsUmux:"'{\"enabled\":false,\"interceptId\":\"SI_89fPHY3eTxQBwtU\",\"projectId\":\"ZN_cLOy77JHkpc8Gai\",\"brandId\":\"frontiersin\"}'\n",qualtricsContinuousBrand:"'{\"enabled\":true,\"interceptId\":\"SI_9ZuT94UT81FcRh4\",\"projectId\":\"ZN_cLOy77JHkpc8Gai\",\"brandId\":\"frontiersin\"}'\n",defaultArticleTemplate:"v3"},app:{baseURL:"/",buildId:"62e73f28-8eac-441a-a592-b394dc036e30",buildAssetsDir:"ap-2024/_nuxt",cdnURL:""}}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" type="text/javascript" async></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript" async></script>
<script src="https://d1bxh8uas1mnw7.cloudfront.net/assets/altmetric_badges-f0bc9b243ff5677d05460c1eb71834ca998946d764eb3bc244ab4b18ba50d21e.js" type="text/javascript" async></script>
<script src="https://api.altmetric.com/v1/doi/10.3389/fnins.2024.1400444?callback=_altmetric.embed_callback&domain=www.frontiersin.org&key=3c130976ca2b8f2e88f8377633751ba1&cache_until=14-15" type="text/javascript" async></script>
<script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js" type="text/javascript" async></script></body></html>